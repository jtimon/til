mode lib

import("src/core/std")

LANG_NAME := "til"
DEBUG_COMPILER := true

TokenType := enum {
    // basic
    Eof,

    // Single-character tokens.
    Minus, Plus, Slash, Star,
    LeftParen, RightParen, LeftBrace, RightBrace, LeftBracket, RightBracket,
    Comma, Colon,

    // One or two character tokens.
    Dot, DoubleDot,
    Not, NotEqual,
    Equal, EqualEqual,
    Greater, GreaterEqual,
    Lesser, LesserEqual,
    Semicolon, DoubleSemicolon,

    // Literals.
    Identifier, String, Number,

    // Reserved words:
    Mut,

    // bool
    True,

    // type definition
    Struct, Enum,

    // function definition
    Returns, Throws,

    // flow control
    If, Else,
    While, For, In,
    Match, Switch, Default, Case,
    Return, Throw,
    Try, Catch,

    // Special in this language:
    Mode,
    Func, Proc, Macro,
    FuncExt, ProcExt,

    // Errors (unsupported tokens)
    Const, Var,
    Fn,
    Invalid,
    UnterminatedString,
    UnterminatedComment,
}

Token := struct {
    mut token_type: TokenType = TokenType.Invalid
    mut token_str: Str = ""
    mut line: I64 = 0
    mut col: I64 = 0

    lang_error := func(self: Token, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
        mut line_copy := self.line
        mut col_copy := self.col
        return format(line_copy.to_str(), ":", col_copy.to_str(), ": ",
            LANG_NAME, " parse ERROR: ", msg, "\n",
            "Explanation: This is not your fault as a user, this is a bug in the language.")
    }

    todo_error := func(self: Token, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
        mut line_copy := self.line
        mut col_copy := self.col
        return format(line_copy.to_str(), ":", col_copy.to_str(), ": ",
            LANG_NAME, " parse ERROR: ", msg, "\n",
            "Explanation: Not implemented yet, this is a missing feature in the language.")
    }

    error := func(self: Token, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
        mut line_copy := self.line
        mut col_copy := self.col
        return format(line_copy.to_str(), ":", col_copy.to_str(), ": parse ERROR: ", msg)
    }
}

scan_push_token := proc(mut tokens: Array, token_type: TokenType, token_str: Str, line: I64, col: I64) throws Str {
    mut t := Token()
    t.token_type = token_type
    t.token_str = token_str
    t.line = line
    t.col = col

    tokens.push(t)
    catch (err: FullError) {
        throw format(loc(), "scan_tokens: tokens array full:\n", err.msg)
    }
}

is_digit := func(source: Str, pos: I64) returns Bool throws IndexOutOfBoundsError, AllocError {
    switch source.get_substr(pos, pos.add(1)) {
    case "0".."9": return true
    case: return false
    }
}

is_id_start := func(source: Str, pos: I64) returns Bool throws IndexOutOfBoundsError, AllocError {
    // current_char := Str.get_substr(source, pos, add(pos, 1)) // TODO FIX throws check doesn't propagate for associated functions (unless called as methods)
    current_char := source.get_substr(pos, pos.add(1))
    switch current_char {
    case "a".."z": return true
    case "A".."Z": return true
    case "_": return true
    case:
    }
    return false
}

scan_reserved_words := func(identifier: Str) returns TokenType {
    switch identifier {
    case "mode": return TokenType.Mode
        // declaration/arg modifiers
    case "mut": return TokenType.Mut
        // bool literals
    case "true": return TokenType.True
        // core data types
    case "enum": return TokenType.Enum
    case "struct": return TokenType.Struct
        // function declaration
    case "returns": return TokenType.Returns
        // Anything that can be thrown must be explicitly declared in the function via 'throws', java style.
        // Except perhaps PanicException or something like that which can be implicit, but still allowed to documment redundantly
        // or perhaps not, for that may degenerate in an extra warning option
        // perhaps just force the user to explicitly catch and exit any potential panic from the callee
    case "throws": return TokenType.Throws // TODO implement
    case "func": return TokenType.Func
    case "proc": return TokenType.Proc
    case "macro": return TokenType.Macro // TODO implement for real once we compile
    case "ext_func": return TokenType.FuncExt // this have to link when we compile
    case "ext_proc": return TokenType.ProcExt // this have to link when we compile

        // control flow
    case "if": return TokenType.If
    case "else": return TokenType.Else
    case "while": return TokenType.While
    case "for": return TokenType.For // TODO
    case "in": return TokenType.In // TODO, or just use semicolon reserve forbid this
    case "switch": return TokenType.Switch
    case "match": return TokenType.Match // TODO like switch but special for declarations/assignments
    case "case": return TokenType.Case
    case "default": return TokenType.Default // TODO currently using "case:", but "default:" is more traditional, grepable and overt
    case "return": return TokenType.Return
    case "throw": return TokenType.Throw
        // TODO throw should just act as a return that gets post-processed by the next catch or rethrown
    case "catch": return TokenType.Catch
        // or should 'try:' be optional?

        // Reserved forbidden/illegal words (intentionally unsupported reserved words)
        // TODO intentionally unsupport more reserved words
        // TODO nicer messages for forbidden words
    case "fn": return TokenType.Fn
    case "function": return TokenType.Invalid
    case "try": return TokenType.Try
    case "global": return TokenType.Invalid // just use mut declaration in the root of the file, but they're not allowed in all modes
        // const/vars are the most abstract types, you can't even explicitly declare them
    case "const": return TokenType.Const
    case "var": return TokenType.Var
        // Do we really need const fields static other than static? (ie can be different per instance, but not modified afterwards)
        // The answer is probably yet, but perhaps static is not the right answer
        // how about this? if it's in the struct body, it is const, if it is in impl, it is static, just like functions\
        // or do we need mut function fields too? probably yes
    case "static": return TokenType.Invalid
    case:
    }
    return TokenType.Identifier
}

scan_tokens := proc(source: Str) returns Array throws Str {
    mut tokens := Array.new_dyn("Token", size_of(Token))

    eof_pos := source.len()
    mut pos := 0
    // NOTE: Rust version starts line at 1, but we start at 0 to avoid adding 1 everywhere
    // Both approaches are valid, this is simpler
    mut line := 0
    mut start_line_pos := 0

    while pos.lt(eof_pos) {
        start := pos
        if source.is_digit(pos) {
            while and(pos.lt(eof_pos), source.is_digit(pos)) {
                pos.inc()
            }
            // Look for a fractional part.
            if and(source.get_substr(pos, pos.add(1)).eq("."), source.is_digit(pos.add(1))) {
                pos.inc()
                while and(pos.lt(eof_pos), source.is_digit(pos)) {
                    pos.inc()
                }
            }
            scan_push_token(tokens, TokenType.Number, source.get_substr(start, pos), line, pos.sub(start_line_pos))
        } else {

            current_char := source.get_substr(pos, pos.add(1))
            switch current_char {
                // chars to ignore in this language:
            case " ":  pos.inc()
            case "\r": pos.inc()
            case "\t": pos.inc()
            case "\n":
                pos.inc()
                line.inc()
                start_line_pos = pos

                // open/close. left/right parentheses, craces and brackets
            case "(":
                scan_push_token(tokens, TokenType.LeftParen, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case ")":
                scan_push_token(tokens, TokenType.RightParen, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "{":
                scan_push_token(tokens, TokenType.LeftBrace, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "}":
                scan_push_token(tokens, TokenType.RightBrace, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "[":
                scan_push_token(tokens, TokenType.LeftBracket, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "]":
                scan_push_token(tokens, TokenType.RightBracket, current_char, line, pos.sub(start_line_pos))
                pos.inc()

            case ";":
                if source.get_substr(pos.add(1), pos.add(2)).eq(";") {
                    scan_push_token(tokens, TokenType.DoubleSemicolon, ";;", line, pos.sub(start_line_pos))
                    pos = pos.add(2)
                } else {
                    scan_push_token(tokens, TokenType.Semicolon, ";", line, pos.sub(start_line_pos))
                    pos.inc()
                }
                // separator for optional type before the equal in declarations or args
            case ":":
                scan_push_token(tokens, TokenType.Colon, current_char, line, pos.sub(start_line_pos))
                pos.inc()
                // separator for args
            case ",":
                scan_push_token(tokens, TokenType.Comma, current_char, line, pos.sub(start_line_pos))
                pos.inc()

                // math
            case "+":
                scan_push_token(tokens, TokenType.Plus, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "-":
                scan_push_token(tokens, TokenType.Minus, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "*":
                scan_push_token(tokens, TokenType.Star, current_char, line, pos.sub(start_line_pos))
                pos.inc()

                // reserved for two chars in a row
            case ".":
                if source.get_substr(pos.add(1), pos.add(2)).eq(".") {
                    scan_push_token(tokens, TokenType.DoubleDot, "..", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Dot, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case "=":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    scan_push_token(tokens, TokenType.EqualEqual, "==", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Equal, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case "<":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    scan_push_token(tokens, TokenType.LesserEqual, "<=", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Lesser, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case ">":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    scan_push_token(tokens, TokenType.GreaterEqual, ">=", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Greater, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case "!":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    scan_push_token(tokens, TokenType.NotEqual, "!=", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Not, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()

                // comments:
            case "#":
                pos.inc()
                while and(lt(pos.add(1), eof_pos), not(source.get_substr(pos, pos.add(2)).eq("\n"))) {
                    pos.inc()
                }
            case "/":
                if source.get_substr(pos.add(1), pos.add(2)).eq("/") {
                    // Single-line comment
                    pos = pos.add(2)
                    while and(lt(pos.add(1), eof_pos), not(source.get_substr(pos, pos.add(2)).eq("\n"))) {
                        pos.inc()
                    }
                    pos.inc()
                } else if source.get_substr(pos.add(1), pos.add(2)).eq("*") {
                    // Nested block comment
                    pos = pos.add(2)
                    depth := 1
                    mut done := false

                    while and(pos.lt(sub(eof_pos, 1)), not(done)) {
                        next_two := source.get_substr(pos, pos.add(2))

                        if next_two.eq("/*") {
                            depth.inc()
                            pos = pos.add(2)
                        } else if next_two.eq("*/") {
                            depth.dec()
                            pos = pos.add(2)
                            if depth.eq(0) {
                                done = true
                            }
                        } else {
                            if source.get_substr(pos, pos.add(1)).eq("\n") {
                                line.inc()
                                start_line_pos = pos.add(1)
                            }
                            pos.inc()
                        }
                    }

                    if not(done) {
                        scan_push_token(tokens, TokenType.UnterminatedComment, "/*", line, pos.sub(start_line_pos))
                    }

                } else {
                    scan_push_token(tokens, TokenType.Slash, current_char, line, pos.sub(start_line_pos))
                    pos.inc()
                }

                // literal strings
            case "\"":
                pos.inc()
                mut done := false
                mut lit_string := ""

                while and(lt(pos, eof_pos), not(done)) {
                    current := source.get_substr(pos, pos.add(1))

                    if current.eq("\"") {
                        done = true
                    } else if current.eq("\\") {
                        pos.inc() // move past '\'
                        if lt(pos.add(1), eof_pos) {
                            esc := source.get_substr(pos, pos.add(1))
                            // Process escape sequences
                            switch esc {
                            case "\"":
                                lit_string = format(lit_string, "\"")
                            case "\\":
                                lit_string = format(lit_string, "\\")
                            case "n":
                                lit_string = format(lit_string, "\n")
                            case "r":
                                lit_string = format(lit_string, "\r")
                            case "t":
                                lit_string = format(lit_string, "\t")
                            case "0":
                                lit_string = format(lit_string, "\0")
                            case:
                                // Unknown escape: keep backslash and character
                                lit_string = format(lit_string, "\\", esc)
                            }
                        }
                    } else {
                        lit_string = format(lit_string, current)
                    }
                    pos.inc()
                }

                if not(done) {
                    token_str := source.get_substr(start, pos.add(1))
                    scan_push_token(tokens, TokenType.UnterminatedString, token_str, line, pos.sub(add(start_line_pos, 1)))
                } else {
                    scan_push_token(tokens, TokenType.String, lit_string, line, pos.sub(add(start_line_pos, 1)))
                    pos.inc()
                }

                // Everything else must be reserved words, identifiers or invalid
            case:
                if source.is_id_start(pos) {
                    pos.inc()
                    while and(pos.lt(eof_pos), or(source.is_digit(pos), source.is_id_start(pos))) {
                        pos.inc()
                        // TODO fix literal strings with escape characters like in the rust implementation
                    }
                    pos.dec()
                    id_str := source.get_substr(start, pos.add(1))
                    scan_push_token(tokens, scan_reserved_words(id_str), id_str, line, pos.sub(add(start_line_pos, 1)))
                } else {
                    scan_push_token(tokens, TokenType.Invalid, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            } // switch
        } // else
        // println("Current char:", current_char)
    } // while
    scan_push_token(tokens, TokenType.Eof, "End of file", line, 0);

    catch (err: IndexOutOfBoundsError) {
        throw format(loc(), "scan_tokens: IndexOutOfBoundsError error thrown")
    }
    catch (err: I64_OverflowError) {
        throw format(loc(), "scan_tokens: I64_OverflowError error thrown")
    }
    catch (err: AllocError) {
        throw format(loc(), "scan_tokens: AllocError error thrown")
    }
    return tokens
}

/** An interface for the Lexer */
Lexer := struct {
    mut path: Str = ""
    // mut tokens: Array = Array.new_dyn("Token", size_of(Token)) // TODO re-enable new_dyn
    mut tokens: Array = Array.new("Token", size_of(Token), 1) // TODO remove this
    mut current_token: I64 = 0

    new := proc(path: Str) returns Lexer {
        mut l := Lexer()
        l.path = path
        l.tokens = scan_tokens(readfile(path))
        return l
    }

    new_from_src := proc(source: Str) returns Lexer {
        mut l := Lexer()
        l.path = "<test>"
        l.tokens = scan_tokens(source)
        return l
    }

    len := func(self: Lexer) returns I64 {
        self_tokens := self.tokens
        return self_tokens.len
    }

    is_eof := func(self: Lexer, offset: I64) returns Bool throws IndexOutOfBoundsError {
        self_tokens := self.tokens
        i := self.current_token.add(offset)
        if i.gteq(self_tokens.len) {
            return true
        }
        mut t := Token()
        self_tokens.get(i, t)
        return TokenType.eq(t.token_type, TokenType.Eof)
    }

    get_token := func(self: Lexer, index: I64) returns Token throws IndexOutOfBoundsError {
        self_tokens := self.tokens
        mut t := Token()
        self_tokens.get(index, t)
        return t
    }

    peek := func(self: Lexer) returns Token throws IndexOutOfBoundsError {
        self_tokens := self.tokens
        mut t := Token()
        self_tokens.get(self.current_token, t)
        return t
    }

    peek_ahead := func(self: Lexer, offset: I64) returns Token throws IndexOutOfBoundsError {
        index := add(self.current_token, offset)
        self_tokens := self.tokens
        if gteq(index, self_tokens.len) {
            t := self.peek()
            self.error(loc(), t, Str.fmt("Peek ahead by ", I64.to_str(offset), " is out of bounds"))
        }
        mut t := Token()
        self_tokens.get(index, t)
        return t
    }

    next := func(self: Lexer) returns Token throws IndexOutOfBoundsError {
        return self.peek_ahead(1)
    }

    previous := func(self: Lexer) returns Token throws IndexOutOfBoundsError {
        if eq(self.current_token, 0) {
            t := self.peek()
            self.error(loc(), t, "No previous token (at position 0)")
        }
        index := sub(self.current_token, 1)
        self_tokens := self.tokens
        if gteq(index, self_tokens.len) {
            t := self.peek()
            self.error(loc(), t, "Previous token is out of bounds")
        }
        mut t := Token()
        self_tokens.get(index, t)
        return t
    }

    advance := func(mut self: Lexer, count: I64) {
        next_index := add(self.current_token, count)
        if gt(next_index, self.tokens.len) {
            t := self.peek()
            self.error(
                loc(),
                t,
                Str.fmt("Advance by ", I64.to_str(count), " from ", I64.to_str(self.current_token),
                    " would exceed bounds (", I64.to_str(self.tokens.len), " tokens total)")
            )
        }
        self.current_token = next_index
    }

    go_back := func(mut self: Lexer, count: I64) {
        if gt(count, self.current_token) {
            t := self.peek()
            self.error(loc(), t,
                Str.fmt("Go back by ", I64.to_str(count), " from ", I64.to_str(self.current_token), " would underflow")
            )
        }
        self.current_token = sub(self.current_token, count)
    }

    expect := func(mut self: Lexer, expected: TokenType) returns Token {
        p := self.peek()
        if TokenType.eq(p.token_type, expected) {
            self.advance(1)
            return p
        }
        self.error(loc(), p, Str.fmt(
            "Expected token '", token_type_to_str(expected), "', but found '", token_type_to_str(p.token_type), "'"))
        return Token() // unreachable
    }
}

print_lex_error := proc(path: Str, t: Token, mut errors_found: I64, msg: Str) {
    print(path, ":", I64.to_str(t.line), ":", I64.to_str(t.col), ": Lexical error ", I64.to_str(errors_found), ": ", msg)
    print(". Offending symbol: '", t.token_str, "'")
    print("\n")
    errors_found.inc()
}

print_if_lex_error := proc(path: Str, t: Token, mut errors_found: I64) {
    switch t.token_type {
    case TokenType.Invalid:
        print_lex_error(path, t, errors_found, "Invalid character")

    case TokenType.UnterminatedString:
        print_lex_error(path, t, errors_found, "Unterminated Str\nSuggestion: add missing '\"'")

    case TokenType.UnterminatedComment:
        print_lex_error(path, t, errors_found, "Unterminated comment\nSuggestion: add missing '*/'")

    case TokenType.Const:
        print_lex_error(path, t, errors_found, "No need to use 'const', everything is const by default unless 'mut' is used")

    case TokenType.Var:
        print_lex_error(path, t, errors_found, "Keyword 'var' is not supported\nSuggestion: use 'mut' instead")

    case TokenType.Fn:
        print_lex_error(path, t, errors_found, "Keyword 'fn' is not supported\nSuggestion: use 'func' or 'proc' instead")

    case TokenType.DoubleSemicolon:
        print_lex_error(path, t, errors_found, "No need for ';;' (aka empty statements)\nSuggestion: try 'if true {}' instead, whatever you want that for")

    case TokenType.Plus:
        print_lex_error(path, t, errors_found, "Operator '+' is not supported yet\nSuggestion: use core func 'add' instead")

    case TokenType.Minus:
        print_lex_error(path, t, errors_found, "Operator '-' is not supported yet\nSuggestion: use core func 'sub' instead")

    case TokenType.Star:
        print_lex_error(path, t, errors_found, "Operator '*' is not supported yet\nSuggestion: use core func 'mul' instead")

    case TokenType.Slash:
        print_lex_error(path, t, errors_found, "Operator '/' is not supported yet\nSuggestion: use core func 'div' instead")

    case TokenType.EqualEqual:
        print_lex_error(path, t, errors_found, "Operator '==' is not supported yet\nSuggestion: use 'I64.eq' or 'Str.eq' instead")

    case TokenType.NotEqual:
        print_lex_error(path, t, errors_found, "Operator '!=' is not supported yet\nSuggestion: use core funcs 'not' and 'I64.eq'/'Str.eq' instead")

    case TokenType.Lesser:
        print_lex_error(path, t, errors_found, "Operator '<' is not supported yet\nSuggestion: use core func 'lt' instead")

    case TokenType.LesserEqual:
        print_lex_error(path, t, errors_found, "Operator '<=' is not supported yet\nSuggestion: use core func 'lteq' instead")

    case TokenType.Greater:
        print_lex_error(path, t, errors_found, "Operator '>' is not supported yet\nSuggestion: use core func 'gt' instead")

    case TokenType.GreaterEqual:
        print_lex_error(path, t, errors_found, "Operator '>=' is not supported yet\nSuggestion: use core func 'gteq' instead")

    case TokenType.Not:
        print_lex_error(path, t, errors_found, "Operator '!' is not supported yet\nSuggestion: use core func 'not' instead")

    case:
        // No error, do nothing
    }
}

print_lex_errors := proc(tokens: Array, path: Str) returns I64 throws IndexOutOfBoundsError {
    mut errors_found := 0
    for i in 0..tokens.len {
        mut t := Token()
        tokens.get(i, t)
        print_if_lex_error(path, t, errors_found)
    }
    return errors_found
}

lexer_from_source := proc(path: Str, source: Str) returns Lexer throws Str, IndexOutOfBoundsError {
    mut l := Lexer()
    l.path = path
    tokens := scan_tokens(source)
    l.tokens = tokens

    if I64.eq(tokens.len, 0) {
        throw path.format(":1:0: compiler ERROR: End of file not found.")
    }

    mut first := Token()
    tokens.get(0, first)
    switch first.token_type {
    case TokenType.Eof:
        throw path.format(":0:0: compiler ERROR: Nothing to be done")
    case:
    }

    mut errors := 0
    for i in 0..tokens.len {
        mut t := Token()
        tokens.get(i, t)
        print_if_lex_error(path, t, errors)
    }
    if gt(errors, 0) {
        throw format("Compiler errors: ", I64.to_str(errors), " lexical errors found")
    }

    return l
}
