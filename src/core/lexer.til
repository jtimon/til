mode lib

import("src/core/std")

LANG_NAME := "til"
DEBUG_COMPILER := true

TokenType := enum {
    // basic
    Eof,

    // Single-character tokens.
    Minus, Plus, Slash, Star,
    LeftParen, RightParen, LeftBrace, RightBrace, LeftBracket, RightBracket,
    Comma, Colon,

    // One or two character tokens.
    Dot, DoubleDot,
    Not, NotEqual,
    Equal, EqualEqual,
    Greater, GreaterEqual,
    Lesser, LesserEqual,
    Semicolon, DoubleSemicolon,

    // Literals.
    Identifier, String, Number,

    // Reserved words:
    Mut,

    // bool
    True,

    // type definition
    Struct, Enum,

    // function definition
    Returns, Throws,

    // flow control
    If, Else,
    While, For, In,
    Match, Switch, Default, Case,
    Return, Throw,
    Try, Catch,

    // Special in this language:
    Mode,
    Func, Proc, Macro,
    FuncExt, ProcExt,

    // Errors (unsupported tokens)
    Const, Var,
    Fn,
    Invalid,
    UnterminatedString,
    UnterminatedComment,
}

Token := struct {
    mut token_type: TokenType = TokenType.Invalid
    mut token_str: Str = ""
    mut line: I64 = 0
    mut col: I64 = 0

    lang_error := func(self: Token, msg: Str) returns Str {
        return Str.concat(
            I64.to_str(self.line), ":", I64.to_str(self.col), ": ",
            LANG_NAME, " parse ERROR: ", msg, "\n",
            "Explanation: This is not your fault as a user, this is a bug in the language.",
        )
    }

    todo_error := func(self: Token, msg: Str) returns Str {
        return Str.concat(
            I64.to_str(self.line), ":", I64.to_str(self.col), ": ",
            LANG_NAME, " parse ERROR: ", msg, "\n",
            "Explanation: Not implemented yet, this is a missing feature in the language.",
        )
    }

    error := func(self: Token, msg: Str) returns Str {
        return Str.concat(
            I64.to_str(self.line), ":", I64.to_str(self.col), ": parse ERROR: ", msg,
        )
    }
}

TokenArray := struct {
    mut ptr      : I64  = 0     // Pointer to the data buffer
    mut len      : I64  = 0     // Current number of elements
    mut cap      : I64  = 0     // Allocated capacity
    mut is_dyn   : Bool = false // Whether this array is dynamic

    INIT_DYN_CAP : I64  = 16
    MAX_CAP      : I64  = 512

    new := proc(capacity: I64) returns TokenArray {
        mut arr := TokenArray()
        size_bytes := mul(capacity, size_of(Token))
        arr.ptr = malloc(size_bytes)
        memset(arr.ptr, 0, size_bytes)
        arr.len = capacity
        arr.cap = capacity
        arr.is_dyn = false
        return arr
    }

    new_dyn := proc() returns TokenArray {
        mut arr := TokenArray()
        arr.cap = TokenArray.INIT_DYN_CAP
        size_bytes := mul(arr.cap, size_of(Token))
        arr.ptr = malloc(size_bytes)
        arr.len = 0
        arr.is_dyn = true
        memset(arr.ptr, 0, size_bytes)
        return arr
    }

    push := proc(mut self: TokenArray, t: Token) throws FullError {
        if not(self.is_dyn) {
            panic(loc(), "TokenArray.push: push is not allowed for static arrays")
        }
        if I64.eq(self.len, self.cap) {
            new_cap := mul(self.cap, 2)
            if gt(new_cap, TokenArray.MAX_CAP) {
                throw FullError.new(format(loc(), "TokenArray.push: capacity exceeded TokenArray.MAX_CAP"))
            }
            new_ptr := malloc(mul(new_cap, size_of(Token)))
            memcpy(new_ptr, self.ptr, mul(self.len, size_of(Token)))
            free(self.ptr)
            self.ptr = new_ptr
            self.cap = new_cap
        }
        dest := add(self.ptr, mul(self.len, size_of(Token)))
        memcpy(dest, to_ptr(t), size_of(Token))
        self.len = add(self.len, 1)
    }

    get := func(self: TokenArray, index: I64, mut t: Token) {
        if index.gteq(self.len) {
            panic(loc(), "TokenArray.get: index out of bounds")
        }
        src := add(self.ptr, mul(index, size_of(Token)))
        memcpy(to_ptr(t), src, size_of(Token))
        return t
    }

    delete := proc(mut self: TokenArray) {
        free(self.ptr)
        self.ptr = 0
        self.len = 0
        self.cap = 0
        self.is_dyn = false
    }
}

scan_push_token := proc(mut tokens: TokenArray, token_type: TokenType, token_str: Str, line: I64, col: I64) {
// scan_push_token := proc(mut tokens: TokenArray, token_type: TokenType, token_str: Str, line: I64, col: I64) throws Str {
    mut t := Token()
    t.token_type = token_type
    t.token_str = token_str
    t.line = line
    t.col = col

    println("Token (", I64.to_str(line), ", ", I64.to_str(col), "): Type: ", enum_to_str(t.token_type), " str: ", token_str)

    // tokens.push(t) // TODO FIX 115:9: eval ERROR: memcpy out of bounds: src=82424 dest=82477 size=24 arena_len=82493
    // catch (err: FullError) {
    //     throw format(loc(), "scan_tokens: tokens array full:\n", err.msg)
    // }
}

is_digit := func(source: Str, pos: I64) returns Bool throws IndexOutOfBoundsError, AllocError {
    switch source.get_substr(pos, pos.add(1)) {
    case "0".."9": return true
    case: return false
    }
}

is_id_start := func(source: Str, pos: I64) returns Bool throws IndexOutOfBoundsError, AllocError {
    // current_char := Str.get_substr(source, pos, add(pos, 1)) // TODO FIX throws check doesn't propagate for associated functions (unless called as methods)
    current_char := source.get_substr(pos, pos.add(1))
    switch current_char {
    case "a".."z": return true
    case "A".."Z": return true
    case "_": return true
    case:
    }
    return false
}

scan_reserved_words := func(identifier: Str) returns TokenType {
    switch identifier {
    case "mode": return TokenType.Mode
        // declaration/arg modifiers
    case "mut": return TokenType.Mut
        // bool literals
    case "true": return TokenType.True
        // core data types
    case "enum": return TokenType.Enum
    case "struct": return TokenType.Struct
        // function declaration
    case "returns": return TokenType.Returns
        // Anything that can be thrown must be explicitly declared in the function via 'throws', java style.
        // Except perhaps PanicException or something like that which can be implicit, but still allowed to documment redundantly
        // or perhaps not, for that may degenerate in an extra warning option
        // perhaps just force the user to explicitly catch and exit any potential panic from the callee
    case "throws": return TokenType.Throws // TODO implement
    case "func": return TokenType.Func
    case "proc": return TokenType.Proc
    case "macro": return TokenType.Macro // TODO implement for real once we compile
    case "ext_func": return TokenType.FuncExt // this have to link when we compile
    case "ext_proc": return TokenType.ProcExt // this have to link when we compile

        // control flow
    case "if": return TokenType.If
    case "else": return TokenType.Else
    case "while": return TokenType.While
    case "for": return TokenType.For // TODO
    case "in": return TokenType.In // TODO, or just use semicolon reserve forbid this
    case "switch": return TokenType.Switch
    case "match": return TokenType.Match // TODO like switch but special for declarations/assignments
    case "case": return TokenType.Case
    case "default": return TokenType.Default // TODO currently using "case:", but "default:" is more traditional, grepable and overt
    case "return": return TokenType.Return
    case "throw": return TokenType.Throw
        // TODO throw should just act as a return that gets post-processed by the next catch or rethrown
    case "catch": return TokenType.Catch
        // or should 'try:' be optional?

        // Reserved forbidden/illegal words (intentionally unsupported reserved words)
        // TODO intentionally unsupport more reserved words
        // TODO nicer messages for forbidden words
    case "fn": return TokenType.Fn
    case "function": return TokenType.Invalid
    case "try": return TokenType.Try
    case "global": return TokenType.Invalid // just use mut declaration in the root of the file, but they're not allowed in all modes
        // const/vars are the most abstract types, you can't even explicitly declare them
    case "const": return TokenType.Const
    case "var": return TokenType.Var
        // Do we really need const fields static other than static? (ie can be different per instance, but not modified afterwards)
        // The answer is probably yet, but perhaps static is not the right answer
        // how about this? if it's in the struct body, it is const, if it is in impl, it is static, just like functions\
        // or do we need mut function fields too? probably yes
    case "static": return TokenType.Invalid
    case:
    }
    return TokenType.Identifier
}

scan_tokens := proc(source: Str) returns TokenArray throws Str {
    mut tokens := TokenArray.new_dyn()

    eof_pos := source.len()
    mut pos := 0
    mut line := 0
    mut start_line_pos := 0

    while pos.lt(eof_pos) {
        start := pos
        if source.is_digit(pos) {
            while and(pos.lt(eof_pos), source.is_digit(pos)) {
                pos.inc()
            }
            // Look for a fractional part.
            if and(Str.eq(source.get_substr(pos, pos.add(1)), "."), source.is_digit(pos.add(1))) {
                pos.inc()
                while and(pos.lt(eof_pos), source.is_digit(pos)) {
                    pos.inc()
                }
            }
            scan_push_token(tokens, TokenType.Number, source.get_substr(start, pos), line, pos.sub(start_line_pos))
        } else {

            current_char := source.get_substr(pos, pos.add(1))
            switch current_char {
                // chars to ignore in this language:
            case " ":  pos.inc()
            case "\r": pos.inc()
            case "\t": pos.inc()
            case "\n":
                pos.inc()
                line.inc()
                start_line_pos = pos

                // open/close. left/right parentheses, craces and brackets
            case "(":
                scan_push_token(tokens, TokenType.LeftParen, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case ")":
                scan_push_token(tokens, TokenType.RightParen, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "{":
                scan_push_token(tokens, TokenType.LeftBrace, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "}":
                scan_push_token(tokens, TokenType.RightBrace, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "[":
                scan_push_token(tokens, TokenType.LeftBracket, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "]":
                scan_push_token(tokens, TokenType.RightBracket, current_char, line, pos.sub(start_line_pos))
                pos.inc()

            case ";":
                if Str.eq(";", source.get_substr(pos.add(1), pos.add(2))) {
                    scan_push_token(tokens, TokenType.DoubleSemicolon, ";;", line, pos.sub(start_line_pos))
                    pos = pos.add(2)
                } else {
                    scan_push_token(tokens, TokenType.Semicolon, ";", line, pos.sub(start_line_pos))
                    pos.inc()
                }
                // separator for optional type before the equal in declarations or args
            case ":":
                scan_push_token(tokens, TokenType.Colon, current_char, line, pos.sub(start_line_pos))
                pos.inc()
                // separator for args
            case ",":
                scan_push_token(tokens, TokenType.Comma, current_char, line, pos.sub(start_line_pos))
                pos.inc()

                // math
            case "+":
                scan_push_token(tokens, TokenType.Plus, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "-":
                scan_push_token(tokens, TokenType.Minus, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "*":
                scan_push_token(tokens, TokenType.Star, current_char, line, pos.sub(start_line_pos))
                pos.inc()

                // reserved for two chars in a row
            case ".":
                if Str.eq(".", source.get_substr(pos.add(1), pos.add(2))) {
                    scan_push_token(tokens, TokenType.DoubleDot, "..", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Dot, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case "=":
                if Str.eq("=", source.get_substr(pos.add(1), pos.add(2))) {
                    scan_push_token(tokens, TokenType.EqualEqual, "==", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Equal, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case "<":
                if Str.eq("=", source.get_substr(pos.add(1), pos.add(2))) {
                    scan_push_token(tokens, TokenType.LesserEqual, "<=", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Lesser, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case ">":
                if Str.eq("=", source.get_substr(pos.add(1), pos.add(2))) {
                    scan_push_token(tokens, TokenType.GreaterEqual, ">=", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Greater, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case "!":
                if Str.eq("=", source.get_substr(pos.add(1), pos.add(2))) {
                    scan_push_token(tokens, TokenType.NotEqual, "!=", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Not, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()

                // comments:
            case "#":
                pos.inc()
                while and(lt(pos.add(1), eof_pos), not(Str.eq("\n", source.get_substr(pos, pos.add(2))))) {
                    pos.inc()
                }
            case "/":
                if Str.eq("/", source.get_substr(pos.add(1), pos.add(2))) {
                    // Single-line comment
                    pos = pos.add(2)
                    while and(lt(pos.add(1), eof_pos), not(Str.eq("\n", source.get_substr(pos, pos.add(2))))) {
                        pos.inc()
                    }
                    pos.inc()
                } else if Str.eq("*", source.get_substr(pos.add(1), pos.add(2))) {
                    // Nested block comment
                    pos = pos.add(2)
                    depth := 1
                    mut done := false

                    while and(pos.lt(sub(eof_pos, 1)), not(done)) {
                        next_two := source.get_substr(pos, add(pos, 2))

                        if Str.eq(next_two, "/*") {
                            depth.inc()
                            pos = pos.add(2)
                        } else if next_two.eq("*/") {
                            depth.dec()
                            pos = pos.add(2)
                            if depth.eq(0) {
                                done = true
                            }
                        } else {
                            if Str.eq("\n", source.get_substr(pos, pos.add(1))) {
                                line.inc()
                                start_line_pos = pos.add(1)
                            }
                            pos.inc()
                        }
                    }

                    if not(done) {
                        scan_push_token(tokens, TokenType.UnterminatedComment, "/*", line, pos.sub(start_line_pos))
                    }

                } else {
                    scan_push_token(tokens, TokenType.Slash, current_char, line, pos.sub(start_line_pos))
                    pos.inc()
                }

                // literal strings
            case "\"":
                pos.inc()
                mut done := false

                while and(lt(pos.add(1), eof_pos), not(done)) {
                    current := source.get_substr(pos, pos.add(1))

                    if current.eq("\"") {
                        done = true
                    } else if current.eq("\\") {
                        pos.inc() // move past '\'
                        if lt(pos.add(1), eof_pos) {
                            esc := source.get_substr(pos, pos.add(1))
                            // accept known escapes
                            switch esc {
                            case "\"":
                            case "\\":
                            case "n":
                            case "r":
                            case "t":
                            case "0":
                                // valid escape, skip over it
                            case:
                                // unknown escape, still skip the character
                            }
                        }
                    }
                    pos.inc()
                }

                if not(done) {
                    token_str := source.get_substr(start, pos.add(1))
                    scan_push_token(tokens, TokenType.UnterminatedString, token_str, line, pos.sub(add(start_line_pos, 1)))
                } else {
                    token_str := source.get_substr(add(start, 1), pos)
                    scan_push_token(tokens, TokenType.String, token_str, line, pos.sub(add(start_line_pos, 1)))
                    pos.inc()
                }

                // Everything else must be reserved words, identifiers or invalid
            case:
                if source.is_id_start(pos) {
                    pos.inc()
                    while and(pos.lt(eof_pos), or(source.is_digit(pos), source.is_id_start(pos))) {
                        pos.inc()
                        // TODO fix literal strings with escape characters like in the rust implementation
                    }
                    pos.dec()
                    id_str := source.get_substr(start, pos.add(1))
                    scan_push_token(tokens, scan_reserved_words(id_str), id_str, line, pos.sub(add(start_line_pos, 1)))
                } else {
                    scan_push_token(tokens, TokenType.Invalid, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            } // switch
        } // else
        // println("Current char:", current_char)
    } // while
    scan_push_token(tokens, TokenType.Eof, "End of file", line, 0);

    catch (err: IndexOutOfBoundsError) {
        throw format(loc(), "scan_tokens: IndexOutOfBoundsError error thrown:\n", err.msg)
    }
    catch (err: AllocError) {
        throw format(loc(), "scan_tokens: AllocError error thrown:\n", err.msg)
    }
    return tokens
}

/** An interface for the Lexer */
Lexer := struct {
    mut path: Str = ""
    // mut tokens: TokenArray = TokenArray.new_dyn() // TODO re-enable new_dyn
    mut tokens: TokenArray = TokenArray.new(1) // TODO remove this
    mut current_token: I64 = 0

    new := proc(path: Str) returns Lexer {
        mut l := Lexer()
        l.path = path
        l.tokens = scan_tokens(readfile(path))
        return l
    }

    len := func(self: Lexer) returns I64 {
        self_tokens := self.tokens
        return self_tokens.len
    }

    is_eof := func(self: Lexer, offset: I64) returns Bool {
        self_tokens := self.tokens
        i := I64.add(self.current_token, offset)
        if gteq(i, self_tokens.len) {
            return true
        }
        t := Token()
        self_tokens.get(i, t)
        return TokenType.eq(t.token_type, TokenType.Eof)
    }

    get_token := func(self: Lexer, index: I64) returns Token {
        self_tokens := self.tokens
        t := Token()
        self_tokens.get(index, t)
        return t
    }

    peek := func(self: Lexer) returns Token {
        return self.tokens.get(self.current_token)
    }

    peek_ahead := func(self: Lexer, offset: I64) returns Token {
        index := I64.add(self.current_token, offset)
        if I64.gteq(index, self.tokens.len()) {
            t := self.peek()
            self.error(loc(), t, Str.fmt("Peek ahead by ", offset.to_str(), " is out of bounds"))
        }
        return self.tokens.get(index)
    }

    next := func(self: Lexer) returns Token {
        return self.peek_ahead(1)
    }

    previous := func(self: Lexer) returns Token {
        if I64.eq(self.current_token, 0) {
            t := self.peek()
            self.error(loc(), t, "No previous token (at position 0)")
        }
        index := I64.sub(self.current_token, 1)
        if I64.gteq(index, self.tokens.len()) {
            t := self.peek()
            self.error(loc(), t, "Previous token is out of bounds")
        }
        return self.tokens.get(index)
    }

    advance := func(mut self: Lexer, count: I64) {
        next_index := I64.add(self.current_token, count)
        if I64.gt(next_index, self.tokens.len()) {
            t := self.peek()
            self.error(
                loc(),
                t,
                Str.fmt("Advance by ", count.to_str(), " from ", self.current_token.to_str(),
                    " would exceed bounds (", I64.to_str(self.tokens.len()), " tokens total)")
            )
        }
        self.current_token = next_index
    }

    go_back := func(mut self: Lexer, count: I64) {
        if I64.gt(count, self.current_token) {
            t := self.peek()
            self.error(loc(), t,
                Str.fmt("Go back by ", count.to_str(), " from ", self.current_token.to_str(), " would underflow")
            )
        }
        self.current_token = I64.sub(self.current_token, count)
    }

    expect := func(mut self: Lexer, expected: TokenType) returns Token {
        p := self.peek()
        if TokenType.eq(p.token_type, expected) {
            self.advance(1)
            return p
        }
        self.error(loc(), p, Str.fmt(
            "Expected token '", token_type_to_str(expected), "', but found '", token_type_to_str(p.token_type), "'"))
        return Token() // unreachable
    }
}

print_lex_error := proc(path: Str, t: Token, mut errors_found: I64, msg: Str) {
    print(path, ":", I64.to_str(t.line), ":", I64.to_str(t.col), ": Lexical error ", I64.to_str(errors_found), ": ", msg)
    print(". Offending symbol: '", t.token_str, "'")
    print("\n")
    errors_found.inc()
}

print_if_lex_error := proc(path: Str, t: Token, mut errors_found: I64) {
    switch t.token_type {
    case TokenType.Invalid:
        print_lex_error(path, t, errors_found, "Invalid character")

    case TokenType.UnterminatedString:
        print_lex_error(path, t, errors_found, "Unterminated Str\nSuggestion: add missing '\"'")

    case TokenType.Const:
        print_lex_error(path, t, errors_found, "No need to use 'const', everything is const by default unless 'mut' is used")

    case TokenType.Var:
        print_lex_error(path, t, errors_found, "Keyword 'var' is not supported\nSuggestion: use 'mut' instead")

    case TokenType.Fn:
        print_lex_error(path, t, errors_found, "Keyword 'fn' is not supported\nSuggestion: use 'func' or 'proc' instead")

    case TokenType.DoubleSemicolon:
        print_lex_error(path, t, errors_found, "No need for ';;' (aka empty statements)\nSuggestion: try 'if true {}' instead, whatever you want that for")

    case TokenType.Plus:
        print_lex_error(path, t, errors_found, "Operator '+' is not supported yet\nSuggestion: use core func 'add' instead")

    case TokenType.Minus:
        print_lex_error(path, t, errors_found, "Operator '-' is not supported yet\nSuggestion: use core func 'sub' instead")

    case TokenType.Star:
        print_lex_error(path, t, errors_found, "Operator '*' is not supported yet\nSuggestion: use core func 'mul' instead")

    case TokenType.Slash:
        print_lex_error(path, t, errors_found, "Operator '/' is not supported yet\nSuggestion: use core func 'div' instead")

    case TokenType.EqualEqual:
        print_lex_error(path, t, errors_found, "Operator '==' is not supported yet\nSuggestion: use 'I64.eq' or 'Str.eq' instead")

    case TokenType.NotEqual:
        print_lex_error(path, t, errors_found, "Operator '!=' is not supported yet\nSuggestion: use core funcs 'not' and 'I64.eq'/'Str.eq' instead")

    case TokenType.Lesser:
        print_lex_error(path, t, errors_found, "Operator '<' is not supported yet\nSuggestion: use core func 'lt' instead")

    case TokenType.LesserEqual:
        print_lex_error(path, t, errors_found, "Operator '<=' is not supported yet\nSuggestion: use core func 'lteq' instead")

    case TokenType.Greater:
        print_lex_error(path, t, errors_found, "Operator '>' is not supported yet\nSuggestion: use core func 'gt' instead")

    case TokenType.GreaterEqual:
        print_lex_error(path, t, errors_found, "Operator '>=' is not supported yet\nSuggestion: use core func 'gteq' instead")

    case TokenType.Not:
        print_lex_error(path, t, errors_found, "Operator '!' is not supported yet\nSuggestion: use core func 'not' instead")

    case:
        // No error, do nothing
    }
}

print_lex_errors := proc(tokens: TokenArray, path: Str) returns I64 {
    mut errors_found := 0
    for i in 0..tokens.len {
        t := Token()
        tokens.get(i, t)
        print_if_lex_error(path, t, errors_found)
    }
    return errors_found
}

lexer_from_source := proc(path: Str, source: Str) returns Lexer throws Str {
    mut l := Lexer()
    l.path = path
    tokens := scan_tokens(source)
    // l.tokens = scan_tokens(source) // TODO fix access to fields of type array

    if I64.eq(tokens.len, 0) {
        throw path.format(":1:0: compiler ERROR: End of file not found.")
    }

    mut first := Token()
    tokens.get(0, first)
    switch first.token_type {
    case TokenType.Eof:
        throw path.format(":0:0: compiler ERROR: Nothing to be done")
    case:
    }

    mut errors := 0
    for i in 0..tokens.len {
        mut t := Token()
        tokens.get(i, t)
        print_if_lex_error(path, t, errors)
    }
    if gt(errors, 0) {
        throw format("Compiler errors: ", I64.to_str(errors), " lexical errors found")
    }

    return l
}
