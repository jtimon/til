mode lib

import("src/core/std")

LANG_NAME := "til"
DEBUG_COMPILER := true

TokenType := enum {
    // basic
    Eof,

    // Single-character tokens.
    Minus, Plus, Slash, Star,
    LeftParen, RightParen, LeftBrace, RightBrace, LeftBracket, RightBracket,
    Comma, Colon,

    // One or two character tokens.
    Dot, DoubleDot,
    Not, NotEqual,
    Equal, EqualEqual,
    Greater, GreaterEqual,
    Lesser, LesserEqual,
    Semicolon, DoubleSemicolon,

    // Literals.
    Identifier, String, Number,

    // Reserved words:
    Mut,

    // bool
    True,

    // type definition
    Struct, Enum,

    // function definition
    Returns, Throws,

    // flow control
    If, Else,
    While, For, In,
    Match, Switch, Default, Case,
    Return, Throw,
    Catch,

    // Special in this language:
    Mode,
    Func, Proc, Macro,
    FuncExt, ProcExt,

    // Errors (unsupported tokens)
    Const, Var,
    Fn, Function,
    Try,
    Let, Class,
    Invalid,
    UnterminatedString,
    UnterminatedComment,
}

Token := struct {
    mut token_type: TokenType = TokenType.Invalid
    mut token_str: Str = ""
    mut line: I64 = 0
    mut col: I64 = 0

    lang_error := func(self: Token, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
        return format(self.line.to_str(), ":", self.col.to_str(), ": ",
            LANG_NAME, " parse ERROR: ", msg, "\n",
            "Explanation: This is not your fault as a user, this is a bug in the language.")
    }

    todo_error := func(self: Token, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
        return format(self.line.to_str(), ":", self.col.to_str(), ": ",
            LANG_NAME, " parse ERROR: ", msg, "\n",
            "Explanation: Not implemented yet, this is a missing feature in the language.")
    }

    error := func(self: Token, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
        return format(self.line.to_str(), ":", self.col.to_str(), ": parse ERROR: ", msg)
    }
}

scan_push_token := proc(mut tokens: Vec, token_type: TokenType, token_str: Str, line: I64, col: I64) throws Str {
    mut t := Token()
    t.token_type = token_type
    t.token_str = token_str
    t.line = line
    t.col = col

    tokens.push(t)
    catch (err: AllocError) {
        throw format("scan_tokens: AllocError while pushing token: ", err.msg)
    }
    catch (err: FullError) {
        throw "scan_tokens: tokens array full"
    }
}

is_digit := func(source: Str, pos: I64) returns Bool throws IndexOutOfBoundsError, AllocError {
    switch source.get_substr(pos, pos.add(1)) {
    case "0".."9": return true
    case: return false
    }
}

is_id_start := func(source: Str, pos: I64) returns Bool throws IndexOutOfBoundsError, AllocError {
    // current_char := Str.get_substr(source, pos, add(pos, 1)) // TODO FIX throws check doesn't propagate for associated functions (unless called as methods)
    current_char := source.get_substr(pos, pos.add(1))
    switch current_char {
    case "a".."z": return true
    case "A".."Z": return true
    case "_": return true
    case:
    }
    return false
}

scan_reserved_words := func(identifier: Str) returns TokenType {
    switch identifier {
    case "mode": return TokenType.Mode
        // declaration/arg modifiers
    case "mut": return TokenType.Mut
        // bool literals
    case "true": return TokenType.True
        // core data types
    case "enum": return TokenType.Enum
    case "struct": return TokenType.Struct
        // function declaration
    case "returns": return TokenType.Returns
        // Anything that can be thrown must be explicitly declared in the function via 'throws', java style.
        // Except perhaps PanicException or something like that which can be implicit, but still allowed to documment redundantly
        // or perhaps not, for that may degenerate in an extra warning option
        // perhaps just force the user to explicitly catch and exit any potential panic from the callee
    case "throws": return TokenType.Throws // TODO implement
    case "func": return TokenType.Func
    case "proc": return TokenType.Proc
    case "macro": return TokenType.Macro // TODO implement for real once we compile
    case "ext_func": return TokenType.FuncExt // this has to link when we compile
    case "ext_proc": return TokenType.ProcExt // this has to link when we compile

        // control flow
    case "if": return TokenType.If
    case "else": return TokenType.Else
    case "while": return TokenType.While
    case "for": return TokenType.For
    case "in": return TokenType.In
    case "switch": return TokenType.Switch
    case "match": return TokenType.Match // TODO like switch but special for declarations/assignments
    case "case": return TokenType.Case
    case "default": return TokenType.Default // TODO currently using "case:", but "default:" is more traditional, grepable and overt
    case "return": return TokenType.Return
    case "throw": return TokenType.Throw
        // TODO throw should just act as a return that gets post-processed by the next catch or rethrown
    case "catch": return TokenType.Catch
        // or should 'try:' be optional?

        // Reserved forbidden/illegal words (intentionally unsupported reserved words)
        // TODO intentionally unsupport more reserved words
        // TODO nicer messages for forbidden words
    case "fn": return TokenType.Fn
    case "function": return TokenType.Function
    case "try": return TokenType.Try // TIL uses catch without try (forbidden for users from other languages)
    case "let": return TokenType.Let
    case "class": return TokenType.Class
    case "global": return TokenType.Invalid // just use mut declaration in the root of the file, but they're not allowed in all modes
        // const/vars are the most abstract types, you can't even explicitly declare them
    case "const": return TokenType.Const
    case "var": return TokenType.Var
        // Do we really need const fields static other than static? (ie can be different per instance, but not modified afterwards)
        // The answer is probably yet, but perhaps static is not the right answer
        // how about this? if it's in the struct body, it is const, if it is in impl, it is static, just like functions\
        // or do we need mut function fields too? probably yes
    case "static": return TokenType.Invalid
    case:
    }
    return TokenType.Identifier
}

scan_tokens := proc(source: Str) returns Vec throws Str, AllocError {
    mut tokens := Vec.new(Token)

    eof_pos := source.len()
    mut pos := 0
    mut line := 1
    mut start_line_pos := sub(0, 1)

    while pos.lt(eof_pos) {
        start := pos
        if source.is_digit(pos) {
            mut continue_scanning := true
            while continue_scanning {
                pos.inc()
                if pos.lt(eof_pos) {
                    if not(source.is_digit(pos)) {
                        continue_scanning = false
                    }
                } else {
                    continue_scanning = false
                }
            }
            // Look for a fractional part.
            if pos.lt(eof_pos) {
                if source.get_substr(pos, pos.add(1)).eq(".") {
                    if pos.add(1).lt(eof_pos) {
                        if source.is_digit(pos.add(1)) {
                            pos.inc()
                            mut continue_scanning2 := true
                            while continue_scanning2 {
                                pos.inc()
                                if pos.lt(eof_pos) {
                                    if not(source.is_digit(pos)) {
                                        continue_scanning2 = false
                                    }
                                } else {
                                    continue_scanning2 = false
                                }
                            }
                        }
                    }
                }
            }
            scan_push_token(tokens, TokenType.Number, source.get_substr(start, pos), line, start.sub(start_line_pos))
        } else {

            current_char := source.get_substr(pos, pos.add(1))
            switch current_char {
                // chars to ignore in this language (increment pos, no token, continue loop):
            case " ":  pos.inc()
            case "\r": pos.inc()
            case "\t": pos.inc()
            case "\n":
                pos.inc()
                line.inc()
                start_line_pos = pos.sub(1)

                // open/close. left/right parentheses, craces and brackets
            case "(":
                scan_push_token(tokens, TokenType.LeftParen, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case ")":
                scan_push_token(tokens, TokenType.RightParen, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "{":
                scan_push_token(tokens, TokenType.LeftBrace, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "}":
                scan_push_token(tokens, TokenType.RightBrace, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "[":
                scan_push_token(tokens, TokenType.LeftBracket, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "]":
                scan_push_token(tokens, TokenType.RightBracket, current_char, line, pos.sub(start_line_pos))
                pos.inc()

            case ";":
                if source.get_substr(pos.add(1), pos.add(2)).eq(";") {
                    scan_push_token(tokens, TokenType.DoubleSemicolon, ";;", line, pos.sub(start_line_pos))
                    pos = pos.add(2)
                } else {
                    scan_push_token(tokens, TokenType.Semicolon, ";", line, pos.sub(start_line_pos))
                    pos.inc()
                }
                // separator for optional type before the equal in declarations or args
            case ":":
                scan_push_token(tokens, TokenType.Colon, current_char, line, pos.sub(start_line_pos))
                pos.inc()
                // separator for args
            case ",":
                scan_push_token(tokens, TokenType.Comma, current_char, line, pos.sub(start_line_pos))
                pos.inc()

                // math
            case "+":
                scan_push_token(tokens, TokenType.Plus, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "-":
                scan_push_token(tokens, TokenType.Minus, current_char, line, pos.sub(start_line_pos))
                pos.inc()
            case "*":
                scan_push_token(tokens, TokenType.Star, current_char, line, pos.sub(start_line_pos))
                pos.inc()

                // reserved for two chars in a row
            case ".":
                if source.get_substr(pos.add(1), pos.add(2)).eq(".") {
                    scan_push_token(tokens, TokenType.DoubleDot, "..", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Dot, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case "=":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    scan_push_token(tokens, TokenType.EqualEqual, "==", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Equal, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case "<":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    scan_push_token(tokens, TokenType.LesserEqual, "<=", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Lesser, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case ">":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    scan_push_token(tokens, TokenType.GreaterEqual, ">=", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Greater, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            case "!":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    scan_push_token(tokens, TokenType.NotEqual, "!=", line, pos.sub(start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(tokens, TokenType.Not, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()

                // comments:
            case "#":
                pos.inc()
                while and(lt(pos.add(1), eof_pos), not(source.get_substr(pos, pos.add(1)).eq("\n"))) {
                    pos.inc()
                }
            case "/":
                if source.get_substr(pos.add(1), pos.add(2)).eq("/") {
                    // Single-line comment
                    pos = pos.add(2)
                    while and(lt(pos.add(1), eof_pos), not(source.get_substr(pos, pos.add(1)).eq("\n"))) {
                        pos.inc()
                    }
                    pos.inc()
                } else if source.get_substr(pos.add(1), pos.add(2)).eq("*") {
                    // Nested block comment
                    pos = pos.add(2)
                    depth := 1
                    mut done := false

                    while and(pos.lt(sub(eof_pos, 1)), not(done)) {
                        next_two := source.get_substr(pos, pos.add(2))

                        if next_two.eq("/*") {
                            depth.inc()
                            pos = pos.add(2)
                        } else if next_two.eq("*/") {
                            depth.dec()
                            pos = pos.add(2)
                            if depth.eq(0) {
                                done = true
                            }
                        } else {
                            if source.get_substr(pos, pos.add(1)).eq("\n") {
                                line.inc()
                                start_line_pos = pos
                            }
                            pos.inc()
                        }
                    }

                    if not(done) {
                        scan_push_token(tokens, TokenType.UnterminatedComment, "/*", line, pos.sub(start_line_pos))
                    }

                } else {
                    scan_push_token(tokens, TokenType.Slash, current_char, line, pos.sub(start_line_pos))
                    pos.inc()
                }

                // literal strings
            case "\"":
                pos.inc()
                mut done := false
                mut lit_string := ""

                while and(lt(pos, eof_pos), not(done)) {
                    current := source.get_substr(pos, pos.add(1))

                    if current.eq("\"") {
                        done = true
                    } else if current.eq("\\") {
                        pos.inc() // move past '\'
                        if lt(pos.add(1), eof_pos) {
                            esc := source.get_substr(pos, pos.add(1))
                            // Process escape sequences
                            switch esc {
                            case "\"":
                                lit_string = format(lit_string, "\"")
                            case "\\":
                                lit_string = format(lit_string, "\\")
                            case "n":
                                lit_string = format(lit_string, "\n")
                            case "r":
                                lit_string = format(lit_string, "\r")
                            case "t":
                                lit_string = format(lit_string, "\t")
                            case "0":
                                lit_string = format(lit_string, "\0")
                            case:
                                // Unknown escape: keep backslash and character
                                lit_string = format(lit_string, "\\", esc)
                            }
                        }
                    } else {
                        lit_string = format(lit_string, current)
                    }
                    pos.inc()
                }

                if not(done) {
                    token_str := source.get_substr(start, pos.add(1))
                    scan_push_token(tokens, TokenType.UnterminatedString, token_str, line, start.sub(start_line_pos))
                } else {
                    scan_push_token(tokens, TokenType.String, lit_string, line, start.sub(start_line_pos))
                    pos.inc()
                }

                // Everything else must be reserved words, identifiers or invalid
            case:
                if source.is_id_start(pos) {
                    pos.inc()
                    mut continue_id_scan := true
                    while continue_id_scan {
                        if pos.lt(eof_pos) {
                            if or(source.is_digit(pos), source.is_id_start(pos)) {
                                pos.inc()
                            } else {
                                continue_id_scan = false
                            }
                        } else {
                            continue_id_scan = false
                        }
                    }
                    pos.dec()
                    id_str := source.get_substr(start, pos.add(1))
                    scan_push_token(tokens, scan_reserved_words(id_str), id_str, line, start.sub(start_line_pos))
                } else {
                    scan_push_token(tokens, TokenType.Invalid, current_char, line, pos.sub(start_line_pos))
                }
                pos.inc()
            } // switch
        } // else
        // println("Current char:", current_char)
    } // while
    scan_push_token(tokens, TokenType.Eof, "End of file", line, 0);

    catch (err: IndexOutOfBoundsError) {
        throw "scan_tokens: IndexOutOfBoundsError"
    }
    catch (err: I64_OverflowError) {
        throw "scan_tokens: I64_OverflowError"
    }
    catch (err: AllocError) {
        throw "scan_tokens: AllocError"
    }
    return tokens
}

/** An interface for the Lexer */
Lexer := struct {
    mut path: Str = ""
    mut tokens: Vec = Vec.new(Token)
    mut current: I64 = 0

    new := proc(path: Str) returns Lexer throws Str {
        mut l := Lexer()
        l.path = path
        l.tokens = scan_tokens(readfile(path))
        return l
    }

    new_from_src := proc(source: Str) returns Lexer throws Str {
        mut l := Lexer()
        l.path = "<test>"
        l.tokens = scan_tokens(source)
        return l
    }

    len := func(self: Lexer) returns I64 {
        return self.tokens.len
    }

    is_eof := func(self: Lexer, offset: I64) returns Bool throws IndexOutOfBoundsError {
        current := self.current
        i := add(current, offset)
        if gteq(i, self.tokens.len) {
            return true
        }
        mut t := Token()
        self.tokens.get(i, t)
        // Compare TokenType using switch
        mut is_eof := false
        switch t.token_type {
        case TokenType.Eof:
            is_eof = true
        case:
            is_eof = false
        }
        return is_eof
    }

    get_token := func(self: Lexer, index: I64) returns Token throws IndexOutOfBoundsError {
        mut t := Token()
        self.tokens.get(index, t)
        return t
    }

    peek := func(self: Lexer) returns Token throws IndexOutOfBoundsError {
        mut t := Token()
        self.tokens.get(self.current, t)
        return t
    }

    peek_ahead := func(self: Lexer, offset: I64) returns Token throws IndexOutOfBoundsError, I64_OverflowError, AllocError {
        index := add(self.current, offset)
        if gteq(index, self.tokens.len) {
            mut err := IndexOutOfBoundsError()
            err.msg = format("Peek ahead by ", I64.to_str(offset), " is out of bounds")
            throw err
        }
        mut t := Token()
        self.tokens.get(index, t)
        return t
    }

    next := func(self: Lexer) returns Token throws IndexOutOfBoundsError, I64_OverflowError, AllocError {
        return self.peek_ahead(1)
    }

    previous := func(self: Lexer) returns Token throws IndexOutOfBoundsError {
        if eq(self.current, 0) {
            mut err := IndexOutOfBoundsError()
            err.msg = "No previous token (at position 0)"
            throw err
        }
        index := sub(self.current, 1)
        if gteq(index, self.tokens.len) {
            mut err := IndexOutOfBoundsError()
            err.msg = "Previous token is out of bounds"
            throw err
        }
        mut t := Token()
        self.tokens.get(index, t)
        return t
    }

    advance := func(mut self: Lexer, count: I64) throws IndexOutOfBoundsError, I64_OverflowError, AllocError {
        next_index := add(self.current, count)
        if gt(next_index, self.tokens.len) {
            mut err := IndexOutOfBoundsError()
            err.msg = format("Advance by ", I64.to_str(count), " from ", I64.to_str(self.current),
                    " would exceed bounds (", I64.to_str(self.tokens.len), " tokens total)")
            throw err
        }
        self.current = next_index
    }

    go_back := func(mut self: Lexer, count: I64) throws IndexOutOfBoundsError, I64_OverflowError, AllocError {
        if gt(count, self.current) {
            mut err := IndexOutOfBoundsError()
            err.msg = format("Go back by ", I64.to_str(count), " from ", I64.to_str(self.current), " would underflow")
            throw err
        }
        self.current = sub(self.current, count)
    }

    expect := func(mut self: Lexer, expected: TokenType) returns Token throws IndexOutOfBoundsError, Str, I64_OverflowError, AllocError {
        p := self.peek()
        switch p.token_type {
        case expected:
            self.advance(1)
            return p
        case:
            throw format(
                "Expected token '", token_type_to_str(expected), "', but found '", p.token_str, "'")
        }
    }

}

// Workaround for Bug #8: Uncaptured return values propagate up call stack
// This standalone function allows callers to capture the return value with _ := lexer_expect(...)
lexer_expect := func(mut lexer: Lexer, expected: TokenType) returns Token throws IndexOutOfBoundsError, Str, I64_OverflowError, AllocError {
    p := lexer.peek()
    switch p.token_type {
    case expected:
        lexer.advance(1)
        return p
    case:
        throw format(
            "Expected token '", token_type_to_str(expected), "', but found '", p.token_str, "'")
    }
}

token_type_to_str := func(tt: TokenType) returns Str {
    switch tt {
    case TokenType.If: return "if"
    case TokenType.Else: return "else"
    case TokenType.While: return "while"
    case TokenType.For: return "for"
    case TokenType.In: return "in"
    case TokenType.Return: return "return"
    case TokenType.Throw: return "throw"
    case TokenType.Catch: return "catch"
    case TokenType.Struct: return "struct"
    case TokenType.Enum: return "enum"
    case TokenType.Mode: return "mode"
    case TokenType.Func: return "func"
    case TokenType.Proc: return "proc"
    case TokenType.Macro: return "macro"
    case TokenType.FuncExt: return "ext_func"
    case TokenType.ProcExt: return "ext_proc"
    case TokenType.Returns: return "returns"
    case TokenType.Throws: return "throws"
    case TokenType.Match: return "match"
    case TokenType.Switch: return "switch"
    case TokenType.Default: return "default"
    case TokenType.Case: return "case"
    case TokenType.Mut: return "mut"
    case TokenType.LeftParen: return "("
    case TokenType.RightParen: return ")"
    case TokenType.LeftBrace: return "{"
    case TokenType.RightBrace: return "}"
    case TokenType.LeftBracket: return "["
    case TokenType.RightBracket: return "]"
    case TokenType.Comma: return ","
    case TokenType.Dot: return "."
    case TokenType.DoubleDot: return ".."
    case TokenType.Colon: return ":"
    case TokenType.Semicolon: return ";"
    case TokenType.DoubleSemicolon: return ";;"
    case TokenType.Equal: return "="
    case TokenType.EqualEqual: return "=="
    case TokenType.Not: return "!"
    case TokenType.NotEqual: return "!="
    case TokenType.Lesser: return "<"
    case TokenType.LesserEqual: return "<="
    case TokenType.Greater: return ">"
    case TokenType.GreaterEqual: return ">="
    case TokenType.Plus: return "+"
    case TokenType.Minus: return "-"
    case TokenType.Star: return "*"
    case TokenType.Slash: return "/"
    case TokenType.Identifier: return "identifier"
    case TokenType.String: return "string"
    case TokenType.Number: return "number"
    case TokenType.True: return "true"
    case TokenType.Eof: return "eof"
    case: return "invalid"
    }
}

print_lex_error := proc(path: Str, t: Token, mut errors_found: I64, msg: Str) {
    print(path, ":", I64.to_str(t.line), ":", I64.to_str(t.col), ": Lexical error ", I64.to_str(errors_found), ": ", msg)
    print(". Offending symbol: '", t.token_str, "'")
    print("\n")
    errors_found.inc()
}

print_if_lex_error := proc(path: Str, t: Token, mut errors_found: I64) {
    switch t.token_type {
    case TokenType.Invalid:
        print_lex_error(path, t, errors_found, "Invalid character")

    case TokenType.UnterminatedString:
        print_lex_error(path, t, errors_found, "Unterminated Str\nSuggestion: add missing '\"'")

    case TokenType.UnterminatedComment:
        print_lex_error(path, t, errors_found, "Unterminated comment\nSuggestion: add missing '*/'")

    case TokenType.Const:
        print_lex_error(path, t, errors_found, "No need to use 'const', everything is const by default unless 'mut' is used")

    case TokenType.Var:
        print_lex_error(path, t, errors_found, "Keyword 'var' is not supported\nSuggestion: use 'mut' instead")

    case TokenType.Fn:
        print_lex_error(path, t, errors_found, "Keyword 'fn' is not supported\nSuggestion: use 'func' or 'proc' instead")

    case TokenType.Function:
        print_lex_error(path, t, errors_found, "Keyword 'function' is not supported\nSuggestion: use 'func' or 'proc' instead")

    case TokenType.Try:
        print_lex_error(path, t, errors_found, "Keyword 'try' is not supported\nSuggestion: TIL uses 'catch' blocks without 'try' (just use throw/catch)")

    case TokenType.Let:
        print_lex_error(path, t, errors_found, "Keyword 'let' is not supported\nSuggestion: use ':=' for declaration")

    case TokenType.Class:
        print_lex_error(path, t, errors_found, "Keyword 'class' is not supported\nSuggestion: use 'struct' instead")

    case TokenType.DoubleSemicolon:
        print_lex_error(path, t, errors_found, "No need for ';;' (aka empty statements)\nSuggestion: try 'if true {}' instead, whatever you want that for")

    case TokenType.Plus:
        print_lex_error(path, t, errors_found, "Operator '+' is not supported yet\nSuggestion: use core func 'add' instead")

    case TokenType.Minus:
        print_lex_error(path, t, errors_found, "Operator '-' is not supported yet\nSuggestion: use core func 'sub' instead")

    case TokenType.Star:
        print_lex_error(path, t, errors_found, "Operator '*' is not supported yet\nSuggestion: use core func 'mul' instead")

    case TokenType.Slash:
        print_lex_error(path, t, errors_found, "Operator '/' is not supported yet\nSuggestion: use core func 'div' instead")

    case TokenType.EqualEqual:
        print_lex_error(path, t, errors_found, "Operator '==' is not supported yet\nSuggestion: use 'I64.eq' or 'Str.eq' instead")

    case TokenType.NotEqual:
        print_lex_error(path, t, errors_found, "Operator '!=' is not supported yet\nSuggestion: use core funcs 'not' and 'I64.eq'/'Str.eq' instead")

    case TokenType.Lesser:
        print_lex_error(path, t, errors_found, "Operator '<' is not supported yet\nSuggestion: use core func 'lt' instead")

    case TokenType.LesserEqual:
        print_lex_error(path, t, errors_found, "Operator '<=' is not supported yet\nSuggestion: use core func 'lteq' instead")

    case TokenType.Greater:
        print_lex_error(path, t, errors_found, "Operator '>' is not supported yet\nSuggestion: use core func 'gt' instead")

    case TokenType.GreaterEqual:
        print_lex_error(path, t, errors_found, "Operator '>=' is not supported yet\nSuggestion: use core func 'gteq' instead")

    case TokenType.Not:
        print_lex_error(path, t, errors_found, "Operator '!' is not supported yet\nSuggestion: use core func 'not' instead")

    case:
        // No error, do nothing
    }
}

print_lex_errors := proc(tokens: Array, path: Str) returns I64 throws IndexOutOfBoundsError {
    mut errors_found := 0
    for i in 0..tokens.len {
        mut t := Token()
        tokens.get(i, t)
        print_if_lex_error(path, t, errors_found)
    }
    return errors_found
}

lexer_from_source := proc(path: Str, source: Str) returns Lexer throws Str, IndexOutOfBoundsError {
    mut l := Lexer()
    l.path = path
    tokens := scan_tokens(source)
    l.tokens = tokens

    if I64.eq(tokens.len, 0) {
        throw path.format(":1:0: compiler ERROR: End of file not found.")
    }

    mut first := Token()
    tokens.get(0, first)
    switch first.token_type {
    case TokenType.Eof:
        throw path.format(":0:0: compiler ERROR: Nothing to be done")
    case:
    }

    errors := print_lex_errors(tokens, path)
    if gt(errors, 0) {
        throw format("Compiler errors: ", I64.to_str(errors), " lexical errors found")
    }

    return l
}
