mode lib

import("src/core/lexer")

INFER_TYPE := "auto"

FunctionType := enum {
    FTFunc,
    FTProc,
    FTMacro,
    FTFuncExt,
    FTProcExt,
}

TTypeDef := enum {
    TEnumDef,
    TStructDef,
}

ValueType := enum {
    TFunction: FunctionType,
    TType: TTypeDef,
    TCustom: Str,
    TMulti: Str,
}

SEnumDef := struct {
    mut enum_map: Map = Map()  // Map<Str, ValueType> - variant name to optional payload type
}

Declaration := struct {
    mut name: Str = ""
    mut value_type: ValueType = ValueType.TCustom("")
    mut is_mut: Bool = false
}

// TODO: PatternInfo is a workaround for homogeneity with TIL's lack of tuple syntax
// Once TIL supports tuple notation like (Str, Str), this can be replaced with:
// Pattern(String, String)  // Pattern(variant_name, binding_var)
PatternInfo := struct {
    mut variant_name: Str = ""
    mut binding_var: Str = ""
}

SFuncDef := struct {
    mut function_type: FunctionType = FunctionType.FTFunc
    mut args: Array = Array()              // Array of Declaration
    mut return_types: Array = Array()      // Array of ValueType - "returns" conflicts with TIL keyword
    mut throw_types: Array = Array()       // Array of ValueType - "throws" conflicts with TIL keyword
    mut body: Array = Array()              // Array of Expr
}
sfuncdef_is_proc := func(self: SFuncDef) returns Bool {
    ft := self.function_type
    switch ft {
    case FunctionType.FTProc:
        return true
    case FunctionType.FTProcExt:
        return true
    case:
        return false
    }
}

sfuncdef_is_ext := func(self: SFuncDef) returns Bool {
    ft := self.function_type
    switch ft {
    case FunctionType.FTFuncExt:
        return true
    case FunctionType.FTProcExt:
        return true
    case:
        return false
    }
}

SStructDef := struct {
    mut members: Array = Array()           // Array of (Str, Declaration) - field name, declaration
    mut default_values: Map = Map()        // Map<Str, Expr> - field name to default value expression
}

Literal := enum {
    Number: Str,  // TODO support more kinds of numbers
    Str: Str,
    Bool: Str,
    List: Str,  // TODO You can call it tupple too. who cares? it's not even tested yet, just parsed
}

NodeType := enum {
    Body,
    LLiteral: Literal,
    FCall,
    Identifier: Str,
    Declaration: Declaration,
    Assignment: Str,
    FuncDef: SFuncDef,
    EnumDef: SEnumDef,
    StructDef: SStructDef,
    Return,
    Throw,
    Catch,
    If,
    While,
    Switch,
    DefaultCase,
    Range,
    Pattern: PatternInfo,  // Pattern matching for switch case with payload extraction
}

Expr := struct {
    mut node_type: NodeType = NodeType.Body
    mut params: Array = Array()
    mut line: I64 = 0
    mut col: I64 = 0
}
expr_new_parse := func(node_type: NodeType, token: Token, params: Array) returns Expr {
    mut e := Expr()
    e.node_type = node_type
    e.params = params
    e.line = token.line
    e.col = token.col
    return e
}

expr_new_explicit := func(node_type: NodeType, params: Array, line: I64, col: I64) returns Expr {
    mut e := Expr()
    e.node_type = node_type
    e.params = params
    e.line = line
    e.col = col
    return e
}

expr_new_clone := func(node_type: NodeType, e: Expr, params: Array) returns Expr {
    return expr_new_explicit(node_type, params, e.line, e.col)
}

is_literal := func(t: Token) returns Bool {
    tt := t.token_type
    switch tt {
    case TokenType.String:
        return true
    case TokenType.Number:
        return true
    case TokenType.True:
        return true
    case:
        return false
    }
}

value_type_to_str := func(arg_type: ValueType) returns Str {
    switch arg_type {
    case ValueType.TType(TTypeDef.TEnumDef):
        return "enum"
    case ValueType.TType(TTypeDef.TStructDef):
        return "struct"
    case ValueType.TFunction(FunctionType.FTFunc):
        return "func"
    case ValueType.TFunction(FunctionType.FTFuncExt):
        return "func"
    case ValueType.TFunction(FunctionType.FTProc):
        return "proc"
    case ValueType.TFunction(FunctionType.FTProcExt):
        return "proc"
    case ValueType.TFunction(FunctionType.FTMacro):
        return "macro"
    case ValueType.TMulti:
        // TODO: need to extract string payload
        return "multi"
    case ValueType.TCustom:
        // TODO: need to extract string payload
        return "custom"
    case:
        return "unknown"
    }
}

str_to_value_type := func(arg_type: Str) returns ValueType {
    if arg_type.eq("func") {
        return ValueType.TFunction(FunctionType.FTFunc)
    }
    if arg_type.eq("proc") {
        return ValueType.TFunction(FunctionType.FTProc)
    }
    if arg_type.eq("macro") {
        return ValueType.TFunction(FunctionType.FTMacro)
    }
    if arg_type.eq("enum") {
        return ValueType.TType(TTypeDef.TEnumDef)
    }
    if arg_type.eq("struct") {
        return ValueType.TType(TTypeDef.TStructDef)
    }
    // Default: custom type
    return ValueType.TCustom(arg_type)
}

ModeDef := struct {
    mut name: Str = ""
    mut allows_procs: Bool = false
    mut allows_base_mut: Bool = false
    mut allows_base_calls: Bool = false
    mut allows_base_anything: Bool = false
    mut needs_main_proc: Bool = false
    mut imports: Array = Array()  // Array of Str
}

can_be_imported := func(mode_def: ModeDef) returns Bool {
    if mode_def.needs_main_proc { return false }  // TODO think harder, why not?
    if mode_def.allows_base_mut { return false }
    if mode_def.allows_base_calls { return false }
    if mode_def.allows_base_anything { return false }
    return true
}

mode_from_name := func(mode_name: Str) returns ModeDef throws Str, FullError, AllocError {
    if mode_name.eq("lib") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = false
        m.allows_base_mut = false
        m.allows_base_anything = false
        m.needs_main_proc = false
        m.imports = Array()
        return m
    }
    if mode_name.eq("pure") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = false
        m.allows_base_calls = false
        m.allows_base_mut = false
        m.allows_base_anything = false
        m.needs_main_proc = false
        m.imports = Array()
        return m
    }
    if mode_name.eq("script") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = true
        m.allows_base_mut = true
        m.allows_base_anything = true
        m.needs_main_proc = false
        m.imports = Array()
        return m
    }
    if mode_name.eq("safe_script") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = true
        m.allows_base_mut = true
        m.allows_base_anything = true
        m.needs_main_proc = false
        m.imports = Array()
        return m
    }
    if mode_name.eq("cli") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = false
        m.allows_base_mut = true
        m.allows_base_anything = false
        m.needs_main_proc = true
        m.imports = Array()
        return m
    }
    if mode_name.eq("test") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = true
        m.allows_base_mut = true
        m.allows_base_anything = false
        m.needs_main_proc = false
        mut test_imports := Array()
        test_imports.push("src/core/modes/test")
        m.imports = test_imports
        return m
    }

    throw format("0:0: ", LANG_NAME, " interpreter implementation doesn't support mode '", mode_name, "'")
}

parse_mode := func(path: Str, mut lexer: Lexer) returns ModeDef throws Str, IndexOutOfBoundsError, I64_OverflowError, AllocError, FullError {
    lexer_expect(lexer, TokenType.Mode)  // Add one for mode

    t := lexer.peek()
    tt := t.token_type
    switch tt {
    case TokenType.Identifier:
        // continue
    case:
        throw "0:0: Expected identifier after 'mode'"
    }
    mode_name := t.token_str
    m := mode_from_name(mode_name)

    m_name := m.name
    if m_name.eq("safe_script") {
        throw format(path, ":0:0: mode '", m.name, "' is not properly supported in '", LANG_NAME, "' yet. Try mode 'script' instead")
    }

    lexer_expect(lexer, TokenType.Identifier)  // Add one for the identifier of the mode
    return m
}

parse_literal := func(mut lexer: Lexer, t: Token) returns Expr throws Str, AllocError, FullError {
    params := Array()
    tt := t.token_type
    node_type := NodeType.Body  // Default, will be overwritten

    switch tt {
    case TokenType.String:
        node_type = NodeType.LLiteral(Literal.Str(t.token_str))
    case TokenType.Number:
        // TODO: parse and validate number
        node_type = NodeType.LLiteral(Literal.Number(t.token_str))
    case TokenType.True:
        node_type = NodeType.LLiteral(Literal.Bool(t.token_str))
    case:
        throw format(t.line.to_str(), ":", t.col.to_str(), ": Trying to parse a token that's not a literal as a literal, found '", token_type_to_str(t.token_type), "'.")
    }

    e := expr_new_parse(node_type, t, params)
    lexer_advance(lexer, 1)
    return e
}

parse_list := func(mut lexer: Lexer) returns Expr throws Str, I64_OverflowError, AllocError, FullError, IndexOutOfBoundsError {
    mut rightparent_found := false
    mut params := Array()
    initial_current := lexer.current
    lexer_expect(lexer, TokenType.LeftParen)
    mut list_t := lexer.peek()
    mut expect_comma := false

    while not(lexer_is_eof(lexer, 0)).and(not(rightparent_found)) {
        lt := list_t.token_type
        switch lt {
        case TokenType.RightParen:
            lexer_expect(lexer, TokenType.RightParen)
            rightparent_found = true
        case TokenType.Comma:
            if expect_comma {
                lexer_expect(lexer, TokenType.Comma)
                expect_comma = false
                list_t = lexer.peek()
            } else {
                throw format(list_t.line.to_str(), ":", list_t.col.to_str(), ": Unexpected ','.")
            }
        case:
            if expect_comma {
                throw format(list_t.line.to_str(), ":", list_t.col.to_str(), ": Expected ')' or ',', found '", token_type_to_str(list_t.token_type), "'.")
            }
            expect_comma = true
            prim := parse_primary(lexer)
            params.push(prim)
            list_t = lexer.peek()
        }
    }

    lt := list_t.token_type
    switch lt {
    case TokenType.RightParen:
        // TODO properly parse lists besides function definition arguments
        initial_token := lexer_get_token(lexer, initial_current)
        return expr_new_parse(NodeType.LLiteral(Literal.List("")), initial_token, params)
    case:
        throw format(list_t.line.to_str(), ":", list_t.col.to_str(), ": Expected closing parentheses.")
    }
}

parse_assignment := func(mut lexer: Lexer, t: Token, name: Str) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    lexer_expect(lexer, TokenType.Equal)
    mut params := Array()
    params.push(parse_primary(lexer))
    return expr_new_parse(NodeType.Assignment(name), t, params)
}

parse_func_proc_args := func(mut lexer: Lexer) returns Array throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    lexer_expect(lexer, TokenType.LeftParen)
    mut rightparent_found := false
    mut args := Array()  // Array of Declaration
    mut t := lexer.peek()
    mut expect_comma := false
    mut expect_colon := false
    mut expect_name := true
    mut is_variadic := false
    mut arg_name := "unnamed"
    mut is_mut := false

    while not(lexer_is_eof(lexer, 0)).and(not(rightparent_found)) {
        tt := t.token_type
        switch tt {
        case TokenType.RightParen:
            rightparent_found = true
            if expect_colon {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected ': Type' after arg name '", arg_name, "' before ')'.")
            }
            lexer_advance(lexer, 1)
        case TokenType.Comma:
            if expect_colon {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected ': Type' after arg name '", arg_name, "', but found ','.")
            }
            if expect_name {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected arg name before ','.")
            }
            if expect_comma {
                expect_comma = false
                expect_colon = false
                expect_name = true
                is_mut = false
                lexer_expect(lexer, TokenType.Comma)
                t = lexer.peek()
            } else {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Unexpected ','.")
            }
        case TokenType.Colon:
            if expect_colon {
                expect_colon = false
                expect_name = false
                expect_comma = false
                lexer_advance(lexer, 1)
                t = lexer.peek()
                tt2 := t.token_type
                switch tt2 {
                case TokenType.Identifier:
                    // continue
                case TokenType.DoubleDot:
                    // continue
                case:
                    throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected type after '", arg_name, ":', but found '", t.token_str, "'.")
                }
            } else {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Unexpected ':'.")
            }
        case TokenType.DoubleDot:
            if expect_colon {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected ': Type' after arg name '", arg_name, "', but found '..'.")
            }
            if expect_comma {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected ',', found '", token_type_to_str(t.token_type), "'.")
            }
            if expect_name {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected arg name, found '", token_type_to_str(t.token_type), "'.")
            }
            is_variadic = true
            lexer_advance(lexer, 1)
            t = lexer.peek()
        case TokenType.Identifier:
            if expect_colon {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected ': Type' after arg name '", arg_name, "', but found '", t.token_str, "'.")
            }
            if expect_comma {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected ',', found identifier '", t.token_str, "'.")
            }
            if expect_name {
                arg_name = t.token_str
                expect_colon = true
                expect_name = false
            } else {
                mut decl := Declaration()
                decl.name = arg_name
                if is_variadic {
                    decl.value_type = ValueType.TMulti(t.token_str)
                    is_variadic = false
                } else {
                    decl.value_type = str_to_value_type(t.token_str)
                }
                decl.is_mut = is_mut
                args.push(decl)
                expect_comma = true
                is_mut = false
            }
            lexer_advance(lexer, 1)
            t = lexer.peek()
        case TokenType.Mut:
            if not(expect_name) {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Unexpected 'mut' in argument list.")
            }
            is_mut = true
            lexer_advance(lexer, 1)
            t = lexer.peek()
        case:
            throw format(t.line.to_str(), ":", t.col.to_str(), ": Unexpected '", token_type_to_str(t.token_type), "' in func/proc args.")
        }
    }

    tt := t.token_type
    switch tt {
    case TokenType.RightParen:
        return args
    case:
        throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected closing parentheses.")
    }
}

func_proc_returns := func(mut lexer: Lexer) returns Array throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    mut end_found := false
    mut return_types := Array()  // Array of ValueType
    mut t := lexer.peek()
    lexer_advance(lexer, 1)
    tt := t.token_type
    if not(tt.eq(TokenType.Returns)) {
        return return_types
    }
    t = lexer.peek()
    mut expect_comma := false

    while not(lexer_is_eof(lexer, 0)).and(not(end_found)) {
        tt2 := t.token_type
        switch tt2 {
        case TokenType.Throws:
            end_found = true
            lexer_advance(lexer, 1)
        case TokenType.LeftBrace:
            end_found = true
            lexer_advance(lexer, 1)
        case TokenType.Semicolon:
            end_found = true
            lexer_advance(lexer, 1)
        case TokenType.Comma:
            if expect_comma {
                lexer_expect(lexer, TokenType.Comma)
                expect_comma = false
                t = lexer.peek()
            } else {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Unexpected ','.")
            }
        case TokenType.Identifier:
            if expect_comma {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected ',', found '", token_type_to_str(t.token_type), "'.")
            }
            return_types.push(str_to_value_type(t.token_str))
            expect_comma = true
            lexer_advance(lexer, 1)
            t = lexer.peek()
        case:
            throw format(t.line.to_str(), ":", t.col.to_str(), ": Unexpected '", token_type_to_str(t.token_type), "' in func/proc returns.")
        }
    }

    if end_found {
        return return_types
    } else {
        throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected '{{' or 'throws' after return values.")
    }
}

// TODO DRY with func_proc_returns ?
func_proc_throws := func(mut lexer: Lexer) returns Array throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    mut end_found := false
    mut return_types := Array()  // Array of ValueType
    mut t := lexer_previous(lexer)
    tt := t.token_type
    if not(tt.eq(TokenType.Throws)) {
        return return_types
    }
    t = lexer.peek()
    mut expect_comma := false

    while not(lexer_is_eof(lexer, 0)).and(not(end_found)) {
        tt2 := t.token_type
        switch tt2 {
        case TokenType.LeftBrace:
            end_found = true
            lexer_advance(lexer, 1)
        case TokenType.Semicolon:
            end_found = true
            lexer_advance(lexer, 1)
        case TokenType.Comma:
            if expect_comma {
                expect_comma = false
                lexer_advance(lexer, 1)
                t = lexer.peek()
            } else {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Unexpected ','.")
            }
        case TokenType.Identifier:
            if expect_comma {
                throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected ',', found '", token_type_to_str(t.token_type), "'.")
            }
            return_types.push(str_to_value_type(t.token_str))
            expect_comma = true
            lexer_advance(lexer, 1)
            t = lexer.peek()
        case:
            throw format(t.line.to_str(), ":", t.col.to_str(), ": Unexpected '", token_type_to_str(t.token_type), "' in func/proc throws.")
        }
    }

    if end_found {
        return return_types
    } else {
        throw format(t.line.to_str(), ":", t.col.to_str(), ": Expected '{{' after throw values.")
    }
}

// Stub for parse_primary - will be translated later
parse_primary := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    throw "parse_primary not yet implemented"
}

// Translation from parser.rs complete up to line 604 (func_proc_throws)
//
// What's translated:
// - All data structures (lines 7-231)
// - Expr constructors: expr_new_parse, expr_new_explicit, expr_new_clone (109-129)
// - is_literal (174-181)
// - value_type_to_str (197-209) - TODO: extract TMulti/TCustom string payloads
// - str_to_value_type (211-220)
// - ModeDef, can_be_imported, mode_from_name, parse_mode (222-322)
// - parse_literal (324-341)
// - parse_list (343-386)
// - parse_assignment (388-393)
// - parse_func_proc_args (395-512)
// - func_proc_returns (514-558)
// - func_proc_throws (561-604)
//
// Missing from lines 1-604:
// - Expr methods: get, exit_error, lang_error, todo_error, error (131-172)
//   (Complex format calls, will add when needed)
// - parse_primary (referenced but not yet translated)
//
// Next to translate: parse_func_proc_definition (line 606)
