mode lib

import("src/core/lexer")

INFER_TYPE := "auto"

// ==============================================================================
// Mode System (mirrors parser.rs lines 222-322)
// ==============================================================================

// Mode definition - controls what language features are allowed in a file
ModeDef := struct {
    mut name: Str = ""
    mut allows_procs: Bool = false
    mut allows_base_mut: Bool = false
    mut allows_base_calls: Bool = false
    mut allows_base_anything: Bool = false
    mut needs_main_proc: Bool = false
    mut imports: Array = Array()  // Array of Str (import paths)
}

// Check if a mode can be imported by other files
can_be_imported := func(mode_def: ModeDef) returns Bool {
    // Modes that need main_proc or allow base-level mutations/calls cannot be imported
    return not(or(or(or(
        mode_def.needs_main_proc,
        mode_def.allows_base_mut),
        mode_def.allows_base_calls),
        mode_def.allows_base_anything))
}

// Create ModeDef from mode name string
mode_from_name := func(mode_name: Str) returns ModeDef throws Str {
    mut m := ModeDef()
    m.name = mode_name

    if mode_name.eq("lib") {
        m.allows_procs = true
        m.allows_base_calls = false
        m.allows_base_mut = false
        m.allows_base_anything = false
        m.needs_main_proc = false
        // m.imports remains empty
        return m
    }

    if mode_name.eq("pure") {
        m.allows_procs = false
        m.allows_base_calls = false
        m.allows_base_mut = false
        m.allows_base_anything = false
        m.needs_main_proc = false
        // m.imports remains empty
        return m
    }

    if mode_name.eq("script") {
        m.allows_procs = true
        m.allows_base_calls = true
        m.allows_base_mut = true
        m.allows_base_anything = true
        m.needs_main_proc = false
        // m.imports remains empty
        return m
    }

    if mode_name.eq("safe_script") {
        m.allows_procs = true
        m.allows_base_calls = true
        m.allows_base_mut = true
        m.allows_base_anything = true
        m.needs_main_proc = false
        // m.imports remains empty
        return m
    }

    if mode_name.eq("cli") {
        m.allows_procs = true
        m.allows_base_calls = false
        m.allows_base_mut = true
        m.allows_base_anything = false
        m.needs_main_proc = true
        // m.imports remains empty
        return m
    }

    if mode_name.eq("test") {
        m.allows_procs = true
        m.allows_base_calls = true
        m.allows_base_mut = true
        m.allows_base_anything = false
        m.needs_main_proc = false
        m.imports.push("src/core/modes/test")
        return m
    }

    // Unknown mode
    throw Str.add("0:0: rstil interpreter implementation doesn't support mode '", Str.add(mode_name, "'"))
}

// Parse mode declaration at start of file
parse_mode := func(path: Str, lexer: Lexer) returns ModeDef throws Str {
    lexer_expect(lexer, TokenType.Mode)  // Consume 'mode' keyword

    t := lexer.peek()
    if not(enum_to_str(t.token_type).eq("TokenType.Identifier")) {
        throw "0:0: Expected identifier after 'mode'"
    }

    mode_name := t.token_str
    m := mode_from_name(mode_name)

    if m.name.eq("safe_script") {
        throw Str.add(Str.add(Str.add(path, ":0:0: mode '"), m.name), "' is not properly supported in 'rstil' yet. Try mode 'script' instead")
    }

    lexer_expect(lexer, TokenType.Identifier)  // Consume mode identifier
    return m
}

// ==============================================================================
// Type System (mirrors parser.rs ValueType and related enums)
// ==============================================================================

// Represents the type of a value (mirrors parser.rs ValueType)
ValueType := enum {
    TFunction,  // Function type
    TType,      // Type itself (for type definitions)
    TCustom: Str,  // Custom type (struct/enum) with name
    TMulti,     // Multiple return values
}

// Type of function/procedure
FunctionType := enum {
    FTFunc,      // Pure function
    FTProc,      // Procedure (can have side effects)
    FTMacro,     // Macro (compile-time)
    FTFuncExt,   // External function
    FTProcExt,   // External procedure
    FTMacroExt,  // External macro
}

// Type definition variants
TTypeDef := enum {
    TEnumDef,    // Enum definition
    TStructDef,  // Struct definition
}

// Enum definition (variant name -> optional payload type)
SEnumDef := struct {
    mut enum_map: Map = Map.new("Str", size_of(Str), "Str", size_of(Str))  // variant_name -> payload_type_name (empty string = no payload)
}

// Struct definition
SStructDef := struct {
    mut members: Map = Map.new("Str", size_of(Str), "Str", size_of(Str))  // field_name -> type_name
    mut default_value_fields: Array = Array()     // Array of field names that have default values
    mut default_value_exprs: Array = Array()      // Array of Expr objects (parallel to default_value_fields)
    mut default_values: Map = Map.new("Str", size_of(Str), "Str", size_of(Str))  // DEPRECATED: kept for compatibility, not used
}

// Function/Procedure definition
SFuncDef := struct {
    mut function_type: FunctionType = FunctionType.FTFunc
    mut args: Map = Map.new("Str", size_of(Str), "Str", size_of(Str))  // arg_name -> type_name
    mut return_types: Array = Array() // return types (usually 0 or 1)
    mut throw_types: Array = Array()  // exception types that can be thrown
    mut body: Expr = Expr()           // function body expression

    // Check if this is a procedure
    is_proc := func(self: SFuncDef) returns Bool {
        ft := enum_to_str(self.function_type)
        return or(ft.eq("FunctionType.FTProc"), ft.eq("FunctionType.FTProcExt"))
    }

    // Check if this is an external function
    is_ext := func(self: SFuncDef) returns Bool {
        ft := enum_to_str(self.function_type)
        return or(or(ft.eq("FunctionType.FTFuncExt"), ft.eq("FunctionType.FTProcExt")), ft.eq("FunctionType.FTMacroExt"))
    }
}

LiteralNodeType := enum {
    List: Str,
    String: Str,
    I64: I64,
    Bool: Bool,
}

Declaration := struct {
    mut name       : Str  = ""
    mut value_type : Str  = ""
    mut is_mut     : Bool = false
}

// TODO: PatternInfo is a workaround for lack of tuple syntax in TIL
// Once TIL supports tuple notation like (Str, Str), this can be replaced with:
// Pattern : (Str, Str)  // Pattern(variant_name, binding_var)
PatternInfo := struct {
    mut variant_name : Str = ""
    mut binding_var  : Str = ""
}

NodeType := enum {
    Body,
    FCall,
    Literal    : LiteralNodeType
    Identifier : Str,
    Declaration : Declaration,
    Assignment : Str,
    FuncDef    : SFuncDef,
    EnumDef    : SEnumDef,
    StructDef  : SStructDef,
    Return,
    Throw,
    Catch,
    If,
    While,
    Switch,
    DefaultCase,
    Range,
    Pattern    : PatternInfo
}

// ExprArray has been removed - we now use Array directly for Expr.params
// Helper function for creating dynamic Expr arrays:

new_expr_array := proc() returns Array throws AllocError {
    return Array.new_dyn("Expr", size_of(Expr))
}

// TODO improve error: when we forget "mut" in fields: "Cannot declare 'Expr.node_type' of custom type 'NodeType'"
Expr := struct {
    mut node_type: NodeType = NodeType.Identifier
    mut params: Array = Array()
    mut line: I64 = 0
    mut col: I64 = 0

    // Constructor from token
    new_parse := func(node_type: NodeType, token: Token, params: Array) returns Expr {
        mut e := Expr()
        e.node_type = node_type
        e.params = params
        e.line = token.line
        e.col = token.col
        return e
    }

    // Constructor with explicit line/col
    new_explicit := func(node_type: NodeType, params: Array, line: I64, col: I64) returns Expr {
        mut e := Expr()
        e.node_type = node_type
        e.params = params
        e.line = line
        e.col = col
        return e
    }

    // Clone with new node_type
    new_clone := func(original: Expr, node_type: NodeType) returns Expr {
        mut e := Expr()
        e.node_type = node_type
        e.params = original.params
        e.line = original.line
        e.col = original.col
        return e
    }

    // Format error message
    error := func(self: Expr, error_type: Str, msg: Str) returns Str throws AllocError, IndexOutOfBoundsError, I64_OverflowError {
        line := self.line
        col := self.col
        return format(I64.to_str(line), ":", I64.to_str(col), ": ", error_type, " ERROR: ", msg)
    }

    // Exit with error (for unrecoverable errors)
    exit_error := proc(self: Expr, error_type: Str, msg: Str) throws AllocError, IndexOutOfBoundsError, I64_OverflowError, Str {
        err_msg := self.error(error_type, msg)
        throw err_msg
    }

    // Language bug error
    lang_error := func(self: Expr, msg: Str) returns Str throws AllocError, IndexOutOfBoundsError, I64_OverflowError {
        full_msg := format(msg, "\nExplanation: This should never happen, this is a bug in the language.")
        return self.error("rstil", full_msg)
    }

    // TODO/not-implemented error
    todo_error := func(self: Expr, msg: Str) returns Str throws AllocError, IndexOutOfBoundsError, I64_OverflowError {
        return self.error("TODO", msg)
    }

    // Safe parameter access (returns expression at index or error)
    get := func(self: Expr, index: I64) returns Expr throws Str, AllocError, IndexOutOfBoundsError, I64_OverflowError {
        if or(lt(index, 0), gteq(index, self.params.len)) {
            err_msg := self.error("eval", format("Parameter index ", index.to_str(), " out of bounds"))
            throw err_msg
        }
        mut result := Expr()
        self.params.get(index, result)
        return result
    }
}

// Convert type name string to ValueType
str_to_value_type := proc(type_name: Str) returns ValueType {
    if type_name.eq("Function") {
        return ValueType.TFunction
    }
    if type_name.eq("Type") {
        return ValueType.TType
    }
    if type_name.eq("Multi") {
        return ValueType.TMulti
    }
    // Everything else is a custom type
    return ValueType.TCustom(type_name)
}

// Convert ValueType to string representation
value_type_to_str := proc(vtype: ValueType) returns Str {
    switch vtype {
    case ValueType.TFunction:
        return "Function"
    case ValueType.TType:
        return "Type"
    case ValueType.TMulti:
        return "Multi"
    case ValueType.TCustom(type_name):
        return type_name
    case:
        return "Unknown"
    }
}

// Check if a token is a literal (string, number, or boolean)
is_literal := func(t: Token) returns Bool {
    tt := t.token_type
    tt_str := enum_to_str(tt)
    if tt_str.eq("TokenType.String") {
        return true
    }
    if tt_str.eq("TokenType.Number") {
        return true
    }
    if tt_str.eq("TokenType.True") {
        return true
    }
    return false
}

// Parse a literal value (string, number, or boolean)
parse_literal := proc(mut lexer: Lexer, t: Token) returns Expr throws Str, AllocError, IndexOutOfBoundsError, I64_OverflowError {
    mut params := new_expr_array()
    tt_str := enum_to_str(t.token_type)

    // Workaround for rstil bug: enum payloads must be variables, not function calls
    // Can't do: NodeType.Literal(LiteralNodeType.String("..."))
    // Must assign to variable first
    mut default_lit := LiteralNodeType.String("")
    mut node_type := NodeType.Literal(default_lit)

    if tt_str.eq("TokenType.String") {
        mut str_lit := LiteralNodeType.String(t.token_str)
        node_type = NodeType.Literal(str_lit)
    }
    else if tt_str.eq("TokenType.Number") {
        // Parse the number string to I64
        mut token_str := t.token_str
        num := token_str.to_i64()
        mut num_lit := LiteralNodeType.I64(num)
        node_type = NodeType.Literal(num_lit)
    }
    else if tt_str.eq("TokenType.True") {
        // Parse boolean - "true" becomes true, anything else is false
        mut token_str := t.token_str
        is_true := token_str.eq("true")
        mut bool_lit := LiteralNodeType.Bool(is_true)
        node_type = NodeType.Literal(bool_lit)
    }
    else {
        throw t.lang_error(format("Trying to parse a token that's not a literal as a literal, found '", tt_str, "'."))
    }

    e := Expr.new_parse(node_type, t, params)
    lexer.advance(1)
    return e
}

// Parse a list (parenthesized expression)
parse_list := proc(mut lexer: Lexer) returns Expr throws Str, AllocError, IndexOutOfBoundsError, I64_OverflowError, FullError {
    mut rightparen_found := false
    mut params := new_expr_array()
    initial_index := lexer.current

    // Inline lexer_expect logic to avoid rstil return bug
    mut p := lexer.peek()
    mut p_type_str := enum_to_str(p.token_type)
    mut expected_str := enum_to_str(TokenType.LeftParen)
    if p_type_str.eq(expected_str) {
        lexer.advance(1)
    } else {
        throw format("Expected token '(', but found '", p.token_str, "'")
    }
    mut list_t := lexer.peek()
    mut expect_comma := false

    while and(not(lexer.is_eof(0)), not(rightparen_found)) {
        tt_str := enum_to_str(list_t.token_type)

        if tt_str.eq("TokenType.RightParen") {
            lexer_expect(lexer, TokenType.RightParen)
            rightparen_found = true
        }
        else if tt_str.eq("TokenType.Comma") {
            if expect_comma {
                lexer_expect(lexer, TokenType.Comma)
                expect_comma = false
                list_t = lexer.peek()
            }
            else {
                throw list_t.error("Unexpected ','.")
            }
        }
        else {
            if expect_comma {
                throw list_t.error(format("Expected ')' or ',', found '", tt_str, "'."))
            }
            expect_comma = true
            prim := parse_primary(lexer)
            params.push(prim)
            list_t = lexer.peek()
        }
    }

    // Return a list literal node
    start_tok := lexer.get_token(initial_index)
    // Workaround for rstil bug: enum payloads must be variables
    mut list_lit := LiteralNodeType.List("")
    list_node := NodeType.Literal(list_lit)
    return Expr.new_parse(list_node, start_tok, params)
}

parse_primary := proc(mut lexer: Lexer) returns Expr throws Str, AllocError, IndexOutOfBoundsError, I64_OverflowError, FullError {
    t := lexer.peek()

    // Check if it's a literal first
    if is_literal(t) {
        mut lit_result := parse_literal(lexer, t)
        return lit_result
    }

    // Match on token type
    tt_str := enum_to_str(t.token_type)

    if tt_str.eq("TokenType.LeftParen") {
        return parse_list(lexer)
    }
    if tt_str.eq("TokenType.Identifier") {
        return parse_primary_identifier(lexer)
    }

    // Function/procedure/enum/struct definitions
    if tt_str.eq("TokenType.Func") {
        return parse_func_proc_definition(lexer)
    }
    if tt_str.eq("TokenType.FuncExt") {
        return parse_func_proc_definition(lexer)
    }
    if tt_str.eq("TokenType.Proc") {
        return parse_func_proc_definition(lexer)
    }
    if tt_str.eq("TokenType.ProcExt") {
        return parse_func_proc_definition(lexer)
    }
    if tt_str.eq("TokenType.Enum") {
        return enum_definition(lexer)
    }
    if tt_str.eq("TokenType.Struct") {
        return parse_struct_definition(lexer)
    }

    throw t.error(format("Expected primary expression, found '", tt_str, "'."))
}

parse_primary_identifier := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current
    t := lexer.peek()
    mut next_t := lexer.next()
    mut current_identifier := t.token_str
    mut params := new_expr_array()

    // Handle dotted identifiers (a.b.c)
    mut continue_dotted := true
    while continue_dotted {
        switch next_t.token_type {
        case TokenType.Dot:
            next2_t := lexer.peek_ahead(2)
            switch next2_t.token_type {
            case TokenType.Identifier:
                current_identifier = next2_t.token_str
                lexer.advance(2)

                // Create identifier expr and add to params
                id_node := NodeType.Identifier(current_identifier)
                id_expr := Expr.new_parse(id_node, t, new_expr_array())
                params.push(id_expr)

                next_t = lexer.next()
            case:
                throw next2_t.error(format("expected identifier after '", current_identifier, ".', found '", enum_to_str(next2_t.token_type), "'."))
            }
        case:
            continue_dotted = false
        }
    }

    // Create the identifier expression
    id_node := NodeType.Identifier(t.token_str)
    start_token := lexer.get_token(initial_index)
    e := Expr.new_parse(id_node, start_token, params)
    lexer.advance(1)

    // Check if it's a function call
    switch next_t.token_type {
    case TokenType.LeftParen:
        // Parse argument list
        arg_list := parse_list(lexer)

        // Create FCall with identifier as first param, followed by arguments
        mut fcall_params := new_expr_array()
        fcall_params.push(e)

        // Add all arguments from the parsed list
        mut arg_list_params := arg_list.params
        for i in 0..arg_list_params.len {
            mut arg := Expr()
            arg_list_params.get(i, arg)
            fcall_params.push(arg)
        }

        fcall_token := lexer.get_token(initial_index)
        mut result := Expr.new_parse(NodeType.FCall, fcall_token, fcall_params)

        // Handle chained method calls: a.method1().method2().method3()
        mut continue_loop := true
        while continue_loop {
            peek_t := lexer.peek()
            switch peek_t.token_type {
            case TokenType.Dot:
                // Consume the dot
                lexer.advance(1)

                // Expect an identifier for the next method name
                method_t := lexer.peek()
                switch method_t.token_type {
                case TokenType.Identifier:
                    method_name := method_t.token_str
                    lexer.advance(1)

                    // Check if it's a method call (has parentheses)
                    next_peek := lexer.peek()
                    switch next_peek.token_type {
                    case TokenType.LeftParen:
                        // Parse the argument list
                        method_args := parse_list(lexer)

                        // Create a new FCall with the method name as identifier and previous result as first arg
                        method_id_node := NodeType.Identifier(method_name)
                        method_id := Expr.new_parse(method_id_node, method_t, new_expr_array())
                        mut new_params := new_expr_array()
                        new_params.push(method_id)
                        new_params.push(result)  // Previous call result becomes first argument

                        // Add method arguments
                        mut method_args_params := method_args.params
                        for i in 0..method_args_params.len {
                            mut arg := Expr()
                            method_args_params.get(i, arg)
                            new_params.push(arg)
                        }

                        result = Expr.new_parse(NodeType.FCall, method_t, new_params)
                    case:
                        throw next_peek.error(format("Expected '(' after method name '", method_name, "', found '", enum_to_str(next_peek.token_type), "'"))
                    }
                case:
                    throw method_t.error(format("Expected method name after '.', found '", enum_to_str(method_t.token_type), "'"))
                }
            case:
                continue_loop = false
            }
        }

        return result
    case:
        // Not a function call, just return the identifier
    }

    return e
}

get_combined_name := proc(e: Expr) returns Str throws Str, AllocError, IndexOutOfBoundsError, I64_OverflowError {
    node_type := e.node_type

    switch node_type {
    case NodeType.Identifier(identifier_name):
        // Check if there are params (dotted access like Foo.bar)
        mut e_params := e.params
        params_len := e_params.len
        if gt(params_len, 0) {
            // Get the first param (the rest of the dotted name)
            mut first_param := Expr()
            e_params.get(0, first_param)

            // Recursively get the combined name for the param
            suffix := get_combined_name(first_param)

            // Combine with a dot and return immediately
            return format(identifier_name, ".", suffix)
        }

        // Simple identifier, just return the extracted name
        return identifier_name
    case:
        t := Token()
        throw t.error("get_combined_name called on non-Identifier NodeType")
    }
}

parse_assignment := proc(mut lexer: Lexer, t: Token, name: Str) returns Expr throws Str, FullError, IndexOutOfBoundsError, I64_OverflowError, AllocError {
    lexer_expect(lexer, TokenType.Equal)
    mut params := new_expr_array()
    params.push(parse_primary(lexer))
    assignment_node := NodeType.Assignment(name)
    return Expr.new_parse(assignment_node, t, params)
}

parse_declaration := proc(mut lexer: Lexer, is_mut: Bool, explicit_type: Str) returns Expr throws Str, IndexOutOfBoundsError, FullError, I64_OverflowError, AllocError {
    t := lexer.peek()
    decl_name := t.token_str
    initial_index := lexer.current

    lexer.advance(3) // identifier, colon, equal
    if not(explicit_type.eq(INFER_TYPE)) {
        lexer.advance(1) // skip type identifier
    }

    mut params := new_expr_array()
    params.push(parse_primary(lexer))

    mut decl := Declaration()
    decl.name = decl_name
    decl.value_type = str_to_value_type(explicit_type)
    decl.is_mut = is_mut

    return Expr.new_parse(NodeType.Declaration(decl), lexer.get_token(initial_index), params)
}

parse_statement_identifier := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    t := lexer.peek()
    mut next_t := lexer.next()
    mut next_token_type := next_t.token_type

    switch next_token_type {
    case TokenType.LeftParen:
        return parse_primary_identifier(lexer)
    case TokenType.Dot:
        e := parse_primary_identifier(lexer)
        switch e.node_type {
        case NodeType.FCall:
            return e
        case NodeType.Identifier:
            // continue
        case:
            throw t.todo_error("a series of identifiers and dots should have been parsed as identifier or function call")
        }

        next_t = lexer.peek()
        next_token_type = next_t.token_type

        switch next_token_type {
        case TokenType.Equal:
            name := get_combined_name(e)
            return parse_assignment(lexer, t, name)
        case:
            throw t.error("While parsing a '.', this should never happen")
        }

    case TokenType.Equal:
        lexer.advance(1)
        return parse_assignment(lexer, t, t.token_str)

    case TokenType.Colon:
        next_next_t := lexer.peek_ahead(2)
        next_next_token_type := next_next_t.token_type
        identifier := t.token_str

        switch next_next_token_type {
        case TokenType.Identifier:
            type_name := next_next_t.token_str
            return parse_declaration(lexer, false, type_name)
        case TokenType.Equal:
            return parse_declaration(lexer, false, INFER_TYPE)
        case:
            throw t.error(format("Expected Type or '=' after '", identifier, " :' in statement, found '", enum_to_str(next_next_token_type), "'."))
        }

    case:
        throw t.error(format("Expected '(', ':' or '=' after identifier in statement, found '", enum_to_str(next_token_type), "'."))
    }
}

parse_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    t := lexer.peek()

    switch t.token_type {
    case TokenType.For:
        return parse_for_statement(lexer)
    case TokenType.Return:
        return parse_return_statement(lexer)
    case TokenType.Throw:
        return parse_throw_statement(lexer)
    case TokenType.If:
        return parse_if_statement(lexer)
    case TokenType.While:
        return parse_while_statement(lexer)
    case TokenType.Switch:
        return parse_switch_statement(lexer)
    case TokenType.Mut:
        return parse_mut_declaration(lexer)
    case TokenType.Identifier:
        return parse_statement_identifier(lexer)
    case TokenType.Catch:
        return parse_catch_statement(lexer)
    case TokenType.LeftBrace:
        // Nested block scope
        lexer.advance(1)
        return parse_body(lexer, TokenType.RightBrace)
    case:
        throw t.error(format("Expected statement, found ", enum_to_str(t.token_type)))
    }

    e := Expr()
    return e
}

// Parse a return statement: return expr1, expr2, ...
parse_return_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current
    lexer.advance(1)
    mut params := new_expr_array()

    // Try to parse first expression (optional - return might have no value)
    prim := parse_primary(lexer)
    params.push(prim)

    catch(err: Str) {
        // No return value - this is OK for empty return statements
    }

    // Parse additional comma-separated return values
    mut t := lexer.peek()
    mut continue_loop := true
    while continue_loop {
        switch t.token_type {
        case TokenType.Comma:
            lexer.advance(1)
            prim2 := parse_primary(lexer)
            params.push(prim2)
            t = lexer.peek()
        case:
            continue_loop = false
        }
    }

    return Expr.new_parse(NodeType.Return, lexer.get_token(initial_index), params)
}

// Parse an if statement: if cond { body } else { body }
parse_if_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current
    lexer.advance(1)
    mut params := new_expr_array()

    // Parse condition
    prim := parse_primary(lexer)
    params.push(prim)

    // Expect left brace
    t := lexer.peek()
    switch t.token_type {
    case TokenType.LeftBrace:
        lexer.advance(1)
    case:
        throw t.error(format("Expected '{{' after condition in 'if' statement, found '", enum_to_str(t.token_type), "'."))
    }

    // Parse body
    body := parse_body(lexer, TokenType.RightBrace)
    params.push(body)

    // Check for else clause
    else_t := lexer.peek()
    switch else_t.token_type {
    case TokenType.Else:
        lexer.advance(1)
        next_t := lexer.peek()
        switch next_t.token_type {
        case TokenType.If:
            // else if - recursively parse as nested if statement
            nested_if := parse_if_statement(lexer)
            params.push(nested_if)
        case TokenType.LeftBrace:
            lexer.advance(1)
            else_body := parse_body(lexer, TokenType.RightBrace)
            params.push(else_body)
        case:
            throw next_t.error(format("Expected '{{' or 'if' after 'else', found '", enum_to_str(next_t.token_type), "'."))
        }
    case:
        // No else clause
    }

    return Expr.new_parse(NodeType.If, lexer.get_token(initial_index), params)
}

// Parse a while statement: while cond { body }
parse_while_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current
    lexer.advance(1)
    mut params := new_expr_array()

    // Parse condition
    prim := parse_primary(lexer)
    params.push(prim)

    // Expect left brace
    t := lexer.peek()
    switch t.token_type {
    case TokenType.LeftBrace:
        lexer.advance(1)
    case:
        throw t.error("Expected '{{' after condition in 'while' statement.")
    }

    // Parse body
    body := parse_body(lexer, TokenType.RightBrace)
    params.push(body)

    return Expr.new_parse(NodeType.While, lexer.get_token(initial_index), params)
}

// Parse a for statement: for loop_var in start..end { body }
// Desugars to: mut loop_var := start; while loop_var.lt(end) { body; loop_var.inc() }
parse_for_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_token := lexer.peek()
    lexer.advance(1) // consume 'for'

    // Expect loop variable name
    ident_token := lexer.peek()
    switch ident_token.token_type {
    case TokenType.Identifier:
        // Good
    case:
        throw ident_token.error("Expected identifier after 'for'")
    }
    loop_var_name := ident_token.token_str
    lexer.advance(1)

    // Expect 'in'
    in_token := lexer.peek()
    switch in_token.token_type {
    case TokenType.In:
        lexer.advance(1)
    case:
        throw in_token.error("Expected 'in' after loop variable")
    }

    // Parse the range expression (e.g., 1..10)
    range_expr := parse_case_expr(lexer)
    switch range_expr.node_type {
    case NodeType.Range:
        // Good
    case:
        throw ident_token.error("Expected range expression (start..end) after 'in'")
    }

    mut start_expr := Expr()
    mut end_expr := Expr()
    range_expr.params.get(0, start_expr)
    range_expr.params.get(1, end_expr)

    // Expect '{'
    brace_token := lexer.peek()
    switch brace_token.token_type {
    case TokenType.LeftBrace:
        lexer.advance(1)
    case:
        throw brace_token.error("Expected '{{' after range expression")
    }

    body_expr := parse_body(lexer, TokenType.RightBrace)

    // Create: mut loop_var := start_expr
    mut decl_params := new_expr_array()
    decl_params.push(start_expr)
    mut loop_var_decl := Declaration()
    loop_var_decl.name = loop_var_name
    loop_var_decl.value_type = str_to_value_type(INFER_TYPE)
    loop_var_decl.is_mut = true
    mut decl_expr := Expr.new_parse(NodeType.Declaration(loop_var_decl), initial_token, decl_params)

    // Create condition: loop_var.lt(end_expr)
    // This creates an AST like: loop_var.lt(end_expr) which becomes a function call
    mut lt_id_params := new_expr_array()
    mut lt_id_node := NodeType.Identifier("lt")
    mut empty_params := new_expr_array()
    mut lt_ident := Expr.new_parse(lt_id_node, initial_token, empty_params)
    lt_id_params.push(lt_ident)
    mut loop_var_node := NodeType.Identifier(loop_var_name)
    mut loop_var_id := Expr.new_parse(loop_var_node, initial_token, lt_id_params)

    mut cond_params := new_expr_array()
    cond_params.push(loop_var_id)
    cond_params.push(end_expr)
    mut cond_expr := Expr.new_explicit(NodeType.FCall, cond_params, initial_token.line, initial_token.col)

    // Create increment: loop_var.inc()
    mut inc_id_params := new_expr_array()
    mut inc_id_node := NodeType.Identifier("inc")
    mut empty_params2 := new_expr_array()
    mut inc_ident := Expr.new_parse(inc_id_node, initial_token, empty_params2)
    inc_id_params.push(inc_ident)
    mut loop_var_node2 := NodeType.Identifier(loop_var_name)
    mut loop_var_id2 := Expr.new_parse(loop_var_node2, initial_token, inc_id_params)
    mut inc_params := new_expr_array()
    inc_params.push(loop_var_id2)
    mut inc_expr := Expr.new_explicit(NodeType.FCall, inc_params, initial_token.line, initial_token.col)

    // Create while body with original body + increment
    mut while_body_params := new_expr_array()
    mut i := 0
    mut body_params := body_expr.params
    while i.lt(body_params.len) {
        mut param := Expr()
        body_params.get(i, param)
        while_body_params.push(param)
        i.inc()
    }
    while_body_params.push(inc_expr)
    mut while_body := Expr.new_explicit(NodeType.Body, while_body_params, body_expr.line, body_expr.col)

    // Create while statement
    mut while_params := new_expr_array()
    while_params.push(cond_expr)
    while_params.push(while_body)
    while_expr := Expr.new_explicit(NodeType.While, while_params, initial_token.line, initial_token.col)

    // Wrap in a body with declaration and while
    mut result_params := new_expr_array()
    result_params.push(decl_expr)
    result_params.push(while_expr)
    return Expr.new_explicit(NodeType.Body, result_params, initial_token.line, initial_token.col)
}

// Parse function/procedure arguments: (name: Type, mut name2: Type, variadic: ..Type)
// Validates syntax - full Declaration array could be returned if needed in the future
parse_func_proc_args := proc(mut lexer: Lexer) throws Str, IndexOutOfBoundsError, I64_OverflowError, AllocError {
    lexer_expect(lexer, TokenType.LeftParen)

    mut rightparen_found := false
    mut t := lexer.peek()
    mut expect_comma := false
    mut expect_colon := false
    mut expect_name := true
    mut is_variadic := false
    mut arg_name := "unnamed"
    mut is_mut := false

    while not(or(lexer.is_eof(0), rightparen_found)) {
        switch t.token_type {
        case TokenType.RightParen:
            if expect_colon {
                throw t.error(format("Expected ': Type' after arg name '", arg_name, "' before ')'."))
            }
            rightparen_found = true
            lexer.advance(1)

        case TokenType.Comma:
            if expect_colon {
                throw t.error(format("Expected ': Type' after arg name '", arg_name, "', but found ','."))
            }
            if expect_name {
                throw t.error("Expected arg name before ','.")
            }
            if expect_comma {
                expect_comma = false
                expect_colon = false
                expect_name = true
                is_mut = false
                lexer_expect(lexer, TokenType.Comma)
                t = lexer.peek()
            } else {
                throw t.error("Unexpected ','.")
            }

        case TokenType.Colon:
            if expect_colon {
                expect_colon = false
                expect_name = false
                expect_comma = false
                lexer.advance(1)
                t = lexer.peek()
                switch t.token_type {
                case TokenType.Identifier:
                    // Type name - good
                case TokenType.DoubleDot:
                    // Variadic marker - good
                case:
                    throw t.error(format("Expected type after '", arg_name, ":', but found '", t.token_str, "'."))
                }
            } else {
                throw t.error("Unexpected ':'.")
            }

        case TokenType.DoubleDot:
            if expect_colon {
                throw t.error(format("Expected ': Type' after arg name '", arg_name, "', but found '..'."))
            }
            if expect_comma {
                throw t.error(format("Expected ',', found '", enum_to_str(t.token_type), "'."))
            }
            if expect_name {
                throw t.error(format("Expected arg name, found '", enum_to_str(t.token_type), "'."))
            }
            is_variadic = true
            lexer.advance(1)
            t = lexer.peek()

        case TokenType.Identifier:
            if expect_colon {
                throw t.error(format("Expected ': Type' after arg name '", arg_name, "', but found '", t.token_str, "'."))
            }
            if expect_comma {
                throw t.error(format("Expected ',', found identifier '", t.token_str, "'."))
            }
            if expect_name {
                arg_name = t.token_str
                expect_colon = true
                expect_name = false
            } else {
                // This is the type name - could be stored if needed
                expect_comma = true
                is_mut = false
                is_variadic = false
            }
            lexer.advance(1)
            t = lexer.peek()

        case TokenType.Mut:
            if not(expect_name) {
                throw t.error("Unexpected 'mut' in argument list.")
            }
            is_mut = true
            lexer.advance(1)
            t = lexer.peek()

        case:
            throw t.error(format("Unexpected '", enum_to_str(t.token_type), "' in func/proc args."))
        }
    }

    switch t.token_type {
    case TokenType.RightParen:
        return
    case:
        throw t.error("Expected closing parentheses.")
    }
}

// Parse function/procedure definition: func/proc(args) returns Type throws Error { body }
parse_func_proc_definition := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_token := lexer.peek()

    // Determine function type
    mut func_type := FunctionType.FTFunc
    switch initial_token.token_type {
    case TokenType.Func:
        func_type = FunctionType.FTFunc
    case TokenType.FuncExt:
        func_type = FunctionType.FTFuncExt
    case TokenType.Proc:
        func_type = FunctionType.FTProc
    case TokenType.ProcExt:
        func_type = FunctionType.FTProcExt
    case:
        throw initial_token.error("Expected 'func', 'func_ext', 'proc', or 'proc_ext'")
    }

    lexer.advance(1) // consume 'func' or 'proc'

    mut t := lexer.peek()
    if lexer.is_eof(1) {
        throw t.error("expected '(' after 'func' or 'proc', found EOF.")
    }
    switch t.token_type {
    case TokenType.LeftParen:
        // Good, continue
    case:
        throw t.error(format("expected '(' after 'func', found '", enum_to_str(t.token_type), "'."))
    }

    // Parse arguments (validates syntax but doesn't return metadata)
    parse_func_proc_args(lexer)

    // Parse returns clause (if present)
    t = lexer.peek()
    lexer.advance(1)
    mut has_returns := false
    switch t.token_type {
    case TokenType.Returns:
        has_returns = true
    case:
        has_returns = false
    }
    if has_returns {
        t = lexer.peek()
        mut returns_end_found := false
        mut expect_comma := false
        while not(or(lexer.is_eof(0), returns_end_found)) {
            switch t.token_type {
            case TokenType.Throws:
                returns_end_found = true
                lexer.advance(1)
            case TokenType.LeftBrace:
                returns_end_found = true
                lexer.advance(1)
            case TokenType.Semicolon:
                returns_end_found = true
                lexer.advance(1)
            case TokenType.Comma:
                if expect_comma {
                    lexer_expect(lexer, TokenType.Comma)
                    expect_comma = false
                    t = lexer.peek()
                } else {
                    throw t.error("Unexpected ','.")
                }
            case TokenType.Identifier:
                if expect_comma {
                    throw t.error(format("Expected ',', found '", enum_to_str(t.token_type), "'."))
                }
                // Return type validated - could be stored if needed
                expect_comma = true
                lexer.advance(1)
                t = lexer.peek()
            case:
                throw t.error(format("Unexpected '", enum_to_str(t.token_type), "' in func/proc returns."))
            }
        }
        if not(returns_end_found) {
            throw t.error("Expected '{{' or 'throws' after return values.")
        }
    }

    // Parse throws clause (if present)
    mut prev_t := lexer.previous()
    mut has_throws := false
    switch prev_t.token_type {
    case TokenType.Throws:
        has_throws = true
    case:
        has_throws = false
    }
    if has_throws {
        t = lexer.peek()
        mut throws_end_found := false
        mut expect_comma := false
        while not(or(lexer.is_eof(0), throws_end_found)) {
            switch t.token_type {
            case TokenType.LeftBrace:
                throws_end_found = true
                lexer.advance(1)
            case TokenType.Semicolon:
                throws_end_found = true
                lexer.advance(1)
            case TokenType.Comma:
                if expect_comma {
                    expect_comma = false
                    lexer.advance(1)
                    t = lexer.peek()
                } else {
                    throw t.error("Unexpected ','.")
                }
            case TokenType.Identifier:
                if expect_comma {
                    throw t.error(format("Expected ',', found '", enum_to_str(t.token_type), "'."))
                }
                // Throw type validated - could be stored if needed
                expect_comma = true
                lexer.advance(1)
                t = lexer.peek()
            case:
                throw t.error(format("Unexpected '", enum_to_str(t.token_type), "' in func/proc throws."))
            }
        }
        if not(throws_end_found) {
            throw t.error("Expected '{{' or ';' after throw types.")
        }
    }

    // Check if this is a declaration (ends with semicolon) or definition (has body)
    prev_t = lexer.previous()
    mut body := Expr()
    mut is_declaration := false
    switch prev_t.token_type {
    case TokenType.Semicolon:
        is_declaration = true
    case:
        is_declaration = false
    }
    if is_declaration {
        // Function declaration without body (e.g., external function)
        body = Expr.new_explicit(NodeType.Body, new_expr_array(), initial_token.line, initial_token.col)
    } else {
        // Parse the function body
        body = parse_body(lexer, TokenType.RightBrace)
    }

    // Create SFuncDef and return as NodeType.FuncDef
    mut func_def := SFuncDef()
    func_def.function_type = func_type
    func_def.body = body

    mut params := new_expr_array()
    mut func_def_node := NodeType.FuncDef(func_def)
    return Expr.new_parse(func_def_node, initial_token, params)
}

// Parse a throw statement: throw expr1, expr2, ...
parse_throw_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current
    lexer.advance(1)
    mut params := new_expr_array()

    // Try to parse first expression (optional - throw might have no value)
    prim := parse_primary(lexer)
    params.push(prim)

    catch(err: Str) {
        // No throw value - this is OK for some throw statements
    }

    // Parse additional comma-separated values
    mut t := lexer.peek()
    mut continue_loop := true
    while continue_loop {
        switch t.token_type {
        case TokenType.Comma:
            lexer.advance(1)
            prim2 := parse_primary(lexer)
            params.push(prim2)
            t = lexer.peek()
        case:
            continue_loop = false
        }
    }

    return Expr.new_parse(NodeType.Throw, lexer.get_token(initial_index), params)
}

// Parse a mutable variable declaration: mut x: Type = value  or  mut x := value
parse_mut_declaration := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, FullError, I64_OverflowError, AllocError {
    t := lexer.peek()
    mut next_t := lexer.next()
    mut next_token_type := next_t.token_type

    // Expect identifier after 'mut'
    switch next_token_type {
    case TokenType.Identifier:
        // Good
    case:
        throw t.error(format("Expected identifier after 'mut', found '", enum_to_str(next_token_type), "'."))
    }

    identifier := next_t.token_str
    lexer.advance(1)
    next_t = lexer.next()
    next_token_type = next_t.token_type

    // Expect ':' after identifier
    switch next_token_type {
    case TokenType.Colon:
        // Good
    case:
        throw t.error(format("Expected ':' after 'mut ", identifier, "', found '", enum_to_str(next_token_type), "'."))
    }

    // Check if we have a type or just '='
    next_next_t := lexer.peek_ahead(2)
    next_next_token_type := next_next_t.token_type

    switch next_next_token_type {
    case TokenType.Identifier:
        // Has explicit type: mut x: I64 = ...
        type_name := next_next_t.token_str
        return parse_declaration(lexer, true, type_name)
    case TokenType.Equal:
        // Type inference: mut x := ...
        return parse_declaration(lexer, true, INFER_TYPE)
    case:
        throw t.error(format("Expected a type identifier or '=' after 'mut ", identifier, " :' in statement, found '", enum_to_str(next_next_token_type), "'."))
    }
}

// Parse a case expression in a switch: either a primary expression or a range (a..b)
parse_case_expr := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    left := parse_primary(lexer)
    t := lexer.peek()

    switch t.token_type {
    case TokenType.DoubleDot:
        // Range expression: a..b
        lexer.advance(1)
        right := parse_primary(lexer)
        mut range_params := new_expr_array()
        range_params.push(left)
        range_params.push(right)
        return Expr.new_parse(NodeType.Range, t, range_params)
    case:
        // Check if this is a pattern match: EnumVariant(binding_var)
        // This would have been parsed as FCall with one Identifier parameter
        mut left_params := left.params
        left_node_type := left.node_type
        if enum_eq(left_node_type, NodeType.FCall) {
            if left_params.len.eq(2) {
                // FCall params are: [function_name, arg1, arg2, ...]
                // For pattern matching, we expect: [variant_identifier, binding_identifier]
                mut variant_expr := Expr()
                left_params.get(0, variant_expr)
                variant_name := get_combined_name(variant_expr)

                mut binding_expr := Expr()
                left_params.get(1, binding_expr)
                binding_node_type := binding_expr.node_type

                switch binding_node_type {
                case NodeType.Identifier(binding_var):
                    // Convert FCall to Pattern
                    mut pattern_info := PatternInfo()
                    pattern_info.variant_name = variant_name
                    pattern_info.binding_var = binding_var
                    return Expr.new_explicit(
                        NodeType.Pattern(pattern_info),
                        new_expr_array(),
                        left.line,
                        left.col
                    )
                case:
                    // Not a pattern match, return as-is
                    return left
                }
            }
        }

        // Just a single expression
        return left
    }
}

// Parse a switch statement: switch expr { case ...: body }
parse_switch_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    t := lexer.peek()
    initial_index := lexer.current
    lexer.advance(1)
    mut params := new_expr_array()

    // Parse the value being switched on
    prim := parse_primary(lexer)
    params.push(prim)

    // Expect left brace
    peek_t := lexer.peek()
    switch peek_t.token_type {
    case TokenType.LeftBrace:
        lexer.advance(1)
    case:
        throw t.error("Expected '{' after primary expression in 'switch' statement.")
    }

    // Parse case statements
    mut end_found := false
    while and(lt(lexer.current, lexer.len()), not(end_found)) {
        mut next_t := lexer.peek()

        // Expect 'case' keyword
        switch next_t.token_type {
        case TokenType.Case:
            // Good
        case:
            throw next_t.error(format("Expected 'case' in switch, found '", enum_to_str(next_t.token_type), "'"))
        }

        lexer.advance(1)
        next_t = lexer.peek()

        // Check if it's default case (case:) or regular case (case expr:)
        switch next_t.token_type {
        case TokenType.Colon:
            // Default case
            default_case := Expr.new_parse(NodeType.DefaultCase, t, new_expr_array())
            params.push(default_case)
        case:
            // Regular case with expression
            case_prim := parse_case_expr(lexer)
            params.push(case_prim)
        }

        // Expect colon
        next_t = lexer.peek()
        switch next_t.token_type {
        case TokenType.Colon:
            lexer.advance(1)
        case:
            throw next_t.error(format("Expected ':' after case expression in switch, found '", enum_to_str(next_t.token_type), "'"))
        }

        // Parse case body
        next_t = lexer.peek()
        mut body_params := new_expr_array()
        mut body_continue := true
        while body_continue {
            switch next_t.token_type {
            case TokenType.RightBrace:
                // End of switch
                body := Expr.new_parse(NodeType.Body, t, body_params)
                params.push(body)
                end_found = true
                body_continue = false
                lexer.advance(1)
            case TokenType.Case:
                // Next case
                body := Expr.new_parse(NodeType.Body, t, body_params)
                params.push(body)
                body_continue = false
            case:
                // Statement in case body
                stmt := parse_statement(lexer)
                body_params.push(stmt)
                next_t = lexer.peek()
            }
        }
    }

    return Expr.new_parse(NodeType.Switch, lexer.get_token(initial_index), params)
}

// Parse a catch statement: catch (err: ErrorType) { body }
parse_catch_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current
    lexer.advance(1) // consume 'catch'

    lexer_expect(lexer, TokenType.LeftParen) // expect '('

    // Parse the error variable name
    name_token := lexer.peek()
    lexer_expect(lexer, TokenType.Identifier)
    name := name_token.token_str
    mut params := new_expr_array()

    // Create name expression manually
    mut name_expr := Expr()
    name_expr.node_type = NodeType.Identifier(name)
    name_expr.line = name_token.line
    name_expr.col = name_token.col
    params.push(name_expr)

    lexer_expect(lexer, TokenType.Colon) // expect ':'

    // Parse the exception type
    type_token := lexer.peek()
    lexer_expect(lexer, TokenType.Identifier)

    // Create type expression manually
    mut type_expr := Expr()
    type_expr.node_type = NodeType.Identifier(type_token.token_str)
    type_expr.line = type_token.line
    type_expr.col = type_token.col
    params.push(type_expr)

    lexer_expect(lexer, TokenType.RightParen) // expect ')'

    lexer_expect(lexer, TokenType.LeftBrace) // expect '{'
    body_expr := parse_body(lexer, TokenType.RightBrace)
    params.push(body_expr)

    return Expr.new_parse(NodeType.Catch, lexer.get_token(initial_index), params)
}

// Parse struct definition: struct { field: Type = value, ... }
parse_struct_definition := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_token := lexer.peek()
    lexer_expect(lexer, TokenType.Struct)

    t := lexer.peek()
    switch t.token_type {
    case TokenType.LeftBrace:
        // Good, continue
    case:
        throw t.error("Expected '{{' after 'struct'.")
    }

    if lexer.is_eof(1) {
        throw t.error("expected 'identifier' after 'struct {{', found EOF.")
    }

    lexer.advance(1)
    body := parse_body(lexer, TokenType.RightBrace)

    // Extract declarations and build members/default_values maps
    mut members := Map.new("Str", size_of(Str), "Str", size_of(Str))
    mut default_value_fields := Array.new_dyn("Str", size_of(Str))
    mut default_value_exprs := Array.new_dyn("Expr", size_of(Expr))

    mut i := 0
    mut body_params := body.params
    while i.lt(body_params.len) {
        mut param := Expr()
        body_params.get(i, param)

        switch param.node_type {
        case NodeType.Declaration(decl):
            // Check that declaration has a value (params.len == 1)
            mut param_params := param.params
            param_count := param_params.len
            if not(param_count.eq(1)) {
                throw t.error("all declarations inside struct definitions must have a value for now")
            }
            // Extract field name, type, and is_mut from Declaration payload
            // Format: "is_mut:type_name" or "const:type_name"
            mut mut_prefix := ""
            if decl.is_mut {
                mut_prefix = "mut:"
            } else {
                mut_prefix = "const:"
            }
            members.set(decl.name, format(mut_prefix, value_type_to_str(decl.value_type)))

            // Store default value expression
            mut default_val := Expr()
            param_params.get(0, default_val)
            default_value_fields.push(decl.name)
            default_value_exprs.push(default_val)
        case:
            throw t.error("expected only declarations inside struct definition")
        }
        i.inc()
    }

    // Create SStructDef and return as NodeType.StructDef
    mut struct_def := SStructDef()
    struct_def.members = members
    struct_def.default_value_fields = default_value_fields
    struct_def.default_value_exprs = default_value_exprs

    mut params := new_expr_array()
    mut struct_def_node := NodeType.StructDef(struct_def)
    return Expr.new_parse(struct_def_node, initial_token, params)
}

// Parse enum definition: enum { Variant1, Variant2: Type, ... }
enum_definition := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current
    lexer.advance(1) // consume 'enum'

    t := lexer.peek()
    switch t.token_type {
    case TokenType.LeftBrace:
        // Good, continue
    case:
        throw t.error("Expected '{{' after 'enum'.")
    }

    if lexer.is_eof(1) {
        throw t.error("expected identifier after 'enum {{', found EOF.")
    }

    lexer.advance(1) // consume '{'
    mut enum_map := Map.new("Str", size_of(Str), "Str", size_of(Str))
    mut end_found := false

    while not(or(lexer.is_eof(0), end_found)) {
        mut it_t := lexer.peek()

        switch it_t.token_type {
        case TokenType.RightBrace:
            end_found = true

        case TokenType.Identifier:
            enum_val_name := it_t.token_str
            next_t := lexer.next()

            switch next_t.token_type {
            case TokenType.Colon:
                // Variant with type: VarName: Type
                next2_t := lexer.peek_ahead(2)
                switch next2_t.token_type {
                case TokenType.Identifier:
                    // Good - this is the type
                case:
                    throw next2_t.error(format("Expected type identifier after '", enum_val_name, " :', found '", enum_to_str(next2_t.token_type), "'."))
                }
                enum_val_type := next2_t.token_str
                enum_map.insert(enum_val_name, enum_val_type)
                lexer.advance(2) // skip colon and type

            case TokenType.Comma:
                // Variant without type, followed by comma
                enum_map.insert(enum_val_name, "")

            case TokenType.RightBrace:
                // Variant without type, end of enum
                enum_map.insert(enum_val_name, "")
                end_found = true

            case:
                throw next_t.error(format("Expected ',' or ':' after '", enum_val_name, "', found '", enum_to_str(next_t.token_type), "'."))
            }

        case TokenType.Comma:
            // Skip comma - already handled

        case:
            throw it_t.error(format("Expected '}}' to end enum or a new identifier, found '", enum_to_str(it_t.token_type), "'."))
        }

        lexer.advance(1)
    }

    if not(end_found) {
        throw t.error("Expected '}}' to end enum.")
    }

    // Create SEnumDef and return as NodeType.EnumDef
    mut enum_def := SEnumDef()
    enum_def.enum_map = enum_map

    mut params := new_expr_array()
    mut enum_def_node := NodeType.EnumDef(enum_def)
    return Expr.new_parse(enum_def_node, lexer.get_token(initial_index), params)
}

parse_body := proc(mut lexer: Lexer, end_token: TokenType) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    mut params := new_expr_array()
    mut end_found := false
    start_token := lexer.peek()

    while and(
        not(end_found),
        lt(lexer.current, lexer.len()),
    ) {
        t := lexer.peek()
        switch t.token_type {
        case end_token:
            end_found = true

        case TokenType.Semicolon:
            lexer.advance(1)

        case:
            stmt := parse_statement(lexer)
            params.push(stmt)
        }
    }
    if end_found {
        return Expr.new_parse(NodeType.Body, start_token, params)
    }
    t := lexer.peek()
    t.error(format(loc(), "Expected the body to end with ", enum_to_str(end_token)))
}

// ==============================================================================
// Public Parser Entry Point (mirrors parser.rs lines 1377-1402)
// ==============================================================================

// Parse tokens from lexer into AST
// This is the main entry point for parsing
parse_tokens := func(lexer: Lexer) returns Expr throws Str {
    e := parse_body(lexer, TokenType.Eof)

    // Check for unparsed tokens
    mut i := lexer.current
    mut unparsed_tokens := 0
    lexer_len := lexer.len()
    if I64.lt(i, lexer_len) {
        unparsed_tokens = I64.sub(lexer_len, i)
    }
    if I64.gt(unparsed_tokens, 0) {
        println(Str.add("Total tokens parsed: ", Str.add(I64.to_str(lexer.current), Str.add("/", I64.to_str(lexer_len)))))
    }
    while I64.lt(i, lexer_len) {
        t := lexer.get_token(i)
        println(Str.add("Token: ", enum_to_str(t.token_type)))
        i = I64.add(i, 1)
    }
    if I64.gt(unparsed_tokens, 0) {
        throw Str.add("Total unparsed tokens: ", Str.add(I64.to_str(unparsed_tokens), Str.add("/", I64.to_str(lexer_len))))
    }
    return e
}
