mode lib

import("src/core/lexer")

INFER_TYPE := "auto"

// Represents the type of a value (mirrors parser.rs ValueType)
ValueType := enum {
    TFunction,  // Function type
    TType,      // Type itself (for type definitions)
    TCustom: Str,  // Custom type (struct/enum) with name
    TMulti,     // Multiple return values
}

// Type of function/procedure
FunctionType := enum {
    FTFunc,      // Pure function
    FTProc,      // Procedure (can have side effects)
    FTMacro,     // Macro (compile-time)
    FTFuncExt,   // External function
    FTProcExt,   // External procedure
    FTMacroExt,  // External macro
}

// Type definition variants
TTypeDef := enum {
    TEnumDef,    // Enum definition
    TStructDef,  // Struct definition
}

// Enum definition (variant name -> optional payload type)
SEnumDef := struct {
    mut enum_map: StrMap = StrMap()  // variant_name -> payload_type_name (empty string = no payload)
}

// Struct definition
SStructDef := struct {
    mut members: StrMap = StrMap()         // field_name -> type_name
    mut default_values: StrMap = StrMap()  // field_name -> default_expr_as_string (for now)
}

// Function/Procedure definition
SFuncDef := struct {
    mut function_type: FunctionType = FunctionType.FTFunc
    mut args: StrMap = StrMap()      // arg_name -> type_name
    mut return_type: Str = ""        // return type name
    mut exception_types: Str = ""    // exception types (comma-separated for now)
    mut body: Expr = Expr()          // function body expression

    // Check if this is a procedure
    is_proc := func(self: SFuncDef) returns Bool {
        ft := enum_to_str(self.function_type)
        return or(ft.eq("FunctionType.FTProc"), ft.eq("FunctionType.FTProcExt"))
    }

    // Check if this is an external function
    is_ext := func(self: SFuncDef) returns Bool {
        ft := enum_to_str(self.function_type)
        return or(or(ft.eq("FunctionType.FTFuncExt"), ft.eq("FunctionType.FTProcExt")), ft.eq("FunctionType.FTMacroExt"))
    }
}

LiteralNodeType := enum {
    List: Str,
    String: Str,
    I64: I64,
    Bool: Bool,
}

Declaration := struct {
    mut name       : Str  = ""
    mut value_type : Str  = ""
    mut is_mut     : Bool = false
}

// TODO support tagged unions
NodeType := enum {
    Body,
    FCall,
    Literal    : LiteralNodeType
    Identifier : Str,
    Declaration : Declaration,
    Assignment : Str,
    // FuncDef : SFuncDef,
    // EnumDef : SEnumDef,
    // StructDef : SStructDef,
    Return,
    Throw,
    Catch,
    If,
    While,
    Switch,
    DefaultCase,
}

ExprArray := struct {
    mut data: Array = Array()      // Array to hold Expr elements
    mut count: I64 = 0              // Number of elements currently stored
    mut capacity: I64 = 16          // Maximum capacity (fixed for Phase 2)

    new := proc() returns ExprArray throws AllocError {
        mut arr := ExprArray()
        // For now, use fixed capacity since dynamic arrays of structs are complex
        arr.data = Array.new("Expr", size_of(Expr), 16)
        arr.count = 0
        arr.capacity = 16
        return arr
    }

    push := proc(mut self: ExprArray, e: Expr) throws IndexOutOfBoundsError, FullError {
        if ge(self.count, self.capacity) {
            throw FullError.new("ExprArray is full")
        }
        self.data.set(self.count, e)
        self.count = add(self.count, 1)
    }

    get := func(self: ExprArray, index: I64, mut result: Expr) throws IndexOutOfBoundsError {
        if or(lt(index, 0), ge(index, self.count)) {
            throw IndexOutOfBoundsError.new("ExprArray index out of bounds")
        }
        self.data.get(index, result)
    }

    len := func(self: ExprArray) returns I64 {
        return self.count
    }
}

// TODO improve error: when we forget "mut" in fields: "Cannot declare 'Expr.node_type' of custom type 'NodeType'"
Expr := struct {
    mut node_type: NodeType = NodeType.Identifier
    mut params: ExprArray = ExprArray()
    mut line: I64 = 0
    mut col: I64 = 0

    // Constructor from token
    new_parse := func(node_type: NodeType, token: Token, params: ExprArray) returns Expr {
        mut e := Expr()
        e.node_type = node_type
        e.params = params
        e.line = token.line
        e.col = token.col
        return e
    }

    // Constructor with explicit line/col
    new_explicit := func(node_type: NodeType, params: ExprArray, line: I64, col: I64) returns Expr {
        mut e := Expr()
        e.node_type = node_type
        e.params = params
        e.line = line
        e.col = col
        return e
    }

    // Clone with new node_type
    new_clone := func(original: Expr, node_type: NodeType) returns Expr {
        mut e := Expr()
        e.node_type = node_type
        e.params = original.params
        e.line = original.line
        e.col = original.col
        return e
    }

    // Format error message
    error := func(self: Expr, error_type: Str, msg: Str) returns Str throws AllocError, IndexOutOfBoundsError, I64_OverflowError {
        return format(self.line.to_str(), ":", self.col.to_str(), ": ", error_type, " ERROR: ", msg)
    }

    // Exit with error (for unrecoverable errors)
    exit_error := proc(self: Expr, error_type: Str, msg: Str) throws AllocError, IndexOutOfBoundsError, I64_OverflowError, Str {
        err_msg := self.error(error_type, msg)
        throw err_msg
    }

    // Language bug error
    lang_error := func(self: Expr, msg: Str) returns Str throws AllocError, IndexOutOfBoundsError, I64_OverflowError {
        full_msg := format(msg, "\nExplanation: This should never happen, this is a bug in the language.")
        return self.error("rstil", full_msg)
    }

    // TODO/not-implemented error
    todo_error := func(self: Expr, msg: Str) returns Str throws AllocError, IndexOutOfBoundsError, I64_OverflowError {
        return self.error("TODO", msg)
    }

    // Safe parameter access (returns expression at index or error)
    get := func(self: Expr, index: I64) returns Expr throws Str, AllocError, IndexOutOfBoundsError, I64_OverflowError {
        // TODO: implement when ExprArray.get is implemented
        if gteq(index, 0) {
            return Expr()  // placeholder
        }
        err_msg := self.error("eval", format("Parameter index ", index.to_str(), " out of bounds"))
        throw err_msg
    }
}

// Convert type name string to ValueType
// TODO: This function is incomplete - need to support TCustom variant creation
str_to_value_type := proc(type_name: Str) returns ValueType {
    if type_name.eq("Function") {
        return ValueType.TFunction
    }
    if type_name.eq("Type") {
        return ValueType.TType
    }
    if type_name.eq("Multi") {
        return ValueType.TMulti
    }
    // Everything else is a custom type
    // TODO: return ValueType.TCustom(type_name) - this doesn't work yet
    // For now, just return TFunction as placeholder
    return ValueType.TFunction
}

// Convert ValueType to string representation
value_type_to_str := proc(vtype: ValueType) returns Str {
    vtype_str := enum_to_str(vtype)

    if vtype_str.eq("ValueType.TFunction") {
        return "Function"
    }
    if vtype_str.eq("ValueType.TType") {
        return "Type"
    }
    if vtype_str.eq("ValueType.TMulti") {
        return "Multi"
    }
    if vtype_str.eq("ValueType.TCustom") {
        // Extract the custom type name from payload
        mut type_name := ""
        enum_extract_payload(vtype, type_name)
        return type_name
    }
    return "Unknown"
}

// Check if a token is a literal (string, number, or boolean)
is_literal := func(t: Token) returns Bool {
    tt := t.token_type
    tt_str := enum_to_str(tt)
    if tt_str.eq("TokenType.String") {
        return true
    }
    if tt_str.eq("TokenType.Number") {
        return true
    }
    if tt_str.eq("TokenType.True") {
        return true
    }
    return false
}

// Parse a literal value (string, number, or boolean)
parse_literal := proc(mut lexer: Lexer, t: Token) returns Expr throws Str, AllocError {
    mut params := ExprArray.new()
    tt_str := enum_to_str(t.token_type)

    mut node_type := NodeType.Literal(LiteralNodeType.String(""))

    if tt_str.eq("TokenType.String") {
        node_type = NodeType.Literal(LiteralNodeType.String(t.token_str))
    }
    else if tt_str.eq("TokenType.Number") {
        // Parse the number string to I64
        mut token_str := t.token_str
        num := token_str.to_i64()
        node_type = NodeType.Literal(LiteralNodeType.I64(num))
    }
    else if tt_str.eq("TokenType.True") {
        // Parse boolean - "true" becomes true, anything else is false
        mut token_str := t.token_str
        is_true := token_str.eq("true")
        node_type = NodeType.Literal(LiteralNodeType.Bool(is_true))
    }
    else {
        throw t.lang_error(format("Trying to parse a token that's not a literal as a literal, found '", tt_str, "'."))
    }

    e := Expr.new_parse(node_type, t, params)
    lexer.advance(1)
    return e
}

// Parse a list (parenthesized expression)
parse_list := proc(mut lexer: Lexer) returns Expr throws Str, AllocError, IndexOutOfBoundsError, I64_OverflowError, FullError {
    mut rightparen_found := false
    mut params := ExprArray.new()
    initial_index := lexer.current_token

    lexer.expect(TokenType.LeftParen)
    mut list_t := lexer.peek()
    mut expect_comma := false

    while and(not(lexer.is_eof(0)), not(rightparen_found)) {
        tt_str := enum_to_str(list_t.token_type)

        if tt_str.eq("TokenType.RightParen") {
            lexer.expect(TokenType.RightParen)
            rightparen_found = true
        }
        else if tt_str.eq("TokenType.Comma") {
            if expect_comma {
                lexer.expect(TokenType.Comma)
                expect_comma = false
                list_t = lexer.peek()
            }
            else {
                throw list_t.error("Unexpected ','.")
            }
        }
        else {
            if expect_comma {
                throw list_t.error(format("Expected ')' or ',', found '", tt_str, "'."))
            }
            expect_comma = true
            prim := parse_primary(lexer)
            params.push(prim)
            list_t = lexer.peek()
        }
    }

    // Return a list literal node
    start_tok := lexer.get_token(initial_index)
    list_node := NodeType.Literal(LiteralNodeType.List(""))
    return Expr.new_parse(list_node, start_tok, params)
}

parse_primary := proc(mut lexer: Lexer) returns Expr throws Str, AllocError, IndexOutOfBoundsError, I64_OverflowError, FullError {
    t := lexer.peek()

    // Check if it's a literal first
    if is_literal(t) {
        return parse_literal(lexer, t)
    }

    // Match on token type
    tt_str := enum_to_str(t.token_type)

    if tt_str.eq("TokenType.LeftParen") {
        return parse_list(lexer)
    }
    if tt_str.eq("TokenType.Identifier") {
        return parse_primary_identifier(lexer)
    }

    // Function/procedure/enum/struct definitions not yet implemented
    if tt_str.eq("TokenType.Func") {
        throw t.todo_error("parse_func_proc_definition not implemented yet")
    }
    if tt_str.eq("TokenType.Proc") {
        throw t.todo_error("parse_func_proc_definition not implemented yet")
    }
    if tt_str.eq("TokenType.Enum") {
        throw t.todo_error("enum_definition not implemented yet")
    }
    if tt_str.eq("TokenType.Struct") {
        throw t.todo_error("parse_struct_definition not implemented yet")
    }

    throw t.error(format("Expected primary expression, found '", tt_str, "'."))
}

parse_primary_identifier := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current_token
    t := lexer.peek()
    mut next_t := lexer.next()
    mut current_identifier := t.token_str
    mut params := ExprArray.new()

    // Handle dotted identifiers (a.b.c)
    mut continue_dotted := true
    while continue_dotted {
        switch next_t.token_type {
        case TokenType.Dot:
            next2_t := lexer.peek_ahead(2)
            switch next2_t.token_type {
            case TokenType.Identifier:
                current_identifier = next2_t.token_str
                lexer.advance(2)

                // Create identifier expr and add to params
                id_node := NodeType.Identifier(current_identifier)
                id_expr := Expr.new_parse(id_node, t, ExprArray.new())
                params.push(id_expr)

                next_t = lexer.next()
            case:
                throw next2_t.error(format("expected identifier after '", current_identifier, ".', found '", enum_to_str(next2_t.token_type), "'."))
            }
        case:
            continue_dotted = false
        }
    }

    // Create the identifier expression
    id_node := NodeType.Identifier(t.token_str)
    start_token := lexer.get_token(initial_index)
    e := Expr.new_parse(id_node, start_token, params)
    lexer.advance(1)

    // Check if it's a function call
    switch next_t.token_type {
    case TokenType.LeftParen:
        // Parse argument list
        arg_list := parse_list(lexer)

        // Create FCall with identifier as first param, followed by arguments
        mut fcall_params := ExprArray.new()
        fcall_params.push(e)

        // Add all arguments from the parsed list
        arg_list_params := arg_list.params
        for i in 0..arg_list_params.len() {
            mut arg := Expr()
            arg_list_params.get(i, arg)
            fcall_params.push(arg)
        }

        fcall_token := lexer.get_token(initial_index)
        mut result := Expr.new_parse(NodeType.FCall, fcall_token, fcall_params)

        // Handle chained method calls: a.method1().method2().method3()
        mut continue_loop := true
        while continue_loop {
            peek_t := lexer.peek()
            switch peek_t.token_type {
            case TokenType.Dot:
                // Consume the dot
                lexer.advance(1)

                // Expect an identifier for the next method name
                method_t := lexer.peek()
                switch method_t.token_type {
                case TokenType.Identifier:
                    method_name := method_t.token_str
                    lexer.advance(1)

                    // Check if it's a method call (has parentheses)
                    next_peek := lexer.peek()
                    switch next_peek.token_type {
                    case TokenType.LeftParen:
                        // Parse the argument list
                        method_args := parse_list(lexer)

                        // Create a new FCall with the method name as identifier and previous result as first arg
                        method_id_node := NodeType.Identifier(method_name)
                        method_id := Expr.new_parse(method_id_node, method_t, ExprArray.new())
                        mut new_params := ExprArray.new()
                        new_params.push(method_id)
                        new_params.push(result)  // Previous call result becomes first argument

                        // Add method arguments
                        method_args_params := method_args.params
                        for i in 0..method_args_params.len() {
                            mut arg := Expr()
                            method_args_params.get(i, arg)
                            new_params.push(arg)
                        }

                        result = Expr.new_parse(NodeType.FCall, method_t, new_params)
                    case:
                        throw next_peek.error(format("Expected '(' after method name '", method_name, "', found '", enum_to_str(next_peek.token_type), "'"))
                    }
                case:
                    throw method_t.error(format("Expected method name after '.', found '", enum_to_str(method_t.token_type), "'"))
                }
            case:
                continue_loop = false
            }
        }

        return result
    case:
        // Not a function call, just return the identifier
    }

    return e
}

get_combined_name := proc(e: Expr) returns Str throws Str, AllocError, IndexOutOfBoundsError, I64_OverflowError {
    node_type := e.node_type

    switch node_type {
    case NodeType.Identifier:
        // Extract the identifier name from the payload
        mut name := ""
        enum_extract_payload(node_type, name)

        // Check if there are params (dotted access like Foo.bar)
        e_params := e.params
        params_len := e_params.len()
        if gt(params_len, 0) {
            // Get the first param (the rest of the dotted name)
            mut first_param := Expr()
            e_params.get(0, first_param)

            // Recursively get the combined name for the param
            rest := get_combined_name(first_param)

            // Combine with a dot
            return format(name, ".", rest)
        }

        // Simple identifier, just return the name
        return name
    case:
        t := Token()
        throw t.error("get_combined_name called on non-Identifier NodeType")
    }
}

parse_assignment := proc(mut lexer: Lexer, t: Token, name: Str) returns Expr throws IndexOutOfBoundsError, AllocError, FullError {
    lexer.expect(TokenType.Equal)
    mut params := ExprArray.new()
    params.push(parse_primary(lexer))
    assignment_node := NodeType.Assignment(name)
    return Expr.new_parse(assignment_node, t, params)
}

parse_declaration := proc(mut lexer: Lexer, is_mut: Bool, explicit_type: Str) returns Expr throws IndexOutOfBoundsError, AllocError, FullError {
    t := lexer.peek()
    decl_name := t.token_str
    initial_index := lexer.current_token

    lexer.advance(3) // identifier, colon, equal
    if not(explicit_type.eq(INFER_TYPE)) {
        lexer.advance(1) // skip type identifier
    }

    mut params := ExprArray.new()
    params.push(parse_primary(lexer))

    mut decl := Declaration()
    decl.name = decl_name
    decl.value_type = str_to_value_type(explicit_type)
    decl.is_mut = is_mut

    return Expr.new_parse(NodeType.Declaration, lexer.get_token(initial_index), params)
    // return Expr.new_parse(NodeType.Declaration(decl), lexer.get_token(initial_index), params) // TODO FIX tagged unions
}

parse_statement_identifier := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    t := lexer.peek()
    mut next_t := lexer.next()
    mut next_token_type := next_t.token_type

    switch next_token_type {
    case TokenType.LeftParen:
        return parse_primary_identifier(lexer)
    case TokenType.Dot:
        e := parse_primary_identifier(lexer)
        switch e.node_type {
        case NodeType.FCall:
            return e
        case NodeType.Identifier:
            // continue
        case:
            throw t.todo_error("a series of identifiers and dots should have been parsed as identifier or function call")
        }

        next_t = lexer.peek()
        next_token_type = next_t.token_type

        switch next_token_type {
        case TokenType.Equal:
            name := get_combined_name(e)
            return parse_assignment(lexer, t, name)
        case:
            throw t.error("While parsing a '.', this should never happen")
        }

    case TokenType.Equal:
        lexer.advance(1)
        return parse_assignment(lexer, t, t.token_str)

    case TokenType.Colon:
        next_next_t := lexer.peek_ahead(2)
        next_next_token_type := next_next_t.token_type
        identifier := t.token_str

        switch next_next_token_type {
        case TokenType.Identifier:
            type_name := next_next_t.token_str
            return parse_declaration(lexer, false, type_name)
        case TokenType.Equal:
            return parse_declaration(lexer, false, INFER_TYPE)
        case:
            throw t.error(format("Expected Type or '=' after '", identifier, " :' in statement, found '", enum_to_str(next_next_token_type), "'."))
        }

    case:
        throw t.error(format("Expected '(', ':' or '=' after identifier in statement, found '", enum_to_str(next_token_type), "'."))
    }
}

parse_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    t := lexer.peek()

    switch t.token_type {
    case TokenType.For:
        throw t.todo_error("Suggestion: use 'while' for now.\nExplanation: keyword 'for' is not supported yet,")
    case TokenType.Return:
        return parse_return_statement(lexer)
    case TokenType.Throw:
        throw t.todo_error("parse throw_statement not implemented yet")
    case TokenType.If:
        return parse_if_statement(lexer)
    case TokenType.While:
        return parse_while_statement(lexer)
    case TokenType.Switch:
        throw t.todo_error("parse switch_statement not implemented yet")
    case TokenType.Mut:
        throw t.todo_error("parse mut_declaration not implemented yet")
    case TokenType.Identifier:
        return parse_statement_identifier(lexer)
    case TokenType.Catch:
        return parse_catch_statement(lexer)
    case:
        throw t.error(format("Expected statement, found ", enum_to_str(t.token_type)))
    }

    e := Expr()
    return e
}

// Parse a return statement: return expr1, expr2, ...
parse_return_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current_token
    lexer.advance(1)
    mut params := ExprArray.new()

    // Try to parse first expression (optional - return might have no value)
    prim := parse_primary(lexer)
    params.push(prim)

    catch(err: Str) {
        // No return value - this is OK for empty return statements
    }

    // Parse additional comma-separated return values
    mut t := lexer.peek()
    mut continue_loop := true
    while continue_loop {
        switch t.token_type {
        case TokenType.Comma:
            lexer.advance(1)
            prim2 := parse_primary(lexer)
            params.push(prim2)
            t = lexer.peek()
        case:
            continue_loop = false
        }
    }

    return Expr.new_parse(NodeType.Return, lexer.get_token(initial_index), params)
}

// Parse an if statement: if cond { body } else { body }
parse_if_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current_token
    lexer.advance(1)
    mut params := ExprArray.new()

    // Parse condition
    prim := parse_primary(lexer)
    params.push(prim)

    // Expect left brace
    t := lexer.peek()
    switch t.token_type {
    case TokenType.LeftBrace:
        lexer.advance(1)
    case:
        throw t.error(format("Expected '{{' after condition in 'if' statement, found '", enum_to_str(t.token_type), "'."))
    }

    // Parse body
    body := parse_body(lexer, TokenType.RightBrace)
    params.push(body)

    // Check for else clause
    else_t := lexer.peek()
    switch else_t.token_type {
    case TokenType.Else:
        lexer.advance(1)
        next_t := lexer.peek()
        switch next_t.token_type {
        case TokenType.LeftBrace:
            lexer.advance(1)
            else_body := parse_body(lexer, TokenType.RightBrace)
            params.push(else_body)
        case:
            throw next_t.error(format("Expected '{{' after 'else', found '", enum_to_str(next_t.token_type), "'."))
        }
    case:
        // No else clause
    }

    return Expr.new_parse(NodeType.If, lexer.get_token(initial_index), params)
}

// Parse a while statement: while cond { body }
parse_while_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current_token
    lexer.advance(1)
    mut params := ExprArray.new()

    // Parse condition
    prim := parse_primary(lexer)
    params.push(prim)

    // Expect left brace
    t := lexer.peek()
    switch t.token_type {
    case TokenType.LeftBrace:
        lexer.advance(1)
    case:
        throw t.error("Expected '{{' after condition in 'while' statement.")
    }

    // Parse body
    body := parse_body(lexer, TokenType.RightBrace)
    params.push(body)

    return Expr.new_parse(NodeType.While, lexer.get_token(initial_index), params)
}

// Parse a catch statement: catch (err: ErrorType) { body }
parse_catch_statement := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    initial_index := lexer.current_token
    lexer.advance(1) // consume 'catch'

    lexer.expect(TokenType.LeftParen) // expect '('

    // Parse the error variable name
    name_token := lexer.peek()
    lexer.expect(TokenType.Identifier)
    name := name_token.token_str
    mut params := ExprArray.new()

    // Create name expression manually
    mut name_expr := Expr()
    name_expr.node_type = NodeType.Identifier(name)
    name_expr.line = name_token.line
    name_expr.col = name_token.col
    params.push(name_expr)

    lexer.expect(TokenType.Colon) // expect ':'

    // Parse the exception type
    type_token := lexer.peek()
    lexer.expect(TokenType.Identifier)

    // Create type expression manually
    mut type_expr := Expr()
    type_expr.node_type = NodeType.Identifier(type_token.token_str)
    type_expr.line = type_token.line
    type_expr.col = type_token.col
    params.push(type_expr)

    lexer.expect(TokenType.RightParen) // expect ')'

    lexer.expect(TokenType.LeftBrace) // expect '{'
    body_expr := parse_body(lexer, TokenType.RightBrace)
    params.push(body_expr)

    return Expr.new_parse(NodeType.Catch, lexer.get_token(initial_index), params)
}

parse_body := proc(mut lexer: Lexer, end_token: TokenType) returns Expr throws Str, IndexOutOfBoundsError, AllocError, FullError, I64_OverflowError {
    mut params := ExprArray.new()
    mut end_found := false
    start_token := lexer.peek()

    while and(
        not(end_found),
        lt(lexer.current_token, lexer.len()),
    ) {
        t := lexer.peek()
        switch t.token_type {
        case end_token:
            end_found = true

        case TokenType.Semicolon:
            lexer.advance(1)

        case:
            stmt := parse_statement(lexer)
            params.push(stmt)
        }
    }
    if end_found {
        return Expr.new_parse(NodeType.Body, start_token, params)
    }
    t := lexer.peek()
    t.error(format(loc(), "Expected the body to end with ", enum_to_str(end_token)))
}
