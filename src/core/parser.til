mode lib

import("src/core/lexer")

INFER_TYPE := "auto"

// Helper to create empty dynamic Expr array
empty_expr_array := func() returns Array {
    return Array.new_dyn("Expr", size_of(Expr))
}

FunctionType := enum {
    FTFunc,
    FTProc,
    FTMacro,
    FTFuncExt,
    FTProcExt,
}

TTypeDef := enum {
    TEnumDef,
    TStructDef,
}

ValueType := enum {
    TFunction: FunctionType,
    TType: TTypeDef,
    TCustom: Str,
    TMulti: Str,
}

SEnumDef := struct {
    mut enum_map: Map = Map()  // Map<Str, ValueType> - variant name to optional payload type
}

Declaration := struct {
    mut name: Str = ""
    mut value_type: ValueType = ValueType.TCustom("")
    mut is_mut: Bool = false
}

// TODO: PatternInfo is a workaround for homogeneity with TIL's lack of tuple syntax
// Once TIL supports tuple notation like (Str, Str), this can be replaced with:
// Pattern(String, String)  // Pattern(variant_name, binding_var)
PatternInfo := struct {
    mut variant_name: Str = ""
    mut binding_var: Str = ""
}

SFuncDef := struct {
    mut function_type: FunctionType = FunctionType.FTFunc
    mut args: Array = Array()              // Array of Declaration
    mut return_types: Array = Array()      // Array of ValueType - "returns" conflicts with TIL keyword
    mut throw_types: Array = Array()       // Array of ValueType - "throws" conflicts with TIL keyword
    mut body: Array = Array()              // Array of Expr

    is_proc := func(self: SFuncDef) returns Bool {
        ft := self.function_type
        switch ft {
        case FunctionType.FTProc:
            return true
        case FunctionType.FTProcExt:
            return true
        case:
            return false
        }
    }

    is_ext := func(self: SFuncDef) returns Bool {
        ft := self.function_type
        switch ft {
        case FunctionType.FTFuncExt:
            return true
        case FunctionType.FTProcExt:
            return true
        case:
            return false
        }
    }
}

SStructDef := struct {
    mut members: Array = Array()           // Array of (Str, Declaration) - field name, declaration
    mut default_values: Map = Map()        // Map<Str, Expr> - field name to default value expression
}

Literal := enum {
    Number: Str,  // TODO support more kinds of numbers
    Str: Str,
    Bool: Str,
    List: Str,  // TODO You can call it tupple too. who cares? it's not even tested yet, just parsed
}

NodeType := enum {
    Body,
    LLiteral: Literal,
    FCall,
    Identifier: Str,
    Declaration: Declaration,
    Assignment: Str,
    FuncDef: SFuncDef,
    EnumDef: SEnumDef,
    StructDef: SStructDef,
    Return,
    Throw,
    Catch,
    If,
    While,
    Switch,
    DefaultCase,
    Range,
    Pattern: PatternInfo,  // Pattern matching for switch case with payload extraction
}

Expr := struct {
    mut node_type: NodeType = NodeType.Body
    mut params: Array = Array()
    mut line: I64 = 0
    mut col: I64 = 0

    new_parse := func(node_type: NodeType, token: Token, params: Array) returns Expr {
        mut e := Expr()
        e.node_type = node_type
        e.params = params
        e.line = token.line
        e.col = token.col
        return e
    }

    new_explicit := func(node_type: NodeType, params: Array, line: I64, col: I64) returns Expr {
        mut e := Expr()
        e.node_type = node_type
        e.params = params
        e.line = line
        e.col = col
        return e
    }

    new_clone := func(node_type: NodeType, e: Expr, params: Array) returns Expr {
        return Expr.new_explicit(node_type, params, e.line, e.col)
    }

    get := func(self: Expr, i: I64) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError {
        if i.lt(self.params.len) {
            mut result := Expr()
            self.params.get(i, result)
            return result
        }
        len_val := self.params.len
        throw self.lang_error("assert", format("Expr index ", i.to_str(), " out of bounds (len: ", len_val.to_str(), ")."))
    }

    exit_error := proc(self: Expr, phase: Str, msg: Str) {
        line_val := self.line
        col_val := self.col
        line_str := line_val.to_str()
        col_str := col_val.to_str()
        if phase.eq("warning") {
            println(format(line_str, ":", col_str, ": ", LANG_NAME, " WARNING: ", msg, "\nExplanation: This should never happen, this is a bug in the language."))
        } else {
            println(format(line_str, ":", col_str, ": ", LANG_NAME, " ", phase, " ERROR: ", msg, "\nExplanation: This should never happen, this is a bug in the language."))
        }
        exit(1)
    }

    lang_error := func(self: Expr, phase: Str, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
        line_val := self.line
        col_val := self.col
        line_str := line_val.to_str()
        col_str := col_val.to_str()
        if phase.eq("warning") {
            return format(line_str, ":", col_str, ": ", LANG_NAME, " WARNING: ", msg, "\nExplanation: This should never happen, this is a bug in the language.")
        }
        return format(line_str, ":", col_str, ": ", LANG_NAME, " ", phase, " ERROR: ", msg, "\nExplanation: This should never happen, this is a bug in the language.")
    }

    todo_error := func(self: Expr, phase: Str, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
        line_val := self.line
        col_val := self.col
        line_str := line_val.to_str()
        col_str := col_val.to_str()
        if phase.eq("warning") {
            return format(line_str, ":", col_str, ": ", LANG_NAME, " WARNING: ", msg, "\nExplanation: Not implemented yet, this is a missing feature in the language.")
        }
        return format(line_str, ":", col_str, ": ", LANG_NAME, " ", phase, " ERROR: ", msg, "\nExplanation: Not implemented yet, this is a missing feature in the language.")
    }

    error := func(self: Expr, phase: Str, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
        line_val := self.line
        col_val := self.col
        line_str := line_val.to_str()
        col_str := col_val.to_str()
        if phase.eq("warning") {
            return format(line_str, ":", col_str, ": WARNING: ", msg)
        }
        return format(line_str, ":", col_str, ": ", phase, " ERROR: ", msg)
    }

    params_clone := func(self: Expr) returns Array throws AllocError, FullError, IndexOutOfBoundsError {
        mut cloned := Array.new_dyn("Expr", size_of(Expr))
        mut i := 0
        while i.lt(self.params.len) {
            mut elem := Expr()
            self.params.get(i, elem)
            cloned.push(elem)
            i = i.add(1)
        }
        return cloned
    }
}

// Helper to format token location string
token_loc := func(t: Token) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
    line_val := t.line
    col_val := t.col
    line_str := line_val.to_str()
    col_str := col_val.to_str()
    return format(line_str, ":", col_str)
}

// Helper to format token error message
token_error := func(t: Token, msg: Str) returns Str throws I64_OverflowError, IndexOutOfBoundsError, AllocError {
    loc_str := token_loc(t)
    return format(loc_str, ": ", msg)
}

// token_type_eq is now defined in lexer.til and imported from there

is_literal := func(t: Token) returns Bool {
    tt := t.token_type
    switch tt {
    case TokenType.String:
        return true
    case TokenType.Number:
        return true
    case TokenType.True:
        return true
    case:
        return false
    }
}

value_type_to_str := func(arg_type: ValueType) returns Str throws Str {
    mut result := ""
    switch arg_type {
    case ValueType.TType(TTypeDef.TEnumDef):
        result = "enum"
    case ValueType.TType(TTypeDef.TStructDef):
        result = "struct"
    case ValueType.TFunction(FunctionType.FTFunc):
        result = "func"
    case ValueType.TFunction(FunctionType.FTFuncExt):
        result = "func"
    case ValueType.TFunction(FunctionType.FTProc):
        result = "proc"
    case ValueType.TFunction(FunctionType.FTProcExt):
        result = "proc"
    case ValueType.TFunction(FunctionType.FTMacro):
        result = "macro"
    case ValueType.TMulti(type_name):
        result = format("{}", type_name)
    case ValueType.TCustom(type_name):
        result = format("{}", type_name)
    case:
        result = "unknown"
    }
    catch (err: I64_OverflowError) {
        throw err.msg
    }
    catch (err: IndexOutOfBoundsError) {
        throw err.msg
    }
    catch (err: AllocError) {
        throw err.msg
    }
    return result
}

str_to_value_type := func(arg_type: Str) returns ValueType {
    if arg_type.eq("func") {
        return ValueType.TFunction(FunctionType.FTFunc)
    }
    if arg_type.eq("proc") {
        return ValueType.TFunction(FunctionType.FTProc)
    }
    if arg_type.eq("macro") {
        return ValueType.TFunction(FunctionType.FTMacro)
    }
    if arg_type.eq("enum") {
        return ValueType.TType(TTypeDef.TEnumDef)
    }
    if arg_type.eq("struct") {
        return ValueType.TType(TTypeDef.TStructDef)
    }
    // Default: custom type
    return ValueType.TCustom(arg_type)
}

ModeDef := struct {
    mut name: Str = ""
    mut allows_procs: Bool = false
    mut allows_base_mut: Bool = false
    mut allows_base_calls: Bool = false
    mut allows_base_anything: Bool = false
    mut needs_main_proc: Bool = false
    mut imports: Array = Array()  // Array of Str
}

can_be_imported := func(mode_def: ModeDef) returns Bool {
    if mode_def.needs_main_proc { return false }  // TODO think harder, why not?
    if mode_def.allows_base_mut { return false }
    if mode_def.allows_base_calls { return false }
    if mode_def.allows_base_anything { return false }
    return true
}

mode_from_name := func(mode_name: Str) returns ModeDef throws Str, FullError, AllocError {
    if mode_name.eq("lib") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = false
        m.allows_base_mut = false
        m.allows_base_anything = false
        m.needs_main_proc = false
        m.imports = Array()
        return m
    }
    if mode_name.eq("pure") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = false
        m.allows_base_calls = false
        m.allows_base_mut = false
        m.allows_base_anything = false
        m.needs_main_proc = false
        m.imports = Array()
        return m
    }
    if mode_name.eq("script") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = true
        m.allows_base_mut = true
        m.allows_base_anything = true
        m.needs_main_proc = false
        m.imports = Array()
        return m
    }
    if mode_name.eq("safe_script") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = true
        m.allows_base_mut = true
        m.allows_base_anything = true
        m.needs_main_proc = false
        m.imports = Array()
        return m
    }
    if mode_name.eq("cli") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = false
        m.allows_base_mut = true
        m.allows_base_anything = false
        m.needs_main_proc = true
        m.imports = Array()
        return m
    }
    if mode_name.eq("test") {
        mut m := ModeDef()
        m.name = mode_name
        m.allows_procs = true
        m.allows_base_calls = true
        m.allows_base_mut = true
        m.allows_base_anything = false
        m.needs_main_proc = false
        mut test_imports := Array.new_dyn("Expr", size_of(Expr))
        test_imports.push("src/core/modes/test")
        m.imports = test_imports
        return m
    }

    throw format("0:0: ", LANG_NAME, " interpreter implementation doesn't support mode '", mode_name, "'")
}

parse_mode := func(path: Str, mut lexer: Lexer) returns ModeDef throws Str, IndexOutOfBoundsError, I64_OverflowError, AllocError, FullError {
    _ := lexer_expect(lexer, TokenType.Mode)  // Add one for mode

    t := lexer.peek()
    tt := t.token_type
    switch tt {
    case TokenType.Identifier:
        // continue
    case:
        throw "0:0: Expected identifier after 'mode'"
    }
    mode_name := t.token_str
    m := mode_from_name(mode_name)

    m_name := m.name
    if m_name.eq("safe_script") {
        throw format(path, ":0:0: mode '", m.name, "' is not properly supported in '", LANG_NAME, "' yet. Try mode 'script' instead")
    }

    _ := lexer_expect(lexer, TokenType.Identifier)  // Add one for the identifier of the mode
    return m
}

parse_literal := func(mut lexer: Lexer, t: Token) returns Expr throws Str, AllocError, FullError {
    params := Array()
    tt := t.token_type
    mut node_type := NodeType.Body  // Default, will be overwritten

    switch tt {
    case TokenType.String:
        node_type = NodeType.LLiteral(Literal.Str(t.token_str))
    case TokenType.Number:
        // TODO: parse and validate number
        node_type = NodeType.LLiteral(Literal.Number(t.token_str))
    case TokenType.True:
        node_type = NodeType.LLiteral(Literal.Bool(t.token_str))
    case:
        throw token_error(t, format("Trying to parse a token that's not a literal as a literal, found '", token_type_to_str(t.token_type), "'."))
    }

    e := Expr.new_parse(node_type, t, params)
    lexer.advance(1)
    return e
}

parse_list := func(mut lexer: Lexer) returns Expr throws Str, I64_OverflowError, AllocError, FullError, IndexOutOfBoundsError {
    mut rightparent_found := false
    mut params := Array.new_dyn("Expr", size_of(Expr))
    initial_current := lexer.current
    _ := lexer_expect(lexer, TokenType.LeftParen)
    mut list_t := lexer.peek()
    mut expect_comma := false

    while and(not(lexer.is_eof(0)), not(rightparent_found)) {
        lt := list_t.token_type
        switch lt {
        case TokenType.RightParen:
            _ := lexer_expect(lexer, TokenType.RightParen)
            rightparent_found = true
        case TokenType.Comma:
            if expect_comma {
                _ := lexer_expect(lexer, TokenType.Comma)
                expect_comma = false
                list_t = lexer.peek()
            } else {
                throw token_error(list_t, "Unexpected ','.")
            }
        case:
            if expect_comma {
                throw token_error(list_t, format("Expected ')' or ',', found '", token_type_to_str(list_t.token_type), "'."))
            }
            expect_comma = true
            prim := parse_primary(lexer)
            params.push(prim)
            list_t = lexer.peek()
        }
    }

    lt := list_t.token_type
    switch lt {
    case TokenType.RightParen:
        // TODO properly parse lists besides function definition arguments
        initial_token := lexer.get_token(initial_current)
        return Expr.new_parse(NodeType.LLiteral(Literal.List("")), initial_token, params)
    case:
        throw token_error(list_t, "Expected closing parentheses.")
    }
}

parse_assignment := func(mut lexer: Lexer, t: Token, name: Str) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    _ := lexer_expect(lexer, TokenType.Equal)
    mut params := Array.new_dyn("Expr", size_of(Expr))
    value := parse_primary(lexer)
    params.push(value)
    return Expr.new_parse(NodeType.Assignment(name), t, params)
}

parse_func_proc_args := func(mut lexer: Lexer) returns Array throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    _ := lexer_expect(lexer, TokenType.LeftParen)
    mut rightparent_found := false
    mut args := Array.new_dyn("Declaration", size_of(Declaration))
    mut t := lexer.peek()
    mut expect_comma := false
    mut expect_colon := false
    mut expect_name := true
    mut is_variadic := false
    mut arg_name := "unnamed"
    mut is_mut := false

    while and(not(lexer.is_eof(0)), not(rightparent_found)) {
        tt := t.token_type
        switch tt {
        case TokenType.RightParen:
            rightparent_found = true
            if expect_colon {
                throw token_error(t, format("Expected ': Type' after arg name '", arg_name, "' before ')'."))
            }
            lexer.advance(1)
        case TokenType.Comma:
            if expect_colon {
                throw token_error(t, format("Expected ': Type' after arg name '", arg_name, "', but found ','."))
            }
            if expect_name {
                throw token_error(t, "Expected arg name before ','.")
            }
            if expect_comma {
                expect_comma = false
                expect_colon = false
                expect_name = true
                is_mut = false
                _ := lexer_expect(lexer, TokenType.Comma)
                t = lexer.peek()
            } else {
                throw token_error(t, "Unexpected ','.")
            }
        case TokenType.Colon:
            if expect_colon {
                expect_colon = false
                expect_name = false
                expect_comma = false
                lexer.advance(1)
                t = lexer.peek()
                tt2 := t.token_type
                switch tt2 {
                case TokenType.Identifier:
                    // continue
                case TokenType.DoubleDot:
                    // continue
                case:
                    throw token_error(t, format("Expected type after '", arg_name, ":', but found '", t.token_str, "'."))
                }
            } else {
                throw token_error(t, "Unexpected ':'.")
            }
        case TokenType.DoubleDot:
            if expect_colon {
                throw token_error(t, format("Expected ': Type' after arg name '", arg_name, "', but found '..'."))
            }
            if expect_comma {
                throw token_error(t, format("Expected ',', found '", token_type_to_str(t.token_type), "'."))
            }
            if expect_name {
                throw token_error(t, format("Expected arg name, found '", token_type_to_str(t.token_type), "'."))
            }
            is_variadic = true
            lexer.advance(1)
            t = lexer.peek()
        case TokenType.Identifier:
            if expect_colon {
                throw token_error(t, format("Expected ': Type' after arg name '", arg_name, "', but found '", t.token_str, "'."))
            }
            if expect_comma {
                throw token_error(t, format("Expected ',', found identifier '", t.token_str, "'."))
            }
            if expect_name {
                arg_name = t.token_str
                expect_colon = true
                expect_name = false
            } else {
                mut decl := Declaration()
                decl.name = arg_name
                if is_variadic {
                    decl.value_type = ValueType.TMulti(t.token_str)
                    is_variadic = false
                } else {
                    decl.value_type = str_to_value_type(t.token_str)
                }
                decl.is_mut = is_mut
                args.push(decl)
                expect_comma = true
                is_mut = false
            }
            lexer.advance(1)
            t = lexer.peek()
        case TokenType.Mut:
            if not(expect_name) {
                throw token_error(t, "Unexpected 'mut' in argument list.")
            }
            is_mut = true
            lexer.advance(1)
            t = lexer.peek()
        case:
            throw token_error(t, format("Unexpected '", token_type_to_str(t.token_type), "' in func/proc args."))
        }
    }

    tt := t.token_type
    switch tt {
    case TokenType.RightParen:
        return args
    case:
        throw token_error(t, "Expected closing parentheses.")
    }
}

func_proc_returns := func(mut lexer: Lexer) returns Array throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    mut end_found := false
    mut return_types := Array.new_dyn("ValueType", size_of(ValueType))
    mut t := lexer.peek()
    lexer.advance(1)
    tt := t.token_type
    switch tt {
    case TokenType.Returns:
        // Continue to parse return types
    case:
        return return_types
    }
    t = lexer.peek()
    mut expect_comma := false

    while and(not(lexer.is_eof(0)), not(end_found)) {
        tt2 := t.token_type
        switch tt2 {
        case TokenType.Throws:
            end_found = true
            lexer.advance(1)
        case TokenType.LeftBrace:
            end_found = true
            lexer.advance(1)
        case TokenType.Semicolon:
            end_found = true
            lexer.advance(1)
        case TokenType.Comma:
            if expect_comma {
                _ := lexer_expect(lexer, TokenType.Comma)
                expect_comma = false
                t = lexer.peek()
            } else {
                throw token_error(t, "Unexpected ','.")
            }
        case TokenType.Identifier:
            if expect_comma {
                throw token_error(t, format("Expected ',', found '", token_type_to_str(t.token_type), "'."))
            }
            return_types.push(str_to_value_type(t.token_str))
            expect_comma = true
            lexer.advance(1)
            t = lexer.peek()
        case:
            throw token_error(t, format("Unexpected '", token_type_to_str(t.token_type), "' in func/proc returns."))
        }
    }

    if end_found {
        return return_types
    } else {
        throw token_error(t, "Expected '{{' or 'throws' after return values.")
    }
}

// TODO DRY with func_proc_returns ?
func_proc_throws := func(mut lexer: Lexer) returns Array throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    mut end_found := false
    mut return_types := Array.new_dyn("ValueType", size_of(ValueType))
    mut t := lexer.previous()
    tt := t.token_type
    switch tt {
    case TokenType.Throws:
        // Continue to parse throw types
    case:
        return return_types
    }
    t = lexer.peek()
    mut expect_comma := false

    while and(not(lexer.is_eof(0)), not(end_found)) {
        tt2 := t.token_type
        switch tt2 {
        case TokenType.LeftBrace:
            end_found = true
            lexer.advance(1)
        case TokenType.Semicolon:
            end_found = true
            lexer.advance(1)
        case TokenType.Comma:
            if expect_comma {
                expect_comma = false
                lexer.advance(1)
                t = lexer.peek()
            } else {
                throw token_error(t, "Unexpected ','.")
            }
        case TokenType.Identifier:
            if expect_comma {
                throw token_error(t, format("Expected ',', found '", token_type_to_str(t.token_type), "'."))
            }
            return_types.push(str_to_value_type(t.token_str))
            expect_comma = true
            lexer.advance(1)
            t = lexer.peek()
        case:
            throw token_error(t, format("Unexpected '", token_type_to_str(t.token_type), "' in func/proc throws."))
        }
    }

    if end_found {
        return return_types
    } else {
        throw token_error(t, "Expected '{{' after throw values.")
    }
}

parse_func_proc_definition := func(mut lexer: Lexer, function_type: FunctionType, do_parse_body: Bool) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    lexer.advance(1)
    t := lexer.peek()
    if lexer.is_eof(1) {
        throw token_error(t, "expected '(' after 'func' or 'proc', found EOF.")
    }
    tt := t.token_type
    switch tt {
    case TokenType.LeftParen:
        // Continue
    case:
        throw token_error(t, format("expected '(' after 'func', found '", token_type_to_str(t.token_type), "'."))
    }
    args := parse_func_proc_args(lexer)
    return_types := func_proc_returns(lexer)
    throw_types := func_proc_throws(lexer)

    mut body := Array.new_dyn("Expr", size_of(Expr))
    if do_parse_body {
        body_expr := parse_body(lexer, TokenType.RightBrace)
        body = body_expr.params
    } else {
        lexer_go_back(lexer, 1)  // Discount the closing brace we won't need
    }

    mut func_def := SFuncDef()
    func_def.function_type = function_type
    func_def.args = args
    func_def.return_types = return_types
    func_def.throw_types = throw_types
    func_def.body = body

    params := Array()
    e := Expr.new_parse(NodeType.FuncDef(func_def), t, params)
    return e
}

enum_definition := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    initial_current := lexer.current
    lexer.advance(1)

    t := lexer.peek()
    tt := t.token_type
    switch tt {
    case TokenType.LeftBrace:
        // Continue
    case:
        throw token_error(t, "Expected '{{' after 'enum'.")
    }
    if lexer.is_eof(1) {
        t2 := lexer.peek()
        throw format(t2.line.to_str(), ":", t2.col.to_str(), ": expected identifier after 'enum {{', found EOF.")
    }
    lexer.advance(1)
    enum_map := Map.new("Str", size_of(Str), "ValueType", size_of(ValueType))

    mut end_found := false
    while and(lexer.current.lt(lexer.len()), not(end_found)) {
        it_t := lexer.peek()
        itt := it_t.token_type
        switch itt {
        case TokenType.RightBrace:
            end_found = true
        case TokenType.Identifier:
            enum_val_name := it_t.token_str
            next_t := lexer.next()
            ntt := next_t.token_type
            switch ntt {
            case TokenType.Colon:
                next2_t := lexer.peek_ahead(2)
                n2tt := next2_t.token_type
                if not(token_type_eq(n2tt, TokenType.Identifier)) {
                    throw token_error(next2_t, format("Expected type identifier after '", enum_val_name, " :', found '", token_type_to_str(next2_t.token_type), "'."))
                }
                enum_val_type := next2_t.token_str
                enum_map.set(enum_val_name, str_to_value_type(enum_val_type))
                lexer.advance(2)
            case TokenType.Comma:
                enum_map.set(enum_val_name, ValueType.TCustom(""))  // None represented as empty TCustom
                // Continue
            case TokenType.RightBrace:
                enum_map.set(enum_val_name, ValueType.TCustom(""))  // None represented as empty TCustom
                end_found = true
            case:
                throw token_error(next_t, format("Expected ',' or ':' after '", enum_val_name, "', found '", token_type_to_str(next_t.token_type), "'."))
            }
        case TokenType.Comma:
            // Skip comma
        case:
            throw token_error(it_t, "Expected '}}' to end enum or a new identifier, found '", token_type_to_str(it_t.token_type), "'.")
        }
        lexer.advance(1)
    }
    if not(end_found) {
        throw token_error(t, "Expected '}}' to end enum.")
    }
    params := Array()
    mut enum_def := SEnumDef()
    enum_def.enum_map = enum_map
    initial_token := lexer.get_token(initial_current)
    return Expr.new_parse(NodeType.EnumDef(enum_def), initial_token, params)
}

parse_struct_definition := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    _ := lexer_expect(lexer, TokenType.Struct)
    t := lexer.peek()
    tt := t.token_type
    switch tt {
    case TokenType.LeftBrace:
        // Continue parsing struct
    case:
        throw token_error(t, "Expected '{{' after 'struct'.")
    }
    if lexer.is_eof(1) {
        t2 := lexer.peek()
        throw format(t2.line.to_str(), ":", t2.col.to_str(), ": expected 'identifier' after 'struct {{', found EOF.")
    }
    lexer.advance(1)
    body := parse_body(lexer, TokenType.RightBrace)

    mut members := Array.new_dyn("Declaration", size_of(Declaration))  // Array of (Str, Declaration)
    mut default_values := Map.new("Str", size_of(Str), "Expr", size_of(Expr))  // Map<Str, Expr>

    mut i := 0
    while i.lt(body.params.len) {
        mut p := Expr()
        body.params.get(i, p)
        nt := p.node_type
        if nt.tag().eq(NodeType.Declaration(Declaration()).tag()) {
            // Extract Declaration from payload
            decl := rsonly_enum_extract_payload(nt)
            members.push(decl)

            if p.params.len.eq(1) {
                mut val := Expr()
                p.params.get(0, val)
                default_values.insert(decl.name, val)
            } else {
                // TODO allow not setting default values in struct members
                throw token_error(t, "all declarations inside struct definitions must have a value for now")
            }
        } else {
            throw token_error(t, "expected only declarations inside struct definition")
        }
        i = i.add(1)
    }

    mut struct_def := SStructDef()
    struct_def.members = members
    struct_def.default_values = default_values
    params := Array()
    return Expr.new_parse(NodeType.StructDef(struct_def), t, params)
}

parse_primary_identifier := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    initial_current := lexer.current
    t := lexer.peek()
    mut next_t := lexer.next()
    mut current_identifier := t.token_str
    mut params := Array.new_dyn("Expr", size_of(Expr))

    mut continue_dot_loop := true
    while continue_dot_loop {
        switch next_t.token_type {
        case TokenType.Dot:
            next2_t := lexer.peek_ahead(2)
            n2tt := next2_t.token_type
            switch n2tt {
            case TokenType.Identifier:
                current_identifier = next2_t.token_str
                lexer.advance(2)
                empty_params := empty_expr_array()
                ident_expr := Expr.new_parse(NodeType.Identifier(current_identifier), t, empty_params)
                params.push(ident_expr)
                next_t = lexer.next()
            case:
                throw token_error(next2_t, format("expected identifier after '", current_identifier, ".', found '", token_type_to_str(next2_t.token_type), "'."))
            }
        case:
            continue_dot_loop = false
        }
    }

    initial_token := lexer.get_token(initial_current)
    e := Expr.new_parse(NodeType.Identifier(t.token_str), initial_token, params)
    lexer.advance(1)

    ntt := next_t.token_type
    switch ntt {
    case TokenType.LeftParen:
        arg_list := parse_list(lexer)
        mut fcall_params := Array.new_dyn("Expr", size_of(Expr))
        fcall_params.push(e)
        mut i := 0
        params_ref := arg_list.params
        while i.lt(params_ref.len) {
            mut arg := Expr()
            params_ref.get(i, arg)
            fcall_params.push(arg)
            i = i.add(1)
        }
        initial_token2 := lexer.get_token(initial_current)
        mut result := Expr.new_parse(NodeType.FCall, initial_token2, fcall_params)

        // Handle chained method calls: a.method1().method2().method3()
        mut continue_loop := true
        while continue_loop {
            peek_t := lexer.peek()
            ptt := peek_t.token_type
            switch ptt {
            case TokenType.Dot:
                // Consume the dot
                lexer.advance(1)

                // Expect an identifier for the next method name
                method_t := lexer.peek()
                mtt := method_t.token_type
                switch mtt {
                case TokenType.Identifier:
                    method_name := method_t.token_str
                    lexer.advance(1)

                    // Check if it's a method call (has parentheses)
                    next_peek := lexer.peek()
                    nptt := next_peek.token_type
                    switch nptt {
                    case TokenType.LeftParen:
                        // Continue with method call parsing below
                    case:
                        throw format(next_peek.line.to_str(), ":", next_peek.col.to_str(), ": Expected '(' after method name '", method_name, "', found '", token_type_to_str(next_peek.token_type), "'")
                    }
                case:
                    throw token_error(method_t, "Expected method name after '.', found '", token_type_to_str(method_t.token_type), "'")
                }
            case:
                continue_loop = false
            }

            if continue_loop {

                // Parse the argument list
                method_args := parse_list(lexer)

                // Create a new FCall with the method name as identifier and previous result as first arg
                // This represents: method_name(result, args...)
                empty_params := empty_expr_array()
                method_id := Expr.new_parse(NodeType.Identifier(method_name), method_t, empty_params)
                mut new_params := Array.new_dyn("Expr", size_of(Expr))
                new_params.push(method_id)
                new_params.push(result)  // Previous call result becomes first argument
                mut j := 0
                while j.lt(method_args.params.len) {
                    mut method_arg := Expr()
                    method_args.params.get(j, method_arg)
                    new_params.push(method_arg)
                    j = j.add(1)
                }

                result = Expr.new_parse(NodeType.FCall, method_t, new_params)
            }
        }

        return result
    case:
        return e
    }
}

get_combined_name := func(e: Expr) returns Str throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    mut to_return := ""
    nt := e.node_type
    if nt.tag().eq(NodeType.Identifier("").tag()) {
        id_str := rsonly_enum_extract_payload(nt)
        to_return = id_str
        to_return = to_return.add(".")
    } else {
        throw e.lang_error("parse", "get_combined_name() is to be called with Identifier expressions only")
    }

    mut i := 0
    while i.lt(e.params.len) {
        mut p := Expr()
        e.params.get(i, p)
        pnt := p.node_type
        if pnt.tag().eq(NodeType.Identifier("").tag()) {
            id_str := rsonly_enum_extract_payload(pnt)
            to_return = to_return.add(id_str)
            to_return = to_return.add(".")
        } else {
            throw e.lang_error("parse", "the params of an identifier expression must be Identifier expressions only")
        }
        i = i.add(1)
    }

    // Remove the last '.'
    to_return = to_return.substring(0, to_return.len.sub(1))
    return to_return
}

parse_statement_identifier := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    t := lexer.peek()
    mut next_t := lexer.next()
    mut next_token_type := next_t.token_type

    ntt := next_token_type
    if token_type_eq(ntt, TokenType.LeftParen) {
        return parse_primary_identifier(lexer)
    }

    if token_type_eq(ntt, TokenType.Dot) {
        e := parse_primary_identifier(lexer)
        ent := e.node_type
        if ent.tag().eq(NodeType.FCall.tag()) {
            return e
        }
        if not(ent.tag().eq(NodeType.Identifier("").tag())) {
            throw token_lang_error(t, "a series of identifiers and dots should have been parsed as identifier or function call")
        }

        next_t = lexer.peek()
        next_token_type = next_t.token_type
        ntt2 := next_token_type
        if token_type_eq(ntt2, TokenType.Equal) {
            name := get_combined_name(e)
            return parse_assignment(lexer, t, name)
        } else {
            throw token_lang_error(t, "While parsing a '.', this should never happen")
        }
    }

    if token_type_eq(ntt, TokenType.Equal) {
        lexer.advance(1)
        return parse_assignment(lexer, t, t.token_str)
    }

    if token_type_eq(ntt, TokenType.Colon) {
        next_next_t := lexer.peek_ahead(2)
        next_next_token_type := next_next_t.token_type
        identifier := t.token_str

        nntt := next_next_token_type
        if token_type_eq(nntt, TokenType.Identifier) {
            type_name := next_next_t.token_str
            return parse_declaration(lexer, false, type_name)
        }
        if token_type_eq(nntt, TokenType.Equal) {
            return parse_declaration(lexer, false, INFER_TYPE)
        }
        throw token_error(t, format("Expected Type or '=' after '", identifier, " :' in statement, found '", token_type_to_str(next_next_token_type), "'."))
    }

    throw token_error(t, format("Expected '(', ':' or '=' after identifier in statement, found '", token_type_to_str(next_token_type), "'."))
}

parse_primary := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    t := lexer.peek()
    if is_literal(t) {
        return parse_literal(lexer, t)
    }

    tt := t.token_type
    if token_type_eq(tt, TokenType.Func) {
        return parse_func_proc_definition(lexer, FunctionType.FTFunc, true)
    }
    if token_type_eq(tt, TokenType.FuncExt) {
        return parse_func_proc_definition(lexer, FunctionType.FTFuncExt, false)
    }
    if token_type_eq(tt, TokenType.Macro) {
        return parse_func_proc_definition(lexer, FunctionType.FTMacro, true)
    }
    if token_type_eq(tt, TokenType.Proc) {
        return parse_func_proc_definition(lexer, FunctionType.FTProc, true)
    }
    if token_type_eq(tt, TokenType.ProcExt) {
        return parse_func_proc_definition(lexer, FunctionType.FTProcExt, false)
    }
    if token_type_eq(tt, TokenType.Enum) {
        return enum_definition(lexer)
    }
    if token_type_eq(tt, TokenType.Struct) {
        return parse_struct_definition(lexer)
    }
    if token_type_eq(tt, TokenType.LeftParen) {
        return parse_list(lexer)
    }
    if token_type_eq(tt, TokenType.Identifier) {
        return parse_primary_identifier(lexer)
    }

    throw token_error(t, format("Expected primary expression, found '", token_type_to_str(t.token_type), "'."))
}

return_statement := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    initial_current := lexer.current
    lexer.advance(1)
    mut params := Array.new_dyn("Expr", size_of(Expr))

    // Try to parse first primary expression
    prim := parse_primary(lexer)
    params.push(prim)
    catch (err: Str) {
        // No primary expression found, that's okay for return statements
    }

    mut t := lexer.peek()
    mut continue_loop := true
    while continue_loop {
        tt := t.token_type
        if not(token_type_eq(tt, TokenType.Comma)) {
            continue_loop = false
        } else {
            lexer.advance(1)
            prim2 := parse_primary(lexer)
            params.push(prim2)
            t = lexer.peek()
        }
    }

    initial_token := lexer.get_token(initial_current)
    return Expr.new_parse(NodeType.Return, initial_token, params)
}

parse_throw_statement := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    initial_current := lexer.current
    lexer.advance(1)
    mut params := Array.new_dyn("Expr", size_of(Expr))

    // Try to parse first primary expression
    prim := parse_primary(lexer)
    params.push(prim)
    catch (err: Str) {
        // No primary expression found, that's okay for throw statements
    }

    mut t := lexer.peek()
    mut continue_loop := true
    while continue_loop {
        tt := t.token_type
        if not(token_type_eq(tt, TokenType.Comma)) {
            continue_loop = false
        } else {
            lexer.advance(1)
            prim2 := parse_primary(lexer)
            params.push(prim2)
            t = lexer.peek()
        }
    }

    initial_token := lexer.get_token(initial_current)
    return Expr.new_parse(NodeType.Throw, initial_token, params)
}

parse_catch_statement := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    initial_current := lexer.current
    lexer.advance(1) // consume 'catch'

    _ := lexer_expect(lexer, TokenType.LeftParen) // expect '('

    // Parse the error variable name
    name_token := lexer_expect(lexer, TokenType.Identifier)
    name := name_token.token_str
    empty_params := empty_expr_array()
    name_expr := Expr.new_parse(NodeType.Identifier(name), name_token, empty_params)

    _ := lexer_expect(lexer, TokenType.Colon) // expect ':'

    // Parse the exception type
    type_token := lexer_expect(lexer, TokenType.Identifier)
    empty_params2 := empty_expr_array()
    type_expr := Expr.new_parse(NodeType.Identifier(type_token.token_str), type_token, empty_params2)

    _ := lexer_expect(lexer, TokenType.RightParen) // expect ')'

    _ := lexer_expect(lexer, TokenType.LeftBrace) // expect '{'
    body_expr := parse_body(lexer, TokenType.RightBrace)

    mut params := Array.new_dyn("Expr", size_of(Expr))
    params.push(name_expr)
    params.push(type_expr)
    params.push(body_expr)

    initial_token := lexer.get_token(initial_current)
    return Expr.new_parse(NodeType.Catch, initial_token, params)
}

if_statement := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    initial_current := lexer.current
    lexer.advance(1)
    mut params := Array.new_dyn("Expr", size_of(Expr))

    prim := parse_primary(lexer)
    params.push(prim)

    t := lexer.peek()
    tt := t.token_type
    switch tt {
    case TokenType.LeftBrace:
        lexer.advance(1)
    case:
        throw token_error(t, format("Expected '{{' after condition in 'if' statement, found '", token_type_to_str(t.token_type), "'."))
    }

    body := parse_body(lexer, TokenType.RightBrace)
    params.push(body)

    t2 := lexer.peek()
    t2tt := t2.token_type
    if token_type_eq(t2tt, TokenType.Else) {
        lexer.advance(1)
        next := lexer.peek()
        ntt := next.token_type
        if token_type_eq(ntt, TokenType.If) {
            nested_if := if_statement(lexer)
            params.push(nested_if)
        } else {
            if token_type_eq(ntt, TokenType.LeftBrace) {
                lexer.advance(1)
                else_body := parse_body(lexer, TokenType.RightBrace)
                params.push(else_body)
            } else {
                throw token_error(t2, format("Expected '{{' or 'if' after 'else', found '", token_type_to_str(next.token_type), "'."))
            }
        }
    }

    initial_token := lexer.get_token(initial_current)
    return Expr.new_parse(NodeType.If, initial_token, params)
}

while_statement := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    initial_current := lexer.current
    lexer.advance(1)
    mut params := Array.new_dyn("Expr", size_of(Expr))

    prim := parse_primary(lexer)
    params.push(prim)

    t := lexer.peek()
    tt := t.token_type
    switch tt {
    case TokenType.LeftBrace:
        lexer.advance(1)
    case:
        throw token_error(t, "Expected '{{' after condition in 'while' statement.")
    }

    body := parse_body(lexer, TokenType.RightBrace)
    params.push(body)

    initial_token := lexer.get_token(initial_current)
    return Expr.new_parse(NodeType.While, initial_token, params)
}

parse_for_statement := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    initial_token := lexer.peek()
    lexer.advance(1) // consume 'for'

    // Expect loop variable name
    ident_token := lexer_expect(lexer, TokenType.Identifier)
    loop_var_name := ident_token.token_str

    _ := lexer_expect(lexer, TokenType.In)

    // Parse the range expression (e.g., 1..10)
    range_expr := parse_case_expr(lexer)
    rent := range_expr.node_type
    if not(rent.eq(NodeType.Range)) {
        throw token_error(ident_token, "Expected range expression (start..end) after 'in'")
    }
    start_expr := range_expr.get(0)
    end_expr := range_expr.get(1)

    _ := lexer_expect(lexer, TokenType.LeftBrace)
    body_expr := parse_body(lexer, TokenType.RightBrace)

    // let loop_var := <start_expr>
    mut decl := Declaration()
    decl.name = loop_var_name
    decl.value_type = str_to_value_type(INFER_TYPE)
    decl.is_mut = true
    mut decl_params := Array.new_dyn("Expr", size_of(Expr))
    decl_params.push(start_expr)
    decl_expr := Expr.new_parse(NodeType.Declaration(decl), initial_token, decl_params)

    // while <loop_var> < <end_expr> {
    // Create: loop_var.lt(end_expr)
    mut lt_ident_params := Array.new_dyn("Expr", size_of(Expr))
    empty_params := empty_expr_array()
    lt_expr := Expr.new_parse(NodeType.Identifier("lt"), initial_token, empty_params)
    lt_ident_params.push(lt_expr)
    loop_var_ident := Expr.new_parse(NodeType.Identifier(loop_var_name), initial_token, lt_ident_params)

    mut cond_params := Array.new_dyn("Expr", size_of(Expr))
    cond_params.push(loop_var_ident)
    cond_params.push(end_expr)
    cond_expr := Expr.new_explicit(NodeType.FCall, cond_params, initial_token.line, initial_token.col)

    // Create: loop_var.inc()
    mut inc_ident_params := Array.new_dyn("Expr", size_of(Expr))
    empty_params2 := empty_expr_array()
    inc_expr := Expr.new_parse(NodeType.Identifier("inc"), initial_token, empty_params2)
    inc_ident_params.push(inc_expr)
    mut inc_params := Array.new_dyn("Expr", size_of(Expr))
    loop_var_inc_expr := Expr.new_parse(NodeType.Identifier(loop_var_name), initial_token, inc_ident_params)
    inc_params.push(loop_var_inc_expr)
    inc_expr := Expr.new_explicit(NodeType.FCall, inc_params, initial_token.line, initial_token.col)

    // Add inc_expr to body
    mut while_body_params := body_expr.params_clone()
    while_body_params.push(inc_expr)
    while_body := Expr.new_explicit(NodeType.Body, while_body_params, body_expr.line, body_expr.col)

    // Create while expression
    mut while_params := Array.new_dyn("Expr", size_of(Expr))
    while_params.push(cond_expr)
    while_params.push(while_body)
    while_expr := Expr.new_explicit(NodeType.While, while_params, initial_token.line, initial_token.col)

    // Return body with declaration and while loop
    mut final_params := Array.new_dyn("Expr", size_of(Expr))
    final_params.push(decl_expr)
    final_params.push(while_expr)
    return Expr.new_explicit(NodeType.Body, final_params, initial_token.line, initial_token.col)
}

// Helper function to extract full identifier name from an expression
// Handles both simple identifiers and dotted names (represented as FCall)
get_full_identifier_name := func(e: Expr) returns Str throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    ent := e.node_type
    if ent.tag().eq(NodeType.Identifier("").tag()) {
        name := rsonly_enum_extract_payload(ent)
        // Check if this is a dotted name: Identifier with params
        // For example, "Color.Green" is Identifier("Color") with params=[Identifier("Green")]
        if e.params.len.eq(1) {
            p0 := e.params.get(0)
            p0nt := p0.node_type
            if p0nt.tag().eq(NodeType.Identifier("").tag()) {
                param_name := rsonly_enum_extract_payload(p0nt)
                return format(name, ".", param_name)
            }
        }
        return name
    }
    if ent.tag().eq(NodeType.FCall.tag()) {
        if e.params.len.gte(1) {
            // For FCall, try to extract the function name
            return get_full_identifier_name(e.params.get(0))
        }
    }
    return ""
}

parse_case_expr := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    left := parse_primary(lexer)
    t := lexer.peek()
    tt := t.token_type
    if token_type_eq(tt, TokenType.DoubleDot) {
        lexer.advance(1)
        right := parse_primary(lexer)
        mut params := Array.new_dyn("Expr", size_of(Expr))
        params.push(left)
        params.push(right)
        return Expr.new_parse(NodeType.Range, t, params)
    }

    // Check if this is a pattern match: EnumVariant(binding_var)
    // This would have been parsed as FCall with one Identifier parameter
    lnt := left.node_type
    if lnt.tag().eq(NodeType.FCall.tag()) {
        if left.params.len.eq(2) {
            // FCall params are: [function_name, arg1, arg2, ...]
            // For pattern matching, we expect: [variant_identifier, binding_identifier]
            // Note: variant_identifier might be a dotted name like "Color.Green" which could be
            // represented as an FCall itself (for the dot access)

            // Get the full variant name (handling dotted names)
            variant_name := get_full_identifier_name(left.params.get(0))

            p1 := left.params.get(1)
            p1nt := p1.node_type
            if p1nt.tag().eq(NodeType.Identifier("").tag()) {
                binding_var := rsonly_enum_extract_payload(p1nt)
                // Convert FCall to Pattern
                mut pattern_info := PatternInfo()
                pattern_info.variant_name = variant_name
                pattern_info.binding_var = binding_var
                return Expr.new_explicit(NodeType.Pattern(pattern_info), Array(), left.line, left.col)
            }
        }
    }

    return left
}

parse_switch_statement := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    t := lexer.peek()
    initial_current := lexer.current
    lexer.advance(1)
    mut params := Array.new_dyn("Expr", size_of(Expr))

    prim := parse_primary(lexer)
    params.push(prim)

    peek_t := lexer.peek()
    ptt := peek_t.token_type
    if not(token_type_eq(ptt, TokenType.LeftBrace)) {
        throw token_error(t, "Expected '{{' after primary expression in 'switch' statement.")
    }
    lexer.advance(1)

    mut end_found := false
    mut continue_outer := true
    while and(lexer.current.lt(lexer.len()), not(end_found)) {
        if not(continue_outer) {
            continue_outer = true
        }

        mut next_t := lexer.peek()
        ntt := next_t.token_type
        if not(token_type_eq(ntt, TokenType.Case)) {
            throw token_error(next_t, format("Expected 'case' in switch, found '", token_type_to_str(next_t.token_type), "'"))
        }

        lexer.advance(1)
        next_t = lexer.peek()
        ntt2 := next_t.token_type
        if token_type_eq(ntt2, TokenType.Colon) {
            empty_params := empty_expr_array()
            default_case_expr := Expr.new_parse(NodeType.DefaultCase, t, empty_params)
            params.push(default_case_expr)
        } else {
            prim2 := parse_case_expr(lexer)
            params.push(prim2)
        }

        next_t = lexer.peek()
        ntt3 := next_t.token_type
        if not(token_type_eq(ntt3, TokenType.Colon)) {
            throw token_error(next_t, format("Expected ':' case <primary_expr> in switch, found '", token_type_to_str(next_t.token_type), "'"))
        }

        lexer.advance(1)
        next_t = lexer.peek()
        mut body_params := Array.new_dyn("Expr", size_of(Expr))
        mut continue_inner := true
        while lexer.current.lt(lexer.len()) {
            if not(continue_inner) {
                continue_inner = true
            }

            ntt4 := next_t.token_type
            if token_type_eq(ntt4, TokenType.RightBrace) {
                body_expr := Expr.new_parse(NodeType.Body, t, body_params)
                params.push(body_expr)
                end_found = true
                lexer.advance(1)
                continue_inner = false
                continue_outer = false
            } else {
                if token_type_eq(ntt4, TokenType.Case) {
                    body_expr := Expr.new_parse(NodeType.Body, t, body_params)
                params.push(body_expr)
                    continue_inner = false
                } else {
                    stmt := parse_statement(lexer)
                    body_params.push(stmt)
                    next_t = lexer.peek()
                    ntt5 := next_t.token_type
                    if token_type_eq(ntt5, TokenType.Semicolon) {
                        lexer.advance(1)
                        next_t = lexer.peek()
                    }
                }
            }
        }
    }

    if end_found {
        initial_token := lexer.get_token(initial_current)
        return Expr.new_parse(NodeType.Switch, initial_token, params)
    }
    throw token_error(t, "Expected '}}' to end switch.")
}

parse_declaration := func(mut lexer: Lexer, is_mut: Bool, explicit_type: Str) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    t := lexer.peek()
    decl_name := t.token_str
    initial_current := lexer.current

    lexer.advance(3) // skip identifier, colon and equal
    if not(explicit_type.eq(INFER_TYPE)) {
        lexer.advance(1) // skip type identifier
    }

    mut params := Array.new_dyn("Expr", size_of(Expr))
    value := parse_primary(lexer)
    params.push(value)

    mut decl := Declaration()
    decl.name = decl_name
    decl.value_type = str_to_value_type(explicit_type)
    decl.is_mut = is_mut
    initial_token := lexer.get_token(initial_current)
    return Expr.new_parse(NodeType.Declaration(decl), initial_token, params)
}

parse_mut_declaration := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    t := lexer.peek()
    mut next_t := lexer.next()
    mut next_token_type := next_t.token_type

    ntt := next_token_type
    if not(token_type_eq(ntt, TokenType.Identifier)) {
        throw token_error(t, format("Expected identifier after 'mut', found '", token_type_to_str(next_token_type), "'."))
    }
    identifier := next_t.token_str
    lexer.advance(1)
    next_t = lexer.next()
    next_token_type = next_t.token_type

    ntt2 := next_token_type
    if not(token_type_eq(ntt2, TokenType.Colon)) {
        throw token_error(t, format("Expected ':' after 'mut ", identifier, "', found '", token_type_to_str(next_token_type), "'."))
    }
    next_next_t := lexer.peek_ahead(2)
    next_next_token_type := next_next_t.token_type

    nntt := next_next_token_type
    if token_type_eq(nntt, TokenType.Identifier) {
        type_name := next_next_t.token_str
        return parse_declaration(lexer, true, type_name)
    }
    if token_type_eq(nntt, TokenType.Equal) {
        return parse_declaration(lexer, true, INFER_TYPE)
    }
    throw token_error(t, format("Expected a type identifier or '=' after 'mut ", identifier, " :' in statement, found '", token_type_to_str(next_next_token_type), "'."))
}

parse_statement := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    t := lexer.peek()
    tt := t.token_type

    if token_type_eq(tt, TokenType.Return) {
        return return_statement(lexer)
    }
    if token_type_eq(tt, TokenType.Throw) {
        return parse_throw_statement(lexer)
    }
    if token_type_eq(tt, TokenType.If) {
        return if_statement(lexer)
    }
    if token_type_eq(tt, TokenType.While) {
        return while_statement(lexer)
    }
    if token_type_eq(tt, TokenType.For) {
        return parse_for_statement(lexer)
    }
    if token_type_eq(tt, TokenType.Switch) {
        return parse_switch_statement(lexer)
    }
    if token_type_eq(tt, TokenType.Mut) {
        return parse_mut_declaration(lexer)
    }
    if token_type_eq(tt, TokenType.Identifier) {
        return parse_statement_identifier(lexer)
    }
    if token_type_eq(tt, TokenType.Catch) {
        return parse_catch_statement(lexer)
    }
    if token_type_eq(tt, TokenType.LeftBrace) {
        lexer.advance(1) // Skip LeftBrace
        return parse_body(lexer, TokenType.RightBrace)
    }

    throw token_error(t, format("Expected statement, found ", token_type_to_str(t.token_type), "."))
}

parse_body := func(mut lexer: Lexer, end_token: TokenType) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    initial_current := lexer.current
    mut params := Array.new_dyn("Expr", size_of(Expr))
    mut end_found := false
    mut t := lexer.peek() // track last token for error reporting

    mut continue_loop := true
    while and(lexer.current.lt(lexer.len()), not(end_found)) {
        if not(continue_loop) {
            continue_loop = true
        }

        t = lexer.peek() // update t with the current token
        token_type := t.token_type
        mut matches_end := false
        switch token_type {
        case end_token:
            matches_end = true
        case:
            matches_end = false
        }
        if matches_end {
            lexer.advance(1)
            end_found = true
            continue_loop = false
        } else {
            mut is_semicolon := false
            switch token_type {
            case TokenType.Semicolon:
                is_semicolon = true
            case:
                is_semicolon = false
            }
            if is_semicolon {
                _ := lexer_expect(lexer, TokenType.Semicolon) // REM: This is suboptimal but more clear in grep
            } else {
                stmt := parse_statement(lexer)
                params.push(stmt)
            }
        }
    }

    if end_found {
        initial_token := lexer.get_token(initial_current)
        return Expr.new_parse(NodeType.Body, initial_token, params)
    }
    throw token_error(t, format("Expected '", token_type_to_str(end_token), "' to end body."))
}

parse_tokens := func(mut lexer: Lexer) returns Expr throws Str, AllocError, FullError, IndexOutOfBoundsError, I64_OverflowError {
    e := parse_body(lexer, TokenType.Eof)

    mut i := lexer.current
    mut unparsed_tokens := 0
    lexer_len_val := lexer.len()

    if i.lt(lexer_len_val) {
        unparsed_tokens = lexer_len_val.sub(i)
    }

    if unparsed_tokens.gt(0) {
        println(format("Total tokens parsed: ", lexer.current.to_str(), "/", lexer_len_val.to_str()))
    }

    while i.lt(lexer_len_val) {
        t := lexer.get_token(i)
        println(format("Token: ", token_to_str(t)))
        i = i.add(1)
    }

    if unparsed_tokens.gt(0) {
        throw format("Total unparsed tokens: ", unparsed_tokens.to_str(), "/", lexer_len_val.to_str())
    }

    return e
}
