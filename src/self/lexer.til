mode lib

import("core.vec")
import("std.std")
import("std.io")            // readfile

LANG_NAME := "til"
DEBUG_COMPILER := true

TokenType := enum {
    // basic
    Eof,

    // Single-character tokens.
    Minus, Plus, Slash, Star,
    LeftParen, RightParen, LeftBrace, RightBrace, LeftBracket, RightBracket,
    Comma, Colon,

    // One or two character tokens.
    Dot, DoubleDot,
    Not, NotEqual,
    Equal, EqualEqual,
    Greater, GreaterEqual,
    Lesser, LesserEqual,
    Semicolon, DoubleSemicolon,

    // Literals.
    Identifier, String, Number,

    // Reserved words:
    Mut, Copy, Own,

    // type definition
    Struct, Enum,

    // function definition
    Returns, Throws,

    // flow control
    If, Else,
    While, For, In,
    Match, Switch, Default, Case,
    Return, Throw,
    Catch,

    // Special in this language:
    Mode,
    Func, Proc, Macro,
    FuncExt, ProcExt,

    // Errors (unsupported tokens)
    Const, Var,
    Fn, Function,
    Try,
    Let, Class,
    Invalid,
    UnterminatedString,
    UnterminatedComment,
}

Token := struct {
    mut token_type: TokenType = TokenType.Invalid
    mut token_str: Str = ""
    mut line: I64 = 0
    mut col: I64 = 0

    lang_error := func(self: Token, path: Str, msg: Str) returns Str throws Str {
        result := format(path, ":", self.line.to_str(), ":", self.col.to_str(), ": ",
            LANG_NAME, " parse ERROR: ", msg, "\n",
            "Explanation: This is not your fault as a user, this is a bug in the language.")
        catch (err: I64_OverflowError) { throw format("Token.lang_error: ", err.msg) }
        catch (err: IndexOutOfBoundsError) { throw format("Token.lang_error: ", err.msg) }
        catch (err: AllocError) { throw format("Token.lang_error: ", err.msg) }
        return result
    }

    todo_error := func(self: Token, path: Str, msg: Str) returns Str throws Str {
        result := format(path, ":", self.line.to_str(), ":", self.col.to_str(), ": ",
            LANG_NAME, " parse ERROR: ", msg, "\n",
            "Explanation: Not implemented yet, this is a missing feature in the language.")
        catch (err: I64_OverflowError) { throw format("Token.todo_error: ", err.msg) }
        catch (err: IndexOutOfBoundsError) { throw format("Token.todo_error: ", err.msg) }
        catch (err: AllocError) { throw format("Token.todo_error: ", err.msg) }
        return result
    }

    error := func(self: Token, path: Str, msg: Str) returns Str throws Str {
        result := format(path, ":", self.line.to_str(), ":", self.col.to_str(), ": parse ERROR: ", msg)
        catch (err: I64_OverflowError) { throw format("Token.error: ", err.msg) }
        catch (err: IndexOutOfBoundsError) { throw format("Token.error: ", err.msg) }
        catch (err: AllocError) { throw format("Token.error: ", err.msg) }
        return result
    }
}

scan_push_token := proc(mut tokens: Vec, token_type: TokenType, token_str: Str, line: I64, col: I64) throws Str {
    mut t := Token()
    t.token_type = token_type
    t.token_str = token_str
    t.line = line
    t.col = col

    tokens.push(t)
    catch (err: AllocError) {
        throw format(line.to_str(), ":", col.to_str(), ": scan_tokens: AllocError while pushing token: ", err.msg)
    }
}

is_digit := func(source: Str, pos: I64) returns Bool throws Str {
    switch source.get_substr(pos, pos.add(1)) {
    case "0".."9": return true
    case: return false
    }
    catch (err: IndexOutOfBoundsError) {
        throw format("is_digit: ", err.msg)
    }
    catch (err: AllocError) {
        throw format("is_digit: ", err.msg)
    }
}

is_id_start := func(source: Str, pos: I64) returns Bool throws Str {
    // current_char := Str.get_substr(source, pos, add(pos, 1)) // TODO FIX throws check doesn't propagate for associated functions (unless called as methods)
    current_char := source.get_substr(pos, pos.add(1))
    switch current_char {
    case "a".."z": return true
    case "A".."Z": return true
    case "_": return true
    case:
    }
    catch (err: IndexOutOfBoundsError) {
        throw format("is_id_start: ", err.msg)
    }
    catch (err: AllocError) {
        throw format("is_id_start: ", err.msg)
    }
    return false
}

scan_reserved_words := func(identifier: Str) returns TokenType {
    switch identifier {
    case "mode": return TokenType.Mode
        // declaration/arg modifiers
    case "mut": return TokenType.Mut
    case "copy": return TokenType.Copy
    case "own": return TokenType.Own
        // core data types
    case "enum": return TokenType.Enum
    case "struct": return TokenType.Struct
        // function declaration
    case "returns": return TokenType.Returns
        // Anything that can be thrown must be explicitly declared in the function via 'throws', java style.
        // Except perhaps PanicException or something like that which can be implicit, but still allowed to documment redundantly
        // or perhaps not, for that may degenerate in an extra warning option
        // perhaps just force the user to explicitly catch and exit any potential panic from the callee
    case "throws": return TokenType.Throws // TODO implement
    case "func": return TokenType.Func
    case "proc": return TokenType.Proc
    case "macro": return TokenType.Macro // TODO implement for real once we compile
    case "ext_func": return TokenType.FuncExt // this has to link when we compile
    case "ext_proc": return TokenType.ProcExt // this has to link when we compile

        // control flow
    case "if": return TokenType.If
    case "else": return TokenType.Else
    case "while": return TokenType.While
    case "for": return TokenType.For
    case "in": return TokenType.In
    case "switch": return TokenType.Switch
    case "match": return TokenType.Match // TODO like switch but special for declarations/assignments
    case "case": return TokenType.Case
    case "default": return TokenType.Default // TODO currently using "case:", but "default:" is more traditional, grepable and overt
    case "return": return TokenType.Return
    case "throw": return TokenType.Throw
        // TODO throw should just act as a return that gets post-processed by the next catch or rethrown
    case "catch": return TokenType.Catch
        // or should 'try:' be optional?

        // Reserved forbidden/illegal words (intentionally unsupported reserved words)
        // TODO intentionally unsupport more reserved words
        // TODO nicer messages for forbidden words
    case "fn": return TokenType.Fn
    case "function": return TokenType.Function
    case "try": return TokenType.Try // TIL uses catch without try (forbidden for users from other languages)
    case "let": return TokenType.Let
    case "class": return TokenType.Class
    case "global": return TokenType.Invalid // just use mut declaration in the root of the file, but they're not allowed in all modes
        // const/vars are the most abstract types, you can't even explicitly declare them
    case "const": return TokenType.Const
    case "var": return TokenType.Var
        // Do we really need const fields static other than static? (ie can be different per instance, but not modified afterwards)
        // The answer is probably yet, but perhaps static is not the right answer
        // how about this? if it's in the struct body, it is const, if it is in impl, it is static, just like functions\
        // or do we need mut function fields too? probably yes
    case "static": return TokenType.Invalid
    case:
    }
    return TokenType.Identifier
}

scan_tokens := proc(source: Str) returns Vec throws Str {
    mut tokens := Vec.new(Token)

    eof_pos := source.len()
    mut pos := 0
    mut line := 1
    mut start_line_pos := sub(0, 1)

    while pos.lt(eof_pos) {
        start := pos
        if source.is_digit(pos) {
            mut continue_scanning := true
            while continue_scanning {
                pos.inc()
                if pos.lt(eof_pos) {
                    if not(source.is_digit(pos)) {
                        continue_scanning = false
                    }
                } else {
                    continue_scanning = false
                }
            }
            // Look for a fractional part.
            if pos.lt(eof_pos) {
                if source.get_substr(pos, pos.add(1)).eq(".") {
                    if pos.add(1).lt(eof_pos) {
                        if source.is_digit(pos.add(1)) {
                            pos.inc()
                            mut continue_scanning2 := true
                            while continue_scanning2 {
                                pos.inc()
                                if pos.lt(eof_pos) {
                                    if not(source.is_digit(pos)) {
                                        continue_scanning2 = false
                                    }
                                } else {
                                    continue_scanning2 = false
                                }
                            }
                        }
                    }
                }
            }
            scan_push_token(tokens, TokenType.Number, source.get_substr(start, pos), line, start.sub(start_line_pos))
        } else {

            current_char := source.get_substr(pos, pos.add(1))
            mut token_type := TokenType.Invalid
            mut skip_token_push := false
            switch current_char {
                // chars to ignore in this language (increment pos, no token, continue loop):
            case " ":
                pos.inc()
                skip_token_push = true
            case "\r":
                pos.inc()
                skip_token_push = true
            case "\t":
                pos.inc()
                skip_token_push = true
            case "\n":
                pos.inc()
                line.inc()
                start_line_pos = pos.sub(1)
                skip_token_push = true

                // open/close. left/right parentheses, craces and brackets
            case "(":
                token_type = TokenType.LeftParen
            case ")":
                token_type = TokenType.RightParen
            case "{":
                token_type = TokenType.LeftBrace
            case "}":
                token_type = TokenType.RightBrace
            case "[":
                token_type = TokenType.LeftBracket
            case "]":
                token_type = TokenType.RightBracket

            case ";":
                if source.get_substr(pos.add(1), pos.add(2)).eq(";") {
                    pos.inc()
                    token_type = TokenType.DoubleSemicolon
                } else {
                    token_type = TokenType.Semicolon
                }
                // separator for optional type before the equal in declarations or args
            case ":":
                token_type = TokenType.Colon
                // separator for args
            case ",":
                token_type = TokenType.Comma

                // math
            case "+":
                token_type = TokenType.Plus
            case "-":
                token_type = TokenType.Minus
            case "*":
                token_type = TokenType.Star

                // reserved for two chars in a row
            case ".":
                if source.get_substr(pos.add(1), pos.add(2)).eq(".") {
                    pos.inc()
                    token_type = TokenType.DoubleDot
                } else {
                    token_type = TokenType.Dot
                }
            case "=":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    pos.inc()
                    token_type = TokenType.EqualEqual
                } else {
                    token_type = TokenType.Equal
                }
            case "<":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    pos.inc()
                    token_type = TokenType.LesserEqual
                } else {
                    token_type = TokenType.Lesser
                }
            case ">":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    pos.inc()
                    token_type = TokenType.GreaterEqual
                } else {
                    token_type = TokenType.Greater
                }
            case "!":
                if source.get_substr(pos.add(1), pos.add(2)).eq("=") {
                    pos.inc()
                    token_type = TokenType.NotEqual
                } else {
                    token_type = TokenType.Not
                }

                // comments:
            case "#":
                pos.inc()
                while and(lt(pos.add(1), eof_pos), not(source.get_substr(pos, pos.add(1)).eq("\n"))) {
                    pos.inc()
                }
                skip_token_push = true
            case "/":
                if source.get_substr(pos.add(1), pos.add(2)).eq("/") {
                    // Single-line comment
                    pos.inc()
                    while and(lt(pos.add(1), eof_pos), not(source.get_substr(pos, pos.add(1)).eq("\n"))) {
                        pos.inc()
                    }
                    skip_token_push = true
                } else if source.get_substr(pos.add(1), pos.add(2)).eq("*") {
                    // Nested block comment
                    pos = pos.add(2)
                    mut depth := 1
                    mut comment_done := false
                    // TODO: Replace comment_done flag with 'break' when break is implemented

                    while and(pos.lt(sub(eof_pos, 1)), not(comment_done)) {
                        next_two := source.get_substr(pos, pos.add(2))

                        if next_two.eq("/*") {
                            depth = depth.add(1)
                            pos = pos.add(2)
                        } else if next_two.eq("*/") {
                            depth = depth.sub(1)
                            pos = pos.add(2)
                            if depth.eq(0) {
                                comment_done = true  // TODO: Replace with 'break'
                            }
                        } else {
                            if source.get_substr(pos, pos.add(1)).eq("\n") {
                                line.inc()
                                start_line_pos = pos
                            }
                            pos.inc()
                        }
                    }

                    if depth.gt(0) {
                        scan_push_token(tokens, TokenType.UnterminatedComment, "/*", line, pos.sub(start_line_pos))
                        skip_token_push = true
                    } else {
                        skip_token_push = true
                    }
                } else {
                    token_type = TokenType.Slash
                }

                // literal strings
            case "\"":
                mut lit_string := ""
                pos.inc()
                // while pos + 1 < eof_pos && &source[pos..pos+1] != "\""
                mut continue_string_scan := true
                while continue_string_scan {
                    if not(lt(pos.add(1), eof_pos)) {
                        continue_string_scan = false
                    } else if source.get_substr(pos, pos.add(1)).eq("\"") {
                        continue_string_scan = false
                    } else {
                        if source.get_substr(pos, pos.add(1)).eq("\\") {
                            pos.inc() // if it's the escape character, skip it
                            if lt(pos.add(1), eof_pos) {
                                esc := source.get_substr(pos, pos.add(1))
                                switch esc {
                                case "\"":
                                    lit_string = format(lit_string, "\"")
                                case "\\":
                                    lit_string = format(lit_string, "\\")
                                case "n":
                                    lit_string = format(lit_string, "\n")
                                case "r":
                                    lit_string = format(lit_string, "\r")
                                case "t":
                                    lit_string = format(lit_string, "\t")
                                case "0":
                                    lit_string = format(lit_string, "\0")
                                case:
                                    // If it's something else, just leave it as is for now, I guess
                                    // TODO print some warning with unkown escaped characters?
                                    lit_string = format(lit_string, "\\")
                                    lit_string = format(lit_string, esc)
                                }
                            }
                        } else {
                            lit_string = format(lit_string, source.get_substr(pos, pos.add(1)))
                        }
                        pos.inc()
                    }
                }

                if not(lt(pos, eof_pos)) {
                    token_type = TokenType.UnterminatedString
                } else {
                    scan_push_token(tokens, TokenType.String, lit_string, line, start.sub(start_line_pos))
                    token_type = TokenType.String
                }

                // Everything else must be reserved words, identifiers or invalid
            case:
                if source.is_id_start(pos) {
                    pos.inc()
                    mut continue_id_scan := true
                    // TODO: Replace continue_id_scan flag with 'break' when break is implemented
                    while and(pos.lt(eof_pos), continue_id_scan) {
                        if or(source.is_digit(pos), source.is_id_start(pos)) {
                            pos.inc()
                        } else {
                            continue_id_scan = false  // TODO: Replace with 'break'
                        }
                    }
                    pos.dec()
                    token_type = scan_reserved_words(source.get_substr(start, pos.add(1)))
                } else {
                    token_type = TokenType.Invalid
                }
            } // switch

            // Push token if not already pushed (similar to Rust lines 487-490)
            if not(skip_token_push) {
                // Note: token_type for String is set but token already pushed inside the case
                // so we check and skip pushing again
                mut is_string := false
                switch token_type {
                case TokenType.String:
                    is_string = true
                case:
                    is_string = false
                }
                if not(is_string) {
                    scan_push_token(tokens, token_type, source.get_substr(start, pos.add(1)), line, start.sub(start_line_pos))
                }
                pos.inc()
            }
        } // else
        // println("Current char:", current_char)
    } // while
    scan_push_token(tokens, TokenType.Eof, "End of file", line, 0);

    catch (err: IndexOutOfBoundsError) {
        throw format(loc(), "scan_tokens: IndexOutOfBoundsError: ", err.msg)
    }
    catch (err: I64_OverflowError) {
        throw format(loc(), "scan_tokens: I64_OverflowError: ", err.msg)
    }
    catch (err: AllocError) {
        throw format(loc(), "scan_tokens: AllocError: ", err.msg)
    }
    return tokens
}

/** An interface for the Lexer */
Lexer := struct {
    mut path: Str = ""
    mut tokens: Vec = Vec.new(Token)
    mut current: I64 = 0

    new := proc(path: Str) returns Lexer throws Str {
        mut l := Lexer()
        l.path = path
        l.tokens = scan_tokens(readfile(path))
        return l
    }

    new_from_src := proc(source: Str) returns Lexer throws Str {
        mut l := Lexer()
        l.path = "<test>"
        l.tokens = scan_tokens(source)
        return l
    }

    len := func(self: Lexer) returns I64 {
        return self.tokens.len()
    }

    is_eof := func(self: Lexer, offset: I64) returns Bool throws IndexOutOfBoundsError {
        current := self.current
        i := add(current, offset)
        if gteq(i, self.tokens.len()) {
            return true
        }
        mut t := Token()
        self.tokens.get(i, t)
        // Compare TokenType using switch
        mut is_eof := false
        switch t.token_type {
        case TokenType.Eof:
            is_eof = true
        case:
            is_eof = false
        }
        return is_eof
    }

    get_token := func(self: Lexer, index: I64) returns Token throws IndexOutOfBoundsError {
        mut t := Token()
        self.tokens.get(index, t)
        return t
    }

    // TODO: If advance/go_back maintain the invariant that current is always valid,
    // peek shouldn't need to throw IndexOutOfBoundsError
    peek := func(self: Lexer) returns Token throws IndexOutOfBoundsError {
        mut t := Token()
        self.tokens.get(self.current, t)
        return t
    }

    peek_ahead := func(self: Lexer, offset: I64) returns Token throws IndexOutOfBoundsError, Str {
        index := add(self.current, offset)
        if gteq(index, self.tokens.len()) {
            mut err := IndexOutOfBoundsError()
            err.msg = format("Peek ahead by ", I64.to_str(offset), " is out of bounds")
            throw err
        }
        mut t := Token()
        self.tokens.get(index, t)
        catch (err: I64_OverflowError) {
            throw err.msg
        }
        catch (err: AllocError) {
            throw err.msg
        }
        return t
    }

    next := func(self: Lexer) returns Token throws IndexOutOfBoundsError, Str {
        return self.peek_ahead(1)
    }

    previous := func(self: Lexer) returns Token throws IndexOutOfBoundsError {
        if eq(self.current, 0) {
            mut err := IndexOutOfBoundsError()
            err.msg = "No previous token (at position 0)"
            throw err
        }
        index := sub(self.current, 1)
        if gteq(index, self.tokens.len()) {
            mut err := IndexOutOfBoundsError()
            err.msg = "Previous token is out of bounds"
            throw err
        }
        mut t := Token()
        self.tokens.get(index, t)
        return t
    }

    advance := func(mut self: Lexer, count: I64) throws IndexOutOfBoundsError, Str {
        next_index := add(self.current, count)
        if gt(next_index, self.tokens.len()) {
            mut err := IndexOutOfBoundsError()
            err.msg = format("Advance by ", I64.to_str(count), " from ", I64.to_str(self.current),
                    " would exceed bounds (", I64.to_str(self.tokens.len()), " tokens total)")
            throw err
        }
        self.current = next_index
        catch (err: I64_OverflowError) {
            throw err.msg
        }
        catch (err: AllocError) {
            throw err.msg
        }
    }

    go_back := func(mut self: Lexer, count: I64) throws IndexOutOfBoundsError, Str {
        if gt(count, self.current) {
            mut err := IndexOutOfBoundsError()
            err.msg = format("Go back by ", I64.to_str(count), " from ", I64.to_str(self.current), " would underflow")
            throw err
        }
        self.current = sub(self.current, count)
        catch (err: I64_OverflowError) {
            throw err.msg
        }
        catch (err: AllocError) {
            throw err.msg
        }
    }

    expect := func(mut self: Lexer, expected: TokenType) returns Token throws IndexOutOfBoundsError, Str {
        p := self.peek()
        switch p.token_type {
        case expected:
            self.advance(1)
            return p
        case:
            throw p.error(self.path, format(
                "Expected token '", token_type_to_str(expected), "', but found '", p.token_str, "'"))
        }
    }

}

// Workaround for Bug #8: Uncaptured return values propagate up call stack
// This standalone function allows callers to capture the return value with _ := lexer_expect(...)
lexer_expect := func(mut lexer: Lexer, expected: TokenType) returns Token throws IndexOutOfBoundsError, Str {
    p := lexer.peek()
    switch p.token_type {
    case expected:
        lexer.advance(1)
        return p
    case:
        throw p.error(lexer.path, format(
            "Expected token '", token_type_to_str(expected), "', but found '", p.token_str, "'"))
    }
}

token_type_to_str := func(tt: TokenType) returns Str {
    switch tt {
    case TokenType.If: return "if"
    case TokenType.Else: return "else"
    case TokenType.While: return "while"
    case TokenType.For: return "for"
    case TokenType.In: return "in"
    case TokenType.Return: return "return"
    case TokenType.Throw: return "throw"
    case TokenType.Catch: return "catch"
    case TokenType.Struct: return "struct"
    case TokenType.Enum: return "enum"
    case TokenType.Mode: return "mode"
    case TokenType.Func: return "func"
    case TokenType.Proc: return "proc"
    case TokenType.Macro: return "macro"
    case TokenType.FuncExt: return "ext_func"
    case TokenType.ProcExt: return "ext_proc"
    case TokenType.Returns: return "returns"
    case TokenType.Throws: return "throws"
    case TokenType.Match: return "match"
    case TokenType.Switch: return "switch"
    case TokenType.Default: return "default"
    case TokenType.Case: return "case"
    case TokenType.Mut: return "mut"
    case TokenType.Copy: return "copy"
    case TokenType.Own: return "own"
    case TokenType.LeftParen: return "("
    case TokenType.RightParen: return ")"
    case TokenType.LeftBrace: return "{"
    case TokenType.RightBrace: return "}"
    case TokenType.LeftBracket: return "["
    case TokenType.RightBracket: return "]"
    case TokenType.Comma: return ","
    case TokenType.Dot: return "."
    case TokenType.DoubleDot: return ".."
    case TokenType.Colon: return ":"
    case TokenType.Semicolon: return ";"
    case TokenType.DoubleSemicolon: return ";;"
    case TokenType.Equal: return "="
    case TokenType.EqualEqual: return "=="
    case TokenType.Not: return "!"
    case TokenType.NotEqual: return "!="
    case TokenType.Lesser: return "<"
    case TokenType.LesserEqual: return "<="
    case TokenType.Greater: return ">"
    case TokenType.GreaterEqual: return ">="
    case TokenType.Plus: return "+"
    case TokenType.Minus: return "-"
    case TokenType.Star: return "*"
    case TokenType.Slash: return "/"
    case TokenType.Identifier: return "identifier"
    case TokenType.String: return "string"
    case TokenType.Number: return "number"
    case TokenType.Eof: return "eof"
    case: return "invalid"
    }
}

build_var_suggestion := func(tokens: Vec, var_index: I64) returns Str throws Str {
    mut result := "Suggestion: use 'mut' instead"

    // Check if pattern is: var <identifier> = <value>
    mut should_check := false
    mut check_result := var_index.add(3)
    mut tokens_length := tokens.len()
    if lt(check_result, tokens_length) {
        should_check = true
    }

    if should_check {
        mut ident_token := Token()
        mut equals_token := Token()
        mut value_token := Token()

        tokens.get(var_index.add(1), ident_token)
        tokens.get(var_index.add(2), equals_token)
        tokens.get(var_index.add(3), value_token)

        mut is_identifier := false
        switch ident_token.token_type {
        case TokenType.Identifier:
            is_identifier = true
        case:
            is_identifier = false
        }

        mut is_equal := false
        switch equals_token.token_type {
        case TokenType.Equal:
            is_equal = true
        case:
            is_equal = false
        }

        if and(is_identifier, is_equal) {
            result = format("Suggestion: mut ", ident_token.token_str, " := ", value_token.token_str)
        }
    }

    catch (err: I64_OverflowError) {
        throw err.msg
    }
    catch (err: IndexOutOfBoundsError) {
        throw err.msg
    }
    catch (err: AllocError) {
        throw err.msg
    }
    return result
}

print_lex_error := proc(path: Str, t: Token, mut errors_found: I64, msg: Str) {
    print(path, ":", I64.to_str(t.line), ":", I64.to_str(t.col), ": Lexical error ", I64.to_str(errors_found), ": ", msg)
    print(". Offending symbol: '", t.token_str, "'")
    print("\n")
    errors_found.inc()
}

print_if_lex_error := proc(path: Str, tokens: Vec, index: I64, t: Token, mut errors_found: I64) throws Str {
    switch t.token_type {
    case TokenType.Invalid:
        print_lex_error(path, t, errors_found, "Invalid character")

    case TokenType.UnterminatedString:
        print_lex_error(path, t, errors_found, "Unterminated Str\nSuggestion: add missing '\"'")

    case TokenType.UnterminatedComment:
        print_lex_error(path, t, errors_found, "Unterminated comment\nSuggestion: add missing '*/'")

    case TokenType.Const:
        print_lex_error(path, t, errors_found, "No need to use 'const', everything is const by default unless 'mut' is used")

    case TokenType.Var:
        suggestion := build_var_suggestion(tokens, index)
        print_lex_error(path, t, errors_found, format("Keyword 'var' is not supported\n", suggestion))

    case TokenType.Fn:
        print_lex_error(path, t, errors_found, "Keyword 'fn' is not supported\nSuggestion: use 'func' or 'proc' instead")

    case TokenType.Function:
        print_lex_error(path, t, errors_found, "Keyword 'function' is not supported\nSuggestion: use 'func' or 'proc' instead")

    case TokenType.Try:
        print_lex_error(path, t, errors_found, "Keyword 'try' is not supported\nSuggestion: TIL uses 'catch' blocks without 'try' (just use throw/catch)")

    case TokenType.Let:
        print_lex_error(path, t, errors_found, "Keyword 'let' is not supported\nSuggestion: use ':=' for declaration")

    case TokenType.Class:
        print_lex_error(path, t, errors_found, "Keyword 'class' is not supported\nSuggestion: use 'struct' instead")

    case TokenType.DoubleSemicolon:
        print_lex_error(path, t, errors_found, "No need for ';;' (aka empty statements)\nSuggestion: try 'if true {}' instead, whatever you want that for")

    case TokenType.Plus:
        print_lex_error(path, t, errors_found, "Operator '+' is not supported yet\nSuggestion: use core func 'add' instead")

    case TokenType.Minus:
        print_lex_error(path, t, errors_found, "Operator '-' is not supported yet\nSuggestion: use core func 'sub' instead")

    case TokenType.Star:
        print_lex_error(path, t, errors_found, "Operator '*' is not supported yet\nSuggestion: use core func 'mul' instead")

    case TokenType.Slash:
        print_lex_error(path, t, errors_found, "Operator '/' is not supported yet\nSuggestion: use core func 'div' instead")

    case TokenType.EqualEqual:
        print_lex_error(path, t, errors_found, "Operator '==' is not supported yet\nSuggestion: use 'I64.eq' or 'Str.eq' instead")

    case TokenType.NotEqual:
        print_lex_error(path, t, errors_found, "Operator '!=' is not supported yet\nSuggestion: use core funcs 'not' and 'I64.eq'/'Str.eq' instead")

    case TokenType.Lesser:
        print_lex_error(path, t, errors_found, "Operator '<' is not supported yet\nSuggestion: use core func 'lt' instead")

    case TokenType.LesserEqual:
        print_lex_error(path, t, errors_found, "Operator '<=' is not supported yet\nSuggestion: use core func 'lteq' instead")

    case TokenType.Greater:
        print_lex_error(path, t, errors_found, "Operator '>' is not supported yet\nSuggestion: use core func 'gt' instead")

    case TokenType.GreaterEqual:
        print_lex_error(path, t, errors_found, "Operator '>=' is not supported yet\nSuggestion: use core func 'gteq' instead")

    case TokenType.Not:
        print_lex_error(path, t, errors_found, "Operator '!' is not supported yet\nSuggestion: use core func 'not' instead")

    case:
        // No error, do nothing
    }
}

print_lex_errors := proc(tokens: Vec, path: Str) returns I64 throws Str {
    mut errors_found := 0
    mut tokens_len := tokens.len()
    for i in 0..tokens_len {
        mut t := Token()
        tokens.get(i, t)
        print_if_lex_error(path, tokens, i, t, errors_found)
    }
    catch (err: IndexOutOfBoundsError) {
        throw err.msg
    }
    return errors_found
}

lexer_from_source := proc(path: Str, source: Str) returns Lexer throws Str, IndexOutOfBoundsError {
    mut l := Lexer()
    l.path = path
    tokens := scan_tokens(source)
    l.tokens = tokens

    if I64.eq(tokens.len(), 0) {
        throw path.format(":1:0: compiler ERROR: End of file not found.")
    }

    mut first := Token()
    tokens.get(0, first)
    switch first.token_type {
    case TokenType.Eof:
        throw path.format(":0:0: compiler ERROR: Nothing to be done")
    case:
    }

    errors := print_lex_errors(tokens, path)
    if gt(errors, 0) {
        throw format(loc(), "Compiler errors: ", I64.to_str(errors), " lexical errors found")
    }

    return l
}
