mode liba

import("core.vec")
import("std.io")            // readfile

LANG_NAME := "til"

TokenType := enum {
    // basic
    Eof,

    // Single-character tokens.
    Minus, Plus, Slash, Star, QuestionMark,
    LeftParen, RightParen, LeftBrace, RightBrace, LeftBracket, RightBracket,
    Comma, Colon,

    // One or two character tokens.
    Dot, DoubleDot,
    Not, NotEqual,
    Equal, EqualEqual,
    Greater, GreaterEqual,
    Lesser, LesserEqual,
    // Semicolons are skipped by lexer (treated as whitespace)

    // Literals.
    Identifier, String, Number,

    // Reserved words:
    Mut, Copy, Own,

    // type definition
    Struct, Enum,

    // function definition
    Returns, Throws,

    // flow control
    If, Else,
    While, For, In,
    Match, Switch, Default, Case,
    Return, Throw,
    Catch,
    Break, Continue,

    // Special in this language:
    Mode,
    Func, Proc, Macro,
    FuncExt, ProcExt,

    // Errors
    Const, Var,
    Fn, Function,
    Try,
    Let, Class,
    Invalid,
    UnterminatedString,
    UnterminatedComment,
}

Token := struct {
    mut token_type: TokenType = TokenType.Invalid
    mut token_str: Str = ""
    mut line: I64 = 0
    mut col: I64 = 0

    lang_error := func(self: Token, path: Str, msg: Str) returns Str {
        return path.concat(":").concat(self.line.to_str()).concat(":").concat(self.col.to_str())
            .concat(": ").concat(LANG_NAME).concat(" parse ERROR: ").concat(msg).concat("\n")
            .concat("Explanation: This is not your fault as a user, this is a bug in the language.")
    }

    #[allow(dead_code)] // Kept for consistency with Expr.todo_error
    todo_error := func(self: Token, path: Str, msg: Str) returns Str {
        return path.concat(":").concat(self.line.to_str()).concat(":").concat(self.col.to_str())
            .concat(": ").concat(LANG_NAME).concat(" parse ERROR: ").concat(msg).concat("\n")
            .concat("Explanation: Not implemented yet, this is a missing feature in the language.")
    }

    error := func(self: Token, path: Str, msg: Str) returns Str {
        return path.concat(":").concat(self.line.to_str()).concat(":").concat(self.col.to_str())
            .concat(": parse ERROR: ").concat(msg)
    }

    clone := func(self: Token) returns Token {
        return Token(
            token_type = self.token_type,
            token_str = self.token_str.clone(),
            line = self.line,
            col = self.col
        )
    }
}

Lexer := struct {
    mut path: Str = ""
    mut tokens: Vec = Vec.new(Token)
    mut current: I64 = 0

    new := proc(source: Str) returns Lexer throws Str {
        mut l := Lexer()
        l.path = "<test>"
        l.tokens = scan_tokens(source)?
        return l
    }

    len := func(self: Lexer) returns I64 {
        return self.tokens.len()
    }

    is_eof := func(self: Lexer, offset: I64) returns Bool throws IndexOutOfBoundsError {
        i := self.current.add(offset)
        if i.gteq(self.tokens.len()) {
            return true
        }
        mut t := Token()
        self.tokens.get(i, t)?
        switch t.token_type {
        case TokenType.Eof:
            return true
        case:
            return false
        }
    }

    get_token := func(self: Lexer, i: I64) returns Token throws Str {
        if i.gteq(self.tokens.len()) {
            mut err_token := self.peek()?
            throw err_token.lang_error(self.path, format("Token in pos ", i.to_str(), " is out of bounds"))
        }
        mut t := Token()
        self.tokens.get(i, t)?
        catch (err: IndexOutOfBoundsError) {
            throw err.msg
        }
        return t
    }

    peek := func(self: Lexer) returns Token throws IndexOutOfBoundsError {
        mut t := Token()
        self.tokens.get(self.current, t)?
        result := t.clone()
        return result
    }

    previous := func(self: Lexer) returns Token throws Str {
        if self.current.eq(0) {
            mut err_token1 := self.peek()?
            throw err_token1.lang_error(self.path, "No previous token (at position 0)")
        }
        if self.current.sub(1).gteq(self.tokens.len()) {
            mut err_token2 := self.peek()?
            throw err_token2.lang_error(self.path, "Previous token is out of bounds")
        }
        mut t := Token()
        self.tokens.get(self.current.sub(1), t)?
        result := t.clone()
        catch (err: IndexOutOfBoundsError) {
            throw err.msg
        }
        return result
    }

    advance := func(mut self: Lexer, count: I64) throws Str {
        if self.current.add(count).gt(self.tokens.len()) {
            mut err_token := self.peek()?
            throw err_token.lang_error(self.path, format("Attempt to advance by ", count.to_str(), " from ", self.current.to_str(),
                    " would exceed bounds (", self.tokens.len().to_str(), " tokens total)"))
        }
        self.current = self.current.add(count)
        catch (err: IndexOutOfBoundsError) {
            throw err.msg
        }
    }

    peek_ahead := func(self: Lexer, offset: I64) returns Token throws Str {
        peek_idx := self.current.add(offset)
        if peek_idx.gteq(self.tokens.len()) {
            mut err_token := self.peek()?
            throw err_token.lang_error(self.path, format("Peek ahead by ", offset.to_str(), " from ", self.current.to_str(), " is out of bounds"))
        }
        mut t := Token()
        self.tokens.get(peek_idx, t)?
        result := t.clone()
        catch (err: IndexOutOfBoundsError) {
            throw err.msg
        }
        return result
    }

    next := func(self: Lexer) returns Token throws Str {
        return self.peek_ahead(1)?
    }

    expect := func(mut self: Lexer, expected: TokenType) returns Token throws Str {
        p := self.peek()?
        switch p.token_type {
        case expected:
            self.advance(1)?
            return p
        case:
            throw p.error(self.path, format(
                "Expected token '", token_type_to_str(expected), "', but found '", p.token_str, "'"))
        }
        catch (err: IndexOutOfBoundsError) {
            throw err.msg
        }
    }

}

token_type_to_str := func(tt: TokenType) returns Str {
    switch tt {
    case TokenType.If: return "if"
    case TokenType.Else: return "else"
    case TokenType.While: return "while"
    case TokenType.For: return "for"
    case TokenType.In: return "in"
    case TokenType.Return: return "return"
    case TokenType.Throw: return "throw"
    case TokenType.Catch: return "catch"
    case TokenType.Break: return "break"
    case TokenType.Continue: return "continue"
    case TokenType.Struct: return "struct"
    case TokenType.Enum: return "enum"
    case TokenType.Mode: return "mode"
    case TokenType.Func: return "func"
    case TokenType.Proc: return "proc"
    case TokenType.Macro: return "macro"
    case TokenType.FuncExt: return "ext_func"
    case TokenType.ProcExt: return "ext_proc"
    case TokenType.Returns: return "returns"
    case TokenType.Throws: return "throws"
    case TokenType.Match: return "match"
    case TokenType.Switch: return "switch"
    case TokenType.Default: return "default"
    case TokenType.Case: return "case"
    case TokenType.Mut: return "mut"
    case TokenType.Copy: return "copy"
    case TokenType.Own: return "own"
    case TokenType.LeftParen: return "("
    case TokenType.RightParen: return ")"
    case TokenType.LeftBrace: return "{"
    case TokenType.RightBrace: return "}"
    case TokenType.LeftBracket: return "["
    case TokenType.RightBracket: return "]"
    case TokenType.Comma: return ","
    case TokenType.Dot: return "."
    case TokenType.DoubleDot: return ".."
    case TokenType.Colon: return ":"
    case TokenType.Equal: return "="
    case TokenType.EqualEqual: return "=="
    case TokenType.Not: return "!"
    case TokenType.NotEqual: return "!="
    case TokenType.Lesser: return "<"
    case TokenType.LesserEqual: return "<="
    case TokenType.Greater: return ">"
    case TokenType.GreaterEqual: return ">="
    case TokenType.Plus: return "+"
    case TokenType.Minus: return "-"
    case TokenType.Star: return "*"
    case TokenType.Slash: return "/"
    case TokenType.QuestionMark: return "?"
    case TokenType.Identifier: return "identifier"
    case TokenType.String: return "string"
    case TokenType.Number: return "number"
    case TokenType.Eof: return "eof"
    case: return "invalid"
    }
}

is_digit := func(source: Str, pos: I64) returns Bool throws Str {
    switch source.get_substr(pos, pos.add(1))? {
    case "0".."9": return true
    case: return false
    }
    catch (err: IndexOutOfBoundsError) {
        throw format("is_digit: ", err.msg)
    }
}

is_id_start := func(source: Str, pos: I64) returns Bool throws Str {
    current_char := source.get_substr(pos, pos.add(1))?
    switch current_char {
    case "a".."z": return true
    case "A".."Z": return true
    case "_": return true
    case:
    }
    catch (err: IndexOutOfBoundsError) {
        throw format("is_id_start: ", err.msg)
    }
    return false
}

scan_reserved_words := func(identifier: Str) returns TokenType {
    switch identifier {
    case "mode": return TokenType.Mode

        // declaration/arg modifiers
    case "mut": return TokenType.Mut
    case "copy": return TokenType.Copy
    case "own": return TokenType.Own

        // core data types
    case "enum": return TokenType.Enum
    case "struct": return TokenType.Struct

        // function declaration
    case "returns": return TokenType.Returns
        // Anything that can be thrown must be explicitly declared in the function via 'throws', java style.
        // Except perhaps PanicException or something like that which can be implicit, but still allowed to documment redundantly
        // or perhaps not, for that may degenerate in an extra warning option
        // perhaps just force the user to explicitly catch and exit any potential panic from the callee
    case "throws": return TokenType.Throws // TODO parse
    case "func": return TokenType.Func
    case "proc": return TokenType.Proc
    case "macro": return TokenType.Macro // TODO implement for real once we compile
    case "ext_func": return TokenType.FuncExt // this has to link when we compile
    case "ext_proc": return TokenType.ProcExt // this has to link when we compile

        // control flow
    case "if": return TokenType.If
    case "else": return TokenType.Else
    case "while": return TokenType.While
    case "for": return TokenType.For
    case "in": return TokenType.In
    case "switch": return TokenType.Switch
    case "match": return TokenType.Match // TODO like switch but special for declarations/assignments
    case "case": return TokenType.Case
    case "default": return TokenType.Default // TODO currently using "case:", but "default:" is more traditional, grepable and overt
    case "return": return TokenType.Return
    case "throw": return TokenType.Throw // TODO
        // TODO throw should just act as a return that gets post-processed by the next catch or rethrown
    case "catch": return TokenType.Catch
    case "break": return TokenType.Break
    case "continue": return TokenType.Continue

        // Reserved forbidden/illegal words (intentionally unsupported reserved words)
        // TODO intentionally unsupport more reserved words
        // TODO nicer messages for forbidden words
    case "fn": return TokenType.Fn
    case "function": return TokenType.Function
    case "try": return TokenType.Try // TIL uses catch without try (try is forbidden for users from other languages)
    case "let": return TokenType.Let
    case "class": return TokenType.Class
    case "global": return TokenType.Invalid // just use mut declaration in the root of the file, but they're not allowed in all modes
        // const/vars are the most abstract types, you can't even explicitly declare them
    case "const": return TokenType.Const
    case "var": return TokenType.Var

        // Do we really need const fields static other than static? (ie can be different per instance, but not modified afterwards)
        // The answer is probably yet, but perhaps static is not the right answer
        // how about this? if it's in the struct body, it is const, if it is in impl, it is static, just like functions
        // or do we need mut function fields too? probably yes
    case "static": return TokenType.Invalid

    case:
    }
    return TokenType.Identifier
}

scan_push_token := proc(mut tokens: Vec, token_type: TokenType, token_str: Str, line: I64, col: I64) {
    t := Token(token_type=token_type, token_str=token_str, line=line, col=col)
    tokens.push(t)
}

scan_tokens := proc(source: Str) returns Vec throws Str {
    mut tokens := Vec.new(Token)

    eof_pos := source.len()
    mut pos := 0
    mut line := 1
    mut start_line_pos := sub(0, 1)
    mut current_char := ""
    mut next_two := ""
    mut esc := ""

    while pos.lt(eof_pos) {
        start := pos
        if source.is_digit(pos)? {
            mut continue_scanning := true
            while continue_scanning {
                pos.inc()
                if pos.lt(eof_pos) {
                    if not(source.is_digit(pos)?) {
                        continue_scanning = false
                    }
                } else {
                    continue_scanning = false
                }
            }
            // Look for a fractional part.
            if pos.lt(eof_pos) {
                if source.get_substr(pos, pos.add(1))?.eq(".") {
                    if pos.add(1).lt(eof_pos) {
                        if source.is_digit(pos.add(1))? {
                            pos.inc()
                            mut continue_scanning2 := true
                            while continue_scanning2 {
                                pos.inc()
                                if pos.lt(eof_pos) {
                                    if not(source.is_digit(pos)?) {
                                        continue_scanning2 = false
                                    }
                                } else {
                                    continue_scanning2 = false
                                }
                            }
                        }
                    }
                }
            }
            scan_push_token(tokens, TokenType.Number, source.get_substr(start, pos)?, line, start.sub(start_line_pos))
        } else {

            current_char = source.get_substr(pos, pos.add(1))?
            mut token_type := TokenType.Invalid
            mut should_process_token := true
            switch current_char {
                // chars to ignore in this language (increment pos, no token, continue loop):
            case " ":
                pos.inc()
                should_process_token = false
            case "\r":
                pos.inc()
                should_process_token = false
            case "\t":
                pos.inc()
                should_process_token = false
            case "\n":
                pos.inc()
                line.inc()
                start_line_pos = pos.sub(1)
                should_process_token = false

                // open/close. left/right parentheses, craces and brackets
            case "(":
                token_type = TokenType.LeftParen
            case ")":
                token_type = TokenType.RightParen
            case "{":
                token_type = TokenType.LeftBrace
            case "}":
                token_type = TokenType.RightBrace
            case "[":
                token_type = TokenType.LeftBracket
            case "]":
                token_type = TokenType.RightBracket

                // separator for optional type before the equal in declarations or args
            case ":":
                token_type = TokenType.Colon
                // separator for args
            case ",":
                token_type = TokenType.Comma // args can/must? have ',', otherwise the language would be too lispy when parsing from C

                // math
            case "-":
                token_type = TokenType.Minus
            case "+":
                token_type = TokenType.Plus
            case "*":
                token_type = TokenType.Star

                // reserved for two chars in a row
            case ".":
                if source.get_substr(pos.add(1), pos.add(2))?.eq(".") {
                    pos.inc()
                    token_type = TokenType.DoubleDot
                } else {
                    token_type = TokenType.Dot
                }
            case "=":
                if source.get_substr(pos.add(1), pos.add(2))?.eq("=") {
                    pos.inc()
                    token_type = TokenType.EqualEqual
                } else {
                    token_type = TokenType.Equal
                }
            case "<":
                if source.get_substr(pos.add(1), pos.add(2))?.eq("=") {
                    pos.inc()
                    token_type = TokenType.LesserEqual
                } else {
                    token_type = TokenType.Lesser
                }
            case ">":
                if source.get_substr(pos.add(1), pos.add(2))?.eq("=") {
                    pos.inc()
                    token_type = TokenType.GreaterEqual
                } else {
                    token_type = TokenType.Greater
                }
            case "!":
                if source.get_substr(pos.add(1), pos.add(2))?.eq("=") {
                    pos.inc()
                    token_type = TokenType.NotEqual
                } else {
                    token_type = TokenType.Not
                }
            case "?":
                token_type = TokenType.QuestionMark

                // Semicolons are completely optional - skip any number of them
            case ";":
                pos.inc() // Skip the first semicolon
                while lt(pos, eof_pos).and(source.get_substr(pos, pos.add(1))?.eq(";")) {
                    pos.inc()
                }
                should_process_token = false

                // comments:
            case "#":
                pos.inc()
                while lt(pos.add(1), eof_pos).and(not(source.get_substr(pos, pos.add(1))?.eq("\n"))) {
                    pos.inc()
                }
                should_process_token = false
            case "/":
                if source.get_substr(pos.add(1), pos.add(2))?.eq("/") {
                    // Single-line comment
                    pos.inc()
                    while lt(pos.add(1), eof_pos).and(not(source.get_substr(pos, pos.add(1))?.eq("\n"))) {
                        pos.inc()
                    }
                    should_process_token = false
                } else if source.get_substr(pos.add(1), pos.add(2))?.eq("*") {
                    // Nested block comment
                    pos = pos.add(2)
                    mut depth := 1
                    mut should_continue_parsing := true

                    while pos.lt(eof_pos.sub(1)).and(should_continue_parsing) {
                        next_two = source.get_substr(pos, pos.add(2))?

                        if next_two.eq("/*") {
                            depth.inc()
                            pos = pos.add(2)
                        } else if next_two.eq("*/") {
                            depth = depth.sub(1)
                            pos = pos.add(2)
                            if depth.eq(0) {
                                should_continue_parsing = false
                            }
                        } else {
                            if source.get_substr(pos, pos.add(1))?.eq("\n") {
                                line.inc()
                                start_line_pos = pos
                            }
                            pos.inc()
                        }
                    }

                    if depth.gt(0) {
                        scan_push_token(tokens, TokenType.UnterminatedComment, "/*", line, pos.sub(start_line_pos))
                    }
                    should_process_token = false
                } else {
                    token_type = TokenType.Slash
                }

                // literal strings
            case "\"":
                mut lit_string := ""
                pos.inc()
                // while pos + 1 < eof_pos && &source[pos..pos+1] != "\""
                mut continue_string_scan := true
                while continue_string_scan {
                    if not(lt(pos.add(1), eof_pos)) {
                        continue_string_scan = false
                    } else if source.get_substr(pos, pos.add(1))?.eq("\"") {
                        continue_string_scan = false
                    } else {
                        if source.get_substr(pos, pos.add(1))?.eq("\\") {
                            pos.inc() // if it's the escape character, skip it
                            if lt(pos.add(1), eof_pos) {
                                esc = source.get_substr(pos, pos.add(1))?
                                switch esc {
                                case "\"":
                                    lit_string = format(lit_string, "\"")
                                case "\\":
                                    lit_string = format(lit_string, "\\")
                                case "n":
                                    lit_string = format(lit_string, "\n")
                                case "r":
                                    lit_string = format(lit_string, "\r")
                                case "t":
                                    lit_string = format(lit_string, "\t")
                                case "0":
                                    lit_string = format(lit_string, "\0")
                                case:
                                    // If it's something else, just leave it as is for now, I guess
                                    // TODO print some warning with unkown escaped characters?
                                    lit_string = format(lit_string, "\\")
                                    lit_string = format(lit_string, esc)
                                }
                            }
                        } else {
                            lit_string = format(lit_string, source.get_substr(pos, pos.add(1))?)
                        }
                        pos.inc()
                    }
                }

                if not(pos.lt(eof_pos)) {
                    token_type = TokenType.UnterminatedString
                } else {
                    scan_push_token(tokens, TokenType.String, lit_string, line, start.sub(start_line_pos))
                    token_type = TokenType.String
                }

                // Everything else must be reserved words, identifiers or invalid
            case:
                if source.is_id_start(pos)? {
                    pos.inc()
                    mut continue_id_scan := true
                    while pos.lt(eof_pos).and(continue_id_scan) {
                        if source.is_digit(pos)?.or(source.is_id_start(pos)?) {
                            pos.inc()
                        } else {
                            continue_id_scan = false
                        }
                    }
                    pos.dec()
                    token_type = scan_reserved_words(source.get_substr(start, pos.add(1))?)
                } else {
                    token_type = TokenType.Invalid
                }
            } // switch

            if should_process_token {
                // Note: token_type for String is set but token already pushed inside the case
                // so we check and skip pushing again
                mut is_string := false
                switch token_type {
                case TokenType.String:
                    is_string = true
                case:
                    is_string = false
                }
                if not(is_string) {
                    scan_push_token(tokens, token_type, source.get_substr(start, pos.add(1))?, line, start.sub(start_line_pos))
                }
                pos.inc()
            }
        } // else
    } // while

    scan_push_token(tokens, TokenType.Eof, "End of file", line, 0);

    catch (err: IndexOutOfBoundsError) {
        throw format(loc(), "scan_tokens: IndexOutOfBoundsError: ", err.msg)
    }
    return tokens
}

print_lex_error := proc(path: Str, t: Token, mut errors_found: I64, msg: Str) {
    print(path, ":", I64.to_str(t.line), ":", I64.to_str(t.col), ": Lexical error ", I64.to_str(errors_found), ": ", msg)
    print(". Offending symbol: '", t.token_str, "'")
    print("\n")
    errors_found.inc()
}

build_var_suggestion := func(tokens: Vec, var_index: I64) returns Str throws Str {
    // Check if pattern is: var <identifier> = <value>
    mut should_check := false
    if var_index.add(3).lt(tokens.len()) {
        should_check = true
    }

    if should_check {
        mut ident_token := Token()
        mut equals_token := Token()
        mut value_token := Token()

        tokens.get(var_index.add(1), ident_token)?
        tokens.get(var_index.add(2), equals_token)?
        tokens.get(var_index.add(3), value_token)?

        mut is_identifier := false
        switch ident_token.token_type {
        case TokenType.Identifier:
            is_identifier = true
        case:
            is_identifier = false
        }

        mut is_equal := false
        switch equals_token.token_type {
        case TokenType.Equal:
            is_equal = true
        case:
            is_equal = false
        }

        if is_identifier.and(is_equal) {
            return format("Suggestion: mut ", ident_token.token_str, " := ", value_token.token_str)
        }
    }

    catch (err: IndexOutOfBoundsError) {
        throw err.msg
    }
    return "Suggestion: use 'mut' instead"
}

print_lex_errors := proc(tokens: Vec, path: Str) returns I64 throws Str {
    mut errors_found := 0
    mut tokens_len := tokens.len()
    for i in 0..tokens_len {
        mut t := Token()
        tokens.get(i, t)?
        print_if_lex_error(path, tokens, i, t, errors_found)?
    }
    catch (err: IndexOutOfBoundsError) {
        throw err.msg
    }
    return errors_found
}

print_if_lex_error := proc(path: Str, tokens: Vec, index: I64, t: Token, mut errors_found: I64) throws Str {
    mut suggestion := ""
    switch t.token_type {
    case TokenType.Invalid:
        print_lex_error(path, t, errors_found, "Invalid character")

    case TokenType.UnterminatedString:
        print_lex_error(path, t, errors_found, "Unterminated Str\nSuggestion: add missing '\"'")

    case TokenType.UnterminatedComment:
        print_lex_error(path, t, errors_found, "Unterminated comment\nSuggestion: add missing '*/'")

    case TokenType.Const:
        print_lex_error(path, t, errors_found, "No need to use 'const', everything is const by default unless 'mut' is used")

    case TokenType.Var:
        suggestion = build_var_suggestion(tokens, index)?
        print_lex_error(path, t, errors_found, format("Keyword 'var' is not supported\n", suggestion))

    case TokenType.Fn:
        print_lex_error(path, t, errors_found, "Keyword 'fn' is not supported\nSuggestion: use 'func' or 'proc' instead")

    case TokenType.Function:
        print_lex_error(path, t, errors_found, "Keyword 'function' is not supported\nSuggestion: use 'func' or 'proc' instead")

    case TokenType.Try:
        print_lex_error(path, t, errors_found, "Keyword 'try' is not supported\nSuggestion: TIL uses 'catch' blocks without 'try' (just use throw/catch)")

    case TokenType.Let:
        print_lex_error(path, t, errors_found, "Keyword 'let' is not supported\nSuggestion: use ':=' for declaration")

    case TokenType.Class:
        print_lex_error(path, t, errors_found, "Keyword 'class' is not supported\nSuggestion: use 'struct' instead")

    case TokenType.Plus:
        print_lex_error(path, t, errors_found, "Operator '+' is not supported yet\nSuggestion: use core func 'add' instead")

    case TokenType.Minus:
        print_lex_error(path, t, errors_found, "Operator '-' is not supported yet\nSuggestion: use core func 'sub' instead")

    case TokenType.Star:
        print_lex_error(path, t, errors_found, "Operator '*' is not supported yet\nSuggestion: use core func 'mul' instead")

    case TokenType.Slash:
        print_lex_error(path, t, errors_found, "Operator '/' is not supported yet\nSuggestion: use core func 'div' instead")

    case TokenType.EqualEqual:
        print_lex_error(path, t, errors_found, "Operator '==' is not supported yet\nSuggestion: use 'I64.eq' or 'Str.eq' instead")

    case TokenType.NotEqual:
        print_lex_error(path, t, errors_found, "Operator '!=' is not supported yet\nSuggestion: use core funcs 'not' and 'I64.eq'/'Str.eq' instead")

    case TokenType.Lesser:
        print_lex_error(path, t, errors_found, "Operator '<' is not supported yet\nSuggestion: use core func 'lt' instead")

    case TokenType.LesserEqual:
        print_lex_error(path, t, errors_found, "Operator '<=' is not supported yet\nSuggestion: use core func 'lteq' instead")

    case TokenType.Greater:
        print_lex_error(path, t, errors_found, "Operator '>' is not supported yet\nSuggestion: use core func 'gt' instead")

    case TokenType.GreaterEqual:
        print_lex_error(path, t, errors_found, "Operator '>=' is not supported yet\nSuggestion: use core func 'gteq' instead")

    case TokenType.Not:
        print_lex_error(path, t, errors_found, "Operator '!' is not supported yet\nSuggestion: use core func 'not' instead")

    case:
        // No error, do nothing
    }
}

lexer_from_source := proc(path: Str, source: Str) returns Lexer throws Str, IndexOutOfBoundsError {
    mut l := Lexer()
    l.path = path.clone()
    tokens := scan_tokens(source)?
    l.tokens = tokens

    if I64.eq(tokens.len(), 0) {
        throw path.format(":1:0: compiler ERROR: End of file not found.")
    }

    mut first := Token()
    tokens.get(0, first)?
    switch first.token_type {
    case TokenType.Eof:
        throw path.format(":0:0: compiler ERROR: Nothing to be done")
    case:
    }

    errors := print_lex_errors(tokens, path)?
    if errors.gt(0) {
        throw format(loc(), "Compiler errors: ", I64.to_str(errors), " lexical errors found")
    }

    return l
}
