mode liba

import("core.vec")
import("core.ptr")
import("std.list")
import("std.map")
import("self.lexer")
import("self.mode")

INFER_TYPE := "auto"

// Counter for generating unique loop variable names when _ is used
// Bug #146: Reset per-function for deterministic output (like Issue #127 fix for precomp_forin_counter)
mut LOOP_VAR_COUNTER := 0

next_loop_var := func() returns Str {
    n := LOOP_VAR_COUNTER
    LOOP_VAR_COUNTER = add(LOOP_VAR_COUNTER, 1)
    return concat("_loop_", n.to_str())
}

save_loop_var_counter := func() returns I64 {
    return LOOP_VAR_COUNTER
}

reset_loop_var_counter := func() returns I64 {
    LOOP_VAR_COUNTER = 0
    return 0
}

restore_loop_var_counter := func(saved: I64) returns I64 {
    LOOP_VAR_COUNTER = saved
    return 0
}

// Bug #38 fix: Use a Vec to preserve variant order instead of Map
EnumVariant := struct {
    mut name: Str = ""
    mut payload_type: Ptr = Ptr()  // Ptr to ValueType, NULL = None (no payload)
}

namespace EnumVariant {
    clone := func(self: EnumVariant) returns EnumVariant {
        mut result := EnumVariant()
        result.name = self.name.clone()
        if not(NULL.eq(self.payload_type.data)) {
            // Bug #67 fix: allocate heap memory for ValueType (stack ptr would be dangling)
            result.payload_type.data = malloc(size_of(ValueType))?
            catch (err: BadAlloc) { panic(loc(), "EnumVariant.clone: malloc failed") }
            memcpy(result.payload_type.data, self.payload_type.data, size_of(ValueType))
        }
        return result
    }
}

SEnumDef := struct {
    mut variants: Vec = Vec.new(EnumVariant)
    mut methods: Map = Map.new(Str, Expr)  // Auto-generated methods (delete, clone)
}

namespace SEnumDef {
    clone := func(self: SEnumDef) returns SEnumDef {
        mut result := SEnumDef()
        // Deep clone: iterate and clone each element
        for i in 0..self.variants.len() {
            mut v := EnumVariant()
            self.variants.get(i, v)?
            result.variants.push(v.clone())
        }
        catch (err: IndexOutOfBoundsError) { panic(loc(), err.msg) }
        // Clone methods map
        result.methods = self.methods.clone()
        return result
    }

    len := func(self: SEnumDef) returns I64 {
        return self.variants.len()
    }

    size := func(self: SEnumDef) returns I64 {
        return self.variants.size()
    }

    // Helper methods for backward compatibility
    get := func(self: SEnumDef, variant_name: Str) returns Ptr throws KeyNotFoundError, IndexOutOfBoundsError {
        for i in 0..self.variants.len() {
            mut v := EnumVariant()
            self.variants.get(i, v)?
            if v.name.eq(variant_name) {
                return v.payload_type
            }
        }
        throw KeyNotFoundError()
    }

    contains_key := func(self: SEnumDef, variant_name: Str) returns Bool throws Str {
        for i in 0..self.variants.len() {
            mut v := EnumVariant()
            self.variants.get(i, v)?
            if v.name.eq(variant_name) {
                return true
            }
        }
        catch (err: IndexOutOfBoundsError) { throw err.msg }  // Should never happen with 0..len()
        return false
    }
}

Declaration := struct {
    mut name: Str = ""
    mut value_type: ValueType = ValueType.TCustom("")
    mut is_mut: Bool = false
    mut is_copy: Bool = false
    mut is_own: Bool = false
    mut default_value: Ptr = Ptr()  // Ptr to Expr, NULL = None (for optional function arguments)
}

namespace Declaration {
    clone := func(self: Declaration) returns Declaration {
        mut result := Declaration()
        result.name = self.name.clone()
        result.value_type = value_type_clone(self.value_type)
        result.is_mut = self.is_mut
        result.is_copy = self.is_copy
        result.is_own = self.is_own
        // Clone default_value if present (must allocate heap memory like Rust's Box<Expr>)
        if not(NULL.eq(self.default_value.data)) {
            mut default_expr := Expr()
            memcpy(to_ptr(default_expr), self.default_value.data, size_of(Expr))
            mut cloned_expr := default_expr.clone()
            result.default_value.data = malloc(size_of(Expr))?
            catch (err: BadAlloc) { panic(loc(), "Declaration.clone: malloc failed") }
            memcpy(result.default_value.data, to_ptr(cloned_expr), size_of(Expr))
        }
        return result
    }
}

// TODO: PatternInfo is a workaround for homogeneity with TIL's lack of tuple syntax
// Once TIL supports tuple notation like (Str, Str), this can be replaced with:
// Pattern(String, String)  // Pattern(variant_name, binding_var)
PatternInfo := struct {
    mut variant_name: Str = ""
    mut binding_var: Str = ""
}

FunctionType := enum {
    FTFunc,
    FTProc,
    FTMacro,
    FTFuncExt,
    FTProcExt,
}

SFuncDef := struct {
    mut function_type: FunctionType = FunctionType.FTFunc
    mut args: Vec = Vec.new(Declaration)
    mut return_types: Vec = Vec.new(ValueType)  // "returns" conflicts with TIL keyword
    mut throw_types: Vec = Vec.new(ValueType)   // "throws" conflicts with TIL keyword
    mut body: Vec = Vec.new(Expr)
    mut source_path: Str = ""  // Path to the file where this function was defined
}

namespace SFuncDef {
    clone := func(self: SFuncDef) returns SFuncDef {
        mut result := SFuncDef()
        result.function_type = self.function_type
        // Deep clone args
        for arg_idx in 0..self.args.len() {
            mut arg := Declaration()
            self.args.get(arg_idx, arg)?
            result.args.push(arg.clone())
        }
        // Deep clone return_types
        for rt_idx in 0..self.return_types.len() {
            mut rt := ValueType.TCustom("")
            self.return_types.get(rt_idx, rt)?
            result.return_types.push(value_type_clone(rt))
        }
        // Deep clone throw_types
        for tt_idx in 0..self.throw_types.len() {
            mut tt := ValueType.TCustom("")
            self.throw_types.get(tt_idx, tt)?
            result.throw_types.push(value_type_clone(tt))
        }
        // Deep clone body
        for body_idx in 0..self.body.len() {
            mut e := Expr()
            self.body.get(body_idx, e)?
            result.body.push(e.clone())
        }
        catch (err: IndexOutOfBoundsError) { panic(loc(), err.msg) }
        result.source_path = self.source_path.clone()
        return result
    }

    is_proc := func(self: SFuncDef) returns Bool {
        ft := self.function_type
        switch ft {
        case FunctionType.FTProc:
            return true
        case FunctionType.FTProcExt:
            return true
        case:
            return false
        }
    }

    is_ext := func(self: SFuncDef) returns Bool {
        ft := self.function_type
        switch ft {
        case FunctionType.FTFuncExt:
            return true
        case FunctionType.FTProcExt:
            return true
        case:
            return false
        }
    }

    is_macro := func(self: SFuncDef) returns Bool {
        ft := self.function_type
        switch ft {
        case FunctionType.FTMacro:
            return true
        case:
            return false
        }
    }
}

SStructDef := struct {
    mut members: Vec = Vec.new(Declaration)
    mut default_values: Map = Map.new(Str, Expr)
}

namespace SStructDef {
    clone := func(self: SStructDef) returns SStructDef {
        mut result := SStructDef()
        // Deep clone members
        for member_idx in 0..self.members.len() {
            mut m := Declaration()
            self.members.get(member_idx, m)?
            result.members.push(m.clone())
        }
        // Deep clone default_values Map<Str, Expr>
        for dv_idx in 0..self.default_values.keys.len() {
            mut k := ""
            mut v := Expr()
            self.default_values.keys.get(dv_idx, k)?
            self.default_values.values.get(dv_idx, v)?
            result.default_values.set(k.clone(), v.clone())
        }
        catch (err: IndexOutOfBoundsError) { panic(loc(), err.msg) }
        return result
    }

    /** Helper to find a member by name */
    get_member := func(self: SStructDef, member_name: Str) returns Declaration throws Str {
        for i in 0..self.members.len() {
            mut member := Declaration()
            self.members.get(i, member)?
            if member.name.eq(member_name) {
                return member
            }
        }
        catch (err: IndexOutOfBoundsError) {
            throw err.msg
        }
        throw format(loc(), "Member '", member_name, "' not found")
    }

    /** Helper to find a member by name or return an error */
    get_member_or_err := func(self: SStructDef, member_name: Str, struct_name: Str, path: Str, e: Expr) returns Declaration throws Str {
        member := self.get_member(member_name)?
        catch (err: Str) {
            throw e.error(path, "type", format("Struct '", struct_name, "' has no member '", member_name, "'"))
        }
        return member
    }
}

SNamespaceDef := struct {
    mut type_name: Str = ""
    mut members: Vec = Vec.new(Declaration)
    mut default_values: Map = Map.new(Str, Expr)
}

namespace SNamespaceDef {
    clone := func(self: SNamespaceDef) returns SNamespaceDef {
        mut result := SNamespaceDef()
        result.type_name = self.type_name.clone()
        // Deep clone members
        for member_idx in 0..self.members.len() {
            mut m := Declaration()
            self.members.get(member_idx, m)?
            result.members.push(m.clone())
        }
        // Deep clone default_values Map<Str, Expr>
        for dv_idx in 0..self.default_values.keys.len() {
            mut k := ""
            mut v := Expr()
            self.default_values.keys.get(dv_idx, k)?
            self.default_values.values.get(dv_idx, v)?
            result.default_values.set(k.clone(), v.clone())
        }
        catch (err: IndexOutOfBoundsError) { panic(loc(), err.msg) }
        return result
    }
}

Literal := enum {
    Number: Str,  // TODO support more kinds of numbers
    Str: Str,
    List: Str,  // TODO You can call it tupple too. who cares? it's not even tested yet, just parsed
}

literal_clone := func(lit: Literal) returns Literal {
    switch lit {
    case Literal.Number(s):
        // TODO Bug #56: inline s.clone() should work here
        num_cloned := s.clone()
        return Literal.Number(num_cloned)
    case Literal.Str(s):
        str_cloned := s.clone()
        return Literal.Str(str_cloned)
    case Literal.List(s):
        list_cloned := s.clone()
        return Literal.List(list_cloned)
    }
    // Unreachable, but TIL requires a return
    return lit
}

NodeType := enum {
    Body,
    LLiteral: Literal,
    FCall: Bool,  // Issue #132: Bool indicates if call has ? (does_throw)
    Identifier: Str,
    Declaration: Declaration,
    Assignment: Str,
    NamedArg: Str,  // Named argument in function call: name=value
    FuncDef: SFuncDef,
    EnumDef: SEnumDef,
    StructDef: SStructDef,
    NamespaceDef: SNamespaceDef,
    Return,
    Throw,
    Catch,
    Break,
    Continue,
    If,
    While,
    Switch,
    DefaultCase,
    Range,
    Pattern: PatternInfo,  // Pattern matching for switch case with payload extraction
    ForIn: Str,  // for VAR: TYPE in COLLECTION - payload is the TYPE name
}

// Deep clone a NodeType, including all payloads
// TODO Bug #56: inline clone calls should work in enum payloads
node_type_clone := func(nt: NodeType) returns NodeType {
    switch nt {
    case NodeType.LLiteral(lit):
        lit_cloned := literal_clone(lit)
        return NodeType.LLiteral(lit_cloned)
    case NodeType.Identifier(s):
        id_cloned := s.clone()
        return NodeType.Identifier(id_cloned)
    case NodeType.Declaration(decl):
        decl_cloned := decl.clone()
        return NodeType.Declaration(decl_cloned)
    case NodeType.Assignment(s):
        assign_cloned := s.clone()
        return NodeType.Assignment(assign_cloned)
    case NodeType.NamedArg(s):
        named_arg_cloned := s.clone()
        return NodeType.NamedArg(named_arg_cloned)
    case NodeType.FuncDef(fd):
        fd_cloned := fd.clone()
        return NodeType.FuncDef(fd_cloned)
    case NodeType.EnumDef(ed):
        ed_cloned := ed.clone()
        return NodeType.EnumDef(ed_cloned)
    case NodeType.StructDef(sd):
        sd_cloned := sd.clone()
        return NodeType.StructDef(sd_cloned)
    case NodeType.NamespaceDef(nd):
        nd_cloned := nd.clone()
        return NodeType.NamespaceDef(nd_cloned)
    case NodeType.Pattern(pi):
        pi_cloned := pi.clone()
        return NodeType.Pattern(pi_cloned)
    case NodeType.ForIn(s):
        forin_cloned := s.clone()
        return NodeType.ForIn(forin_cloned)
    case NodeType.FCall(does_throw):
        return NodeType.FCall(does_throw)
    case:
        // Simple variants (Body, Return, etc.) have no payload to clone
        return nt
    }
}

Expr := struct {
    mut node_type: NodeType = NodeType.Body
    mut params: Vec = Vec.new(Expr)
    mut line: I64 = 0
    mut col: I64 = 0
}

namespace Expr {
    new_parse := func(node_type: NodeType, token: Token, params: Vec) returns Expr {
        return Expr(node_type=node_type, params=params, line=token.line, col=token.col)
    }

    new_explicit := func(node_type: NodeType, params: Vec, line: I64, col: I64) returns Expr {
        return Expr(node_type=node_type, params=params, line=line, col=col)
    }

    new_clone := func(node_type: NodeType, e: Expr, params: Vec) returns Expr {
        return Expr.new_explicit(node_type, params, e.line, e.col)
    }

    len := func(self: Expr) returns I64 {
        return self.params.len()
    }

    to_str := func(self: Expr) returns Str {
        mut s := "Expr{"
        switch self.node_type {
        case NodeType.Identifier(name):
            s = s.concat("Identifier(\"")
            s = s.concat(name)
            s = s.concat("\")")
        case:
            s = s.concat("NodeType.?")
        }
        s = s.concat(", line=")
        s = s.concat(self.line.to_str())
        s = s.concat(", params=[")
        mut i := 0
        for p: Expr in self.params {
            if gt(i, 0) {
                s = s.concat(", ")
            }
            s = s.concat(p.to_str())
            i = add(i, 1)
        }
        s = s.concat("]}")
        return s
    }

    size := func(_self: Expr) returns I64 {
        return size_of(Expr)
    }

    get := func(self: Expr, i: I64) returns Expr throws Str {
        if i.lt(self.params.len()) {
            mut result := Expr()
            self.params.get(i, result)?
            return result
        }
        catch (err: IndexOutOfBoundsError) {
            throw err.msg
        }
        throw self.lang_error("<internal>", "assert", format("Expr index ", i.to_str(), " out of bounds (len: ", self.params.len().to_str(), ")."))
    }

    clone := func(self: Expr) returns Expr {
        // Deep clone: clone node_type and each element in params
        mut cloned_params := Vec.new(Expr)
        for i in 0..self.params.len() {
            mut e := Expr()
            self.params.get(i, e)?
            cloned_params.push(e.clone())
        }
        catch (err: IndexOutOfBoundsError) { panic(loc(), err.msg) }
        return Expr(node_type=node_type_clone(self.node_type), params=cloned_params, line=self.line, col=self.col)
    }

    exit_error := proc(self: Expr, phase: Str, msg: Str) {
        line_str := self.line.to_str()
        col_str := self.col.to_str()
        if phase.eq("warning") {
            println(format(line_str, ":", col_str, ": ", LANG_NAME, " WARNING: ", msg, "\nExplanation: This should never happen, this is a bug in the language."))
        } else {
            println(format(line_str, ":", col_str, ": ", LANG_NAME, " ", phase, " ERROR: ", msg, "\nExplanation: This should never happen, this is a bug in the language."))
        }
        exit(1)
    }

    lang_error := func(self: Expr, path: Str, phase: Str, msg: Str) returns Str {
        prefix := path.concat(":").concat(self.line.to_str()).concat(":").concat(self.col.to_str()).concat(": ")
        suffix := "\nExplanation: This should never happen, this is a bug in the language."
        if phase.eq("warning") {
            return prefix.concat(LANG_NAME).concat(" WARNING: ").concat(msg).concat(suffix)
        }
        return prefix.concat(LANG_NAME).concat(" ").concat(phase).concat(" ERROR: ").concat(msg).concat(suffix)
    }

    todo_error := func(self: Expr, path: Str, phase: Str, msg: Str) returns Str {
        prefix := path.concat(":").concat(self.line.to_str()).concat(":").concat(self.col.to_str()).concat(": ")
        suffix := "\nExplanation: Not implemented yet, this is a missing feature in the language."
        if phase.eq("warning") {
            return prefix.concat(LANG_NAME).concat(" WARNING: ").concat(msg).concat(suffix)
        }
        return prefix.concat(LANG_NAME).concat(" ").concat(phase).concat(" ERROR: ").concat(msg).concat(suffix)
    }

    error := func(self: Expr, path: Str, phase: Str, msg: Str) returns Str {
        prefix := path.concat(":").concat(self.line.to_str()).concat(":").concat(self.col.to_str()).concat(": ")
        if phase.eq("warning") {
            return prefix.concat("WARNING: ").concat(msg)
        }
        return prefix.concat(phase).concat(" ERROR: ").concat(msg)
    }

    params_clone := func(self: Expr) returns Vec throws Str {
        mut cloned := Vec.new(Expr)
        for i in 0..self.params.len() {
            mut elem := Expr()
            self.params.get(i, elem)?
            cloned.push(elem)
        }
        catch (err: IndexOutOfBoundsError) { throw concat("Expr.params_clone: ", err.msg) }
        return cloned
    }
}

is_literal := func(t: Token) returns Bool {
    tt := t.token_type
    switch tt {
    case TokenType.String:
        return true
    case TokenType.Number:
        return true
    case:
        return false
    }
}

TTypeDef := enum {
    TEnumDef,
    TStructDef,
}

ValueType := enum {
    TFunction: FunctionType,
    TType: TTypeDef,
    TCustom: Str,
    TMulti: Str,
}

// Deep clone a ValueType, including string payloads
// TODO Bug #56: inline clone calls should work in enum payloads
value_type_clone := func(vt: ValueType) returns ValueType {
    switch vt {
    case ValueType.TCustom(s):
        custom_cloned := s.clone()
        return ValueType.TCustom(custom_cloned)
    case ValueType.TMulti(s):
        multi_cloned := s.clone()
        return ValueType.TMulti(multi_cloned)
    case:
        // TFunction and TType have no heap allocations to clone
        return vt
    }
}

value_type_to_str := func(arg_type: ValueType) returns Str {
    switch arg_type {
    case ValueType.TType(TTypeDef.TEnumDef):
        return "enum"
    case ValueType.TType(TTypeDef.TStructDef):
        return "struct"
    case ValueType.TFunction(FunctionType.FTFunc):
        return "func"
    case ValueType.TFunction(FunctionType.FTFuncExt):
        return "func"
    case ValueType.TFunction(FunctionType.FTProc):
        return "proc"
    case ValueType.TFunction(FunctionType.FTProcExt):
        return "proc"
    case ValueType.TFunction(FunctionType.FTMacro):
        return "macro"
    case ValueType.TMulti(type_name):
        return type_name
    case ValueType.TCustom(type_name):
        return type_name
    case:
        return "unknown"
    }
}

str_to_value_type := func(arg_type: Str) returns ValueType {
    if arg_type.eq("func") {
        return ValueType.TFunction(FunctionType.FTFunc)
    }
    if arg_type.eq("proc") {
        return ValueType.TFunction(FunctionType.FTProc)
    }
    if arg_type.eq("macro") {
        return ValueType.TFunction(FunctionType.FTMacro)
    }
    if arg_type.eq("enum") {
        return ValueType.TType(TTypeDef.TEnumDef)
    }
    if arg_type.eq("struct") {
        return ValueType.TType(TTypeDef.TStructDef)
    }
    // Default: custom type
    return ValueType.TCustom(arg_type)
}

parse_literal := func(mut lexer: Lexer, t: Token) returns Expr throws Str {
    params := Vec.new(Expr)
    tt := t.token_type
    mut node_type := NodeType.Body  // Default, will be overwritten

    switch tt {
    case TokenType.String:
        // TODO Bug #56: inline clone should work here
        node_type = node_type_clone(NodeType.LLiteral(Literal.Str(t.token_str)))
    case TokenType.Number:
        // TODO: parse and validate number
        node_type = NodeType.LLiteral(Literal.Number(t.token_str))
    case:
        throw t.error(lexer.path, format("Trying to parse a token that's not a literal as a literal, found '", token_type_to_str(t.token_type), "'."))
    }

    e := Expr.new_parse(node_type, t.clone(), params)
    lexer.advance(1)?
    return e
}

parse_args := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    mut rightparent_found := false
    mut params := Vec.new(Expr)
    initial_current := lexer.current
    _ := lexer.expect(TokenType.LeftParen)?
    mut list_t := lexer.peek()?
    mut expect_comma := false

    while not(lexer.is_eof(0)?).and(not(rightparent_found)) {
        switch list_t.token_type {
        case TokenType.RightParen:
            _ := lexer.expect(TokenType.RightParen)?
            rightparent_found = true
        case TokenType.Comma:
            if expect_comma {
                _ := lexer.expect(TokenType.Comma)?
                expect_comma = false
                list_t = lexer.peek()?
            } else {
                throw list_t.error(lexer.path, "Unexpected ','.")
            }
        case:
            if expect_comma {
                throw list_t.error(lexer.path, format("Expected ')' or ',', found '", token_type_to_str(list_t.token_type), "'."))
            }
            expect_comma = true
            prim := parse_primary(lexer)?
            // Check for named argument: identifier = value
            next_t := lexer.peek()?
            mut is_named_arg := false
            switch next_t.token_type {
            case TokenType.Equal:
                is_named_arg = true
            case:
            }
            if is_named_arg {
                switch prim.node_type {
                case NodeType.Identifier(arg_name):
                    if prim.params.len().gt(0) {
                        throw next_t.error(lexer.path, "Named argument name cannot be a dotted identifier")
                    }
                    lexer.advance(1)? // consume '='
                    value := parse_primary(lexer)?
                    mut named_arg_params := Vec.new(Expr)
                    named_arg_params.push(value)
                    // TODO Bug #56: inline clone should work here
                    named_arg := Expr.new_explicit(
                        node_type_clone(NodeType.NamedArg(arg_name)),
                        named_arg_params, prim.line, prim.col)
                    params.push(named_arg)
                case:
                    throw next_t.error(lexer.path, "Named argument requires a simple identifier before '='")
                }
            } else {
                params.push(prim)
            }
            list_t = lexer.peek()?
        }
    }

    switch list_t.token_type {
    case TokenType.RightParen:
        // TODO properly parse lists besides function definition arguments
        initial_token := lexer.get_token(initial_current)?.clone()
        return Expr.new_parse(NodeType.LLiteral(Literal.List("")), initial_token, params)
    case:
        throw list_t.error(lexer.path, "Expected closing parentheses.")
    }
}

parse_assignment := func(mut lexer: Lexer, t: Token, name: Str) returns Expr throws Str {
    _ := lexer.expect(TokenType.Equal)?
    mut params := Vec.new(Expr)
    value := parse_primary(lexer)?
    params.push(value)
    result := Expr.new_parse(NodeType.Assignment(name), t.clone(), params)
    return result
}

validate_mut_copy_own_modifiers := func(t: Token, path: Str, modifier: Str, is_mut: Bool, is_copy: Bool, is_own: Bool) throws Str {
    if modifier.eq("mut") {
        if is_copy {
            throw t.error(path, "Cannot use both 'mut' and 'copy' on the same parameter. Use 'mut' for mutable reference or 'copy' for explicit copy.")
        }
        if is_own {
            throw t.error(path, "Cannot use both 'mut' and 'own' on the same parameter. Use 'mut' for mutable reference or 'own' for ownership transfer.")
        }
    }
    if modifier.eq("copy") {
        if is_mut {
            throw t.error(path, "Cannot use both 'mut' and 'copy' on the same parameter. Use 'mut' for mutable reference or 'copy' for explicit copy.")
        }
        if is_own {
            throw t.error(path, "Cannot use both 'own' and 'copy' on the same parameter. Use 'own' for ownership transfer or 'copy' for explicit copy.")
        }
    }
    if modifier.eq("own") {
        if is_mut {
            throw t.error(path, "Cannot use both 'mut' and 'own' on the same parameter. Use 'mut' for mutable reference or 'own' for ownership transfer.")
        }
        if is_copy {
            throw t.error(path, "Cannot use both 'own' and 'copy' on the same parameter. Use 'own' for ownership transfer or 'copy' for explicit copy.")
        }
    }
}

parse_func_proc_args := func(mut lexer: Lexer) returns Vec throws Str, IndexOutOfBoundsError {
    _ := lexer.expect(TokenType.LeftParen)?
    mut rightparent_found := false
    mut args := Vec.new(Declaration)
    mut t := lexer.peek()?
    mut expect_comma := false
    mut expect_colon := false
    mut expect_name := true
    mut is_variadic := false
    mut arg_name := "unnamed"
    mut is_mut := false
    mut is_copy := false
    mut is_own := false

    while not(lexer.is_eof(0)?).and(not(rightparent_found)) {
        tt := t.token_type
        switch tt {
        case TokenType.RightParen:
            rightparent_found = true
            if expect_colon {
                throw t.error(lexer.path, format("Expected ': Type' after arg name '", arg_name, "' before ')'."))
            }
            lexer.advance(1)?
        case TokenType.Comma:
            if expect_colon {
                throw t.error(lexer.path, format("Expected ': Type' after arg name '", arg_name, "', but found ','."))
            }
            if expect_name {
                throw t.error(lexer.path, "Expected arg name before ','.")
            }
            if expect_comma {
                expect_comma = false
                expect_colon = false
                expect_name = true
                is_mut = false
                is_copy = false
                is_own = false
                _ := lexer.expect(TokenType.Comma)?
                t = lexer.peek()?
            } else {
                throw t.error(lexer.path, "Unexpected ','.")
            }
        case TokenType.Colon:
            if expect_colon {
                expect_colon = false
                expect_name = false
                expect_comma = false
                lexer.advance(1)?
                t = lexer.peek()?
                tt2 := t.token_type
                switch tt2 {
                case TokenType.Identifier:
                    // continue
                case TokenType.DoubleDot:
                    // continue
                case:
                    throw t.error(lexer.path, format("Expected type after '", arg_name, ":', but found '", t.token_str, "'."))
                }
            } else {
                throw t.error(lexer.path, "Unexpected ':'.")
            }
        case TokenType.DoubleDot:
            if expect_colon {
                throw t.error(lexer.path, format("Expected ': Type' after arg name '", arg_name, "', but found '..'."))
            }
            if expect_comma {
                throw t.error(lexer.path, format("Expected ',', found '", token_type_to_str(t.token_type), "'."))
            }
            if expect_name {
                throw t.error(lexer.path, format("Expected arg name, found '", token_type_to_str(t.token_type), "'."))
            }
            is_variadic = true
            lexer.advance(1)?
            t = lexer.peek()?
        case TokenType.Identifier:
            if expect_colon {
                throw t.error(lexer.path, format("Expected ': Type' after arg name '", arg_name, "', but found '", t.token_str, "'."))
            }
            if expect_comma {
                throw t.error(lexer.path, format("Expected ',', found identifier '", t.token_str, "'."))
            }
            if expect_name {
                arg_name = t.token_str
                expect_colon = true
                expect_name = false
                lexer.advance(1)?
                t = lexer.peek()?
            } else {
                mut decl := Declaration()
                decl.name = arg_name
                if is_variadic {
                    // TODO Bug #56: inline clone should work here
                    decl.value_type = value_type_clone(ValueType.TMulti(t.token_str))
                    is_variadic = false
                } else {
                    decl.value_type = str_to_value_type(t.token_str)
                }
                decl.is_mut = is_mut
                decl.is_copy = is_copy
                decl.is_own = is_own
                lexer.advance(1)?
                t = lexer.peek()?
                // Check for optional default value: `= expr`
                tt3 := t.token_type
                switch tt3 {
                case TokenType.Equal:
                    lexer.advance(1)?  // consume '='
                    mut default_expr := parse_primary(lexer)?
                    // Must allocate heap memory - to_ptr(default_expr) would create dangling pointer
                    decl.default_value.data = malloc(size_of(Expr))?
                    catch (err: BadAlloc) { panic(loc(), "parse_func_proc_args: malloc failed") }
                    memcpy(decl.default_value.data, to_ptr(default_expr), size_of(Expr))
                case:
                    // No default value
                }
                args.push(decl)
                expect_comma = true
                is_mut = false
                is_copy = false
                is_own = false
                // Don't advance again - we already did above or parse_expression did
                t = lexer.peek()?
                continue
            }
        case TokenType.Mut:
            if not(expect_name) {
                throw t.error(lexer.path, "Unexpected 'mut' in argument list.")
            }
            validate_mut_copy_own_modifiers(t, lexer.path, "mut", is_mut, is_copy, is_own)?
            is_mut = true
            lexer.advance(1)?
            t = lexer.peek()?
        case TokenType.Copy:
            if not(expect_name) {
                throw t.error(lexer.path, "Unexpected 'copy' in argument list.")
            }
            validate_mut_copy_own_modifiers(t, lexer.path, "copy", is_mut, is_copy, is_own)?
            is_copy = true
            lexer.advance(1)?
            t = lexer.peek()?
        case TokenType.Own:
            if not(expect_name) {
                throw t.error(lexer.path, "Unexpected 'own' in argument list.")
            }
            validate_mut_copy_own_modifiers(t, lexer.path, "own", is_mut, is_copy, is_own)?
            is_own = true
            lexer.advance(1)?
            t = lexer.peek()?
        case:
            throw t.error(lexer.path, format("Unexpected '", token_type_to_str(t.token_type), "' in func/proc args."))
        }
    }


    final_tt := t.token_type
    switch final_tt {
    case TokenType.RightParen:
        return args
    case:
        throw t.error(lexer.path, "Expected closing parentheses.")
    }
}

func_proc_returns := func(mut lexer: Lexer) returns Vec throws Str, IndexOutOfBoundsError {
    mut end_found := false
    mut return_types := Vec.new(ValueType)
    mut t := lexer.peek()?
    lexer.advance(1)?
    tt := t.token_type
    switch tt {
    case TokenType.Returns:
        // Continue to parse return types
    case:
        return return_types
    }
    t = lexer.peek()?
    mut expect_comma := false

    while not(lexer.is_eof(0)?).and(not(end_found)) {
        tt2 := t.token_type
        switch tt2 {
        case TokenType.Throws:
            end_found = true
            lexer.advance(1)?
        case TokenType.LeftBrace:
            end_found = true
            lexer.advance(1)?
        case TokenType.Comma:
            if expect_comma {
                _ := lexer.expect(TokenType.Comma)?
                expect_comma = false
                t = lexer.peek()?
            } else {
                throw t.error(lexer.path, "Unexpected ','.")
            }
        case TokenType.Identifier:
            if expect_comma {
                throw t.error(lexer.path, format("Expected ',', found '", token_type_to_str(t.token_type), "'."))
            }
            return_types.push(str_to_value_type(t.token_str))
            expect_comma = true
            lexer.advance(1)?
            t = lexer.peek()?
        case:
            throw t.error(lexer.path, format("Unexpected '", token_type_to_str(t.token_type), "' in func/proc returns."))
        }
    }


    if end_found {
        return return_types
    } else {
        throw t.error(lexer.path, "Expected '{{' or 'throws' after return values.")
    }
}

// TODO DRY with func_proc_returns ?
func_proc_throws := func(mut lexer: Lexer) returns Vec throws Str, IndexOutOfBoundsError {
    mut end_found := false
    mut return_types := Vec.new(ValueType)
    mut t := lexer.previous()?
    tt := t.token_type
    switch tt {
    case TokenType.Throws:
        // Continue to parse throw types
    case:
        return return_types
    }
    t = lexer.peek()?
    mut expect_comma := false

    while not(lexer.is_eof(0)?).and(not(end_found)) {
        tt2 := t.token_type
        switch tt2 {
        case TokenType.LeftBrace:
            end_found = true
            lexer.advance(1)?
        case TokenType.Comma:
            if expect_comma {
                expect_comma = false
                lexer.advance(1)?
                t = lexer.peek()?
            } else {
                throw t.error(lexer.path, "Unexpected ','.")
            }
        case TokenType.Identifier:
            if expect_comma {
                throw t.error(lexer.path, format("Expected ',', found '", token_type_to_str(t.token_type), "'."))
            }
            return_types.push(str_to_value_type(t.token_str))
            expect_comma = true
            lexer.advance(1)?
            t = lexer.peek()?
        case:
            throw t.error(lexer.path, format("Unexpected '", token_type_to_str(t.token_type), "' in func/proc throws."))
        }
    }


    if end_found {
        return return_types
    } else {
        throw t.error(lexer.path, "Expected '{{' after throw values.")
    }
}

parse_func_proc_definition := func(mut lexer: Lexer, function_type: FunctionType) returns Expr throws Str {
    lexer.advance(1)?
    t := lexer.peek()?
    if lexer.is_eof(1)? {
        throw t.error(lexer.path, "expected '(' after 'func' or 'proc', found EOF.")
    }
    tt := t.token_type
    switch tt {
    case TokenType.LeftParen:
        // Continue
    case:
        throw t.error(lexer.path, format("expected '(' after 'func', found '", token_type_to_str(t.token_type), "'."))
    }
    args := parse_func_proc_args(lexer)?
    return_types := func_proc_returns(lexer)?
    throw_types := func_proc_throws(lexer)?

    // Bug #146: Reset loop var counter per-function for deterministic output
    saved_loop_counter := save_loop_var_counter()
    mut _ignore := reset_loop_var_counter()

    body_expr := parse_body(lexer, TokenType.RightBrace)?
    body := body_expr.params

    _ignore = restore_loop_var_counter(saved_loop_counter)

    // ext_func/ext_proc cannot have a body
    mut is_ext := false
    switch function_type {
    case FunctionType.FTFuncExt:
        is_ext = true
    case FunctionType.FTProcExt:
        is_ext = true
    case:
        is_ext = false
    }
    if is_ext.and(body.len().gt(0)) {
        throw t.error(lexer.path, "ext_func/ext_proc cannot have a body")
    }

    mut func_def := SFuncDef()
    func_def.function_type = function_type
    func_def.args = args
    func_def.return_types = return_types
    func_def.throw_types = throw_types
    func_def.body = body
    func_def.source_path = lexer.path.clone()

    params := Vec.new(Expr)
    e := Expr.new_parse(NodeType.FuncDef(func_def), t.clone(), params)
    catch (err: IndexOutOfBoundsError) {
        throw err.msg
    }
    return e
}

enum_definition := func(mut lexer: Lexer) returns Expr throws Str {
    initial_current := lexer.current
    lexer.advance(1)?

    t := lexer.peek()?
    tt := t.token_type
    switch tt {
    case TokenType.LeftBrace:
        // Continue
    case:
        throw t.error(lexer.path, "Expected '{{' after 'enum'.")
    }
    if lexer.is_eof(1)? {
        t2 := lexer.peek()?
        throw t2.error(lexer.path, "expected identifier after 'enum {{', found EOF.")
    }
    lexer.advance(1)?
    mut variants := Vec.new(EnumVariant)  // Bug #38 fix: Use Vec for deterministic order

    mut end_found := false
    while lexer.current.lt(lexer.len()).and(not(end_found)) {
        it_t := lexer.peek()?
        itt := it_t.token_type
        switch itt {
        case TokenType.RightBrace:
            end_found = true
        case TokenType.Identifier:
            enum_val_name := it_t.token_str
            next_t := lexer.next()?
            ntt := next_t.token_type
            switch ntt {
            case TokenType.Colon:
                next2_t := lexer.peek_ahead(2)?
                n2tt := next2_t.token_type
                switch n2tt {
                case TokenType.Identifier:
                    enum_val_type := next2_t.token_str
                    // Some(ValueType) - store pointer to payload type
                    // Bug #67 fix: allocate heap memory (stack ptr would be dangling after loop)
                    mut payload_type := str_to_value_type(enum_val_type)
                    mut payload_ptr := Ptr()
                    payload_ptr.data = malloc(size_of(ValueType))?
                    catch (err: BadAlloc) { panic(loc(), "enum_definition: malloc failed") }
                    memcpy(payload_ptr.data, to_ptr(payload_type), size_of(ValueType))
                    mut variant := EnumVariant()
                    variant.name = enum_val_name
                    variant.payload_type = payload_ptr
                    variants.push(variant)
                    lexer.advance(2)?
                case:
                    throw next2_t.error(lexer.path, format("Expected type identifier after '", enum_val_name, " :', found '", token_type_to_str(next2_t.token_type), "'."))
                }
            case TokenType.Comma:
                mut comma_variant := EnumVariant()
                comma_variant.name = enum_val_name
                variants.push(comma_variant)  // None = NULL pointer (default)
                // Continue
            case TokenType.RightBrace:
                mut brace_variant := EnumVariant()
                brace_variant.name = enum_val_name
                variants.push(brace_variant)  // None = NULL pointer (default)
                end_found = true
                lexer.advance(1)?  // Advance past the RightBrace since we peeked it with next()
            case:
                throw next_t.error(lexer.path, format("Expected ',' or ':' after '", enum_val_name, "', found '", token_type_to_str(next_t.token_type), "'."))
            }
        case TokenType.Comma:
            // Skip comma
        case:
            throw it_t.error(lexer.path, format("Expected '}}' to end enum or a new identifier, found '", token_type_to_str(it_t.token_type), "'."))
        }
        lexer.advance(1)?
    }
    if not(end_found) {
        throw t.error(lexer.path, "Expected '}}' to end enum.")
    }
    params := Vec.new(Expr)
    mut enum_def := SEnumDef()
    enum_def.variants = variants
    initial_token := lexer.get_token(initial_current)?.clone()
    e := Expr.new_parse(NodeType.EnumDef(enum_def), initial_token, params)
    catch (err: IndexOutOfBoundsError) {
        throw err.msg
    }
    return e
}

parse_struct_definition := func(mut lexer: Lexer) returns Expr throws Str {
    _ := lexer.expect(TokenType.Struct)?
    t := lexer.peek()?
    tt := t.token_type
    switch tt {
    case TokenType.LeftBrace:
        // Continue parsing struct
    case:
        throw t.error(lexer.path, "Expected '{{' after 'struct'.")
    }
    if lexer.is_eof(1)? {
        t2 := lexer.peek()?
        throw t2.error(lexer.path, "expected 'identifier' after 'struct {{', found EOF.")
    }
    lexer.advance(1)?
    body := parse_body(lexer, TokenType.RightBrace)?

    mut members := Vec.new(Declaration)  // Vec of Declaration
    mut default_values := Map.new(Str, Expr)  // Map<Str, Expr>

    for i in 0..body.params.len() {
        mut p := Expr()
        body.params.get(i, p)?
        nt := p.node_type

        switch nt {
        case NodeType.Declaration(decl):
            members.push(decl.clone())

            if p.params.len().eq(1) {
                mut val := Expr()
                p.params.get(0, val)?
                default_values.set(decl.name.clone(), val.clone())
            } else {
                // TODO allow not setting default values in struct members
                throw t.error(lexer.path, "all declarations inside struct definitions must have a value for now")
            }
        case NodeType.Assignment(name):
            // Handle const := value inside structs (methods like eq := func(...))
            if p.params.len().eq(1) {
                mut assign_val := Expr()
                p.params.get(0, assign_val)?
                // Create a Declaration for the method
                mut assign_decl := Declaration()
                assign_decl.name = name.clone()
                assign_decl.value_type = ValueType.TCustom(INFER_TYPE)
                assign_decl.is_mut = false
                assign_decl.is_copy = false
                assign_decl.is_own = false
                members.push(assign_decl.clone())
                default_values.set(name.clone(), assign_val.clone())
            } else {
                throw t.error(lexer.path, format("struct assignment '", name, "' must have exactly one value"))
            }
        case:
            throw t.error(lexer.path, "expected only declarations inside struct definition")
        }
    }

    mut struct_def := SStructDef()
    struct_def.members = members
    struct_def.default_values = default_values
    params := Vec.new(Expr)
    result := Expr.new_parse(NodeType.StructDef(struct_def), t.clone(), params)
    catch (err: IndexOutOfBoundsError) {
        throw concat("parse_struct_definition: ", err.msg)
    }
    return result
}

// Parse namespace TypeName { declarations... }
parse_namespace_definition := func(mut lexer: Lexer) returns Expr throws Str {
    t := lexer.peek()?
    _ := lexer.expect(TokenType.Namespace)?

    // Expect type name (identifier)
    type_token := lexer.peek()?
    switch type_token.token_type {
    case TokenType.Identifier:
        // Continue
    case:
        throw type_token.error(lexer.path, format("Expected type name after 'namespace', found '", token_type_to_str(type_token.token_type), "'."))
    }
    type_name := type_token.token_str
    lexer.advance(1)?

    // Expect opening brace
    brace_token := lexer.peek()?
    switch brace_token.token_type {
    case TokenType.LeftBrace:
        // Continue
    case:
        throw brace_token.error(lexer.path, "Expected '{{' after namespace type name.")
    }
    lexer.advance(1)?

    // Parse body (same as struct body)
    body := parse_body(lexer, TokenType.RightBrace)?

    // Extract members and default values (same as struct)
    mut members := Vec.new(Declaration)
    mut default_values := Map.new(Str, Expr)

    for i in 0..body.params.len() {
        mut p := Expr()
        body.params.get(i, p)?
        nt := p.node_type

        switch nt {
        case NodeType.Declaration(decl):
            members.push(decl.clone())

            if p.params.len().eq(1) {
                mut val := Expr()
                p.params.get(0, val)?
                default_values.set(decl.name.clone(), val.clone())
            } else {
                throw t.error(lexer.path, "all declarations inside namespace blocks must have a value")
            }
        case NodeType.Assignment(name):
            // Handle const := value inside namespace (methods like eq := func(...))
            if p.params.len().eq(1) {
                mut assign_val := Expr()
                p.params.get(0, assign_val)?
                mut assign_decl := Declaration()
                assign_decl.name = name.clone()
                assign_decl.value_type = ValueType.TCustom(INFER_TYPE)
                assign_decl.is_mut = false
                assign_decl.is_copy = false
                assign_decl.is_own = false
                members.push(assign_decl.clone())
                default_values.set(name.clone(), assign_val.clone())
            } else {
                throw t.error(lexer.path, format("namespace assignment '", name, "' must have exactly one value"))
            }
        case:
            throw t.error(lexer.path, "expected only declarations inside namespace block")
        }
    }

    mut ns_def := SNamespaceDef()
    ns_def.type_name = type_name
    ns_def.members = members
    ns_def.default_values = default_values
    params := Vec.new(Expr)
    result := Expr.new_parse(NodeType.NamespaceDef(ns_def), t.clone(), params)

    catch (err: IndexOutOfBoundsError) {
        throw concat("parse_namespace_definition: ", err.msg)
    }
    return result
}

parse_primary_identifier := func(mut lexer: Lexer) returns Expr throws Str {
    initial_current := lexer.current
    t := lexer.peek()?
    mut next_t := lexer.next()?
    mut current_identifier := t.token_str
    mut params := Vec.new(Expr)
    mut result := Expr()

    mut continue_dot_loop := true
    while continue_dot_loop {
        switch next_t.token_type {
        case TokenType.Dot:
            next2_t := lexer.peek_ahead(2)?
            n2tt := next2_t.token_type
            switch n2tt {
            case TokenType.Identifier:
                current_identifier = next2_t.token_str
                lexer.advance(2)?
                // TODO Bug #56: inline clone should work here
                ident_expr := Expr.new_parse(
                    node_type_clone(NodeType.Identifier(current_identifier)),
                    t.clone(), Vec.new(Expr))
                params.push(ident_expr)
                next_t = lexer.next()?
            case:
                throw next2_t.error(lexer.path, format("expected identifier after '", current_identifier, ".', found '", token_type_to_str(next2_t.token_type), "'."))
            }
        case:
            continue_dot_loop = false
        }
    }

    initial_token := lexer.get_token(initial_current)?.clone()
    token_str_cloned := t.token_str.clone()
    e := Expr.new_parse(NodeType.Identifier(token_str_cloned), initial_token, params)
    lexer.advance(1)?

    ntt := next_t.token_type
    switch ntt {
    case TokenType.LeftParen:
        arg_list := parse_args(lexer)?
        mut fcall_params := Vec.new(Expr)
        fcall_params.push(e)
        for i in 0..arg_list.params.len() {
            mut arg := Expr()
            arg_list.params.get(i, arg)?
            fcall_params.push(arg)
        }
        initial_token2 := lexer.get_token(initial_current)?.clone()
        // Issue #132: Check for ? after function call (indicates call to throwing function)
        mut does_throw := false
        does_throw_peek := lexer.peek()?
        switch does_throw_peek.token_type {
        case TokenType.QuestionMark:
            lexer.advance(1)?
            does_throw = true
        case:
            does_throw = false
        }
        result = Expr.new_parse(NodeType.FCall(does_throw), initial_token2, fcall_params)

        // Handle chained method calls: a.method1().method2().method3()
        mut continue_loop := true
        while continue_loop {
            peek_t := lexer.peek()?
            ptt := peek_t.token_type
            switch ptt {
            case TokenType.Dot:
                // Consume the dot
                lexer.advance(1)?

                // Expect an identifier for the next method name
                method_t := lexer.peek()?
                mtt := method_t.token_type
                switch mtt {
                case TokenType.Identifier:
                    method_name := method_t.token_str.clone()
                    lexer.advance(1)?

                    // Check if it's a method call (has parentheses) or field access (no parentheses)
                    next_peek := lexer.peek()?
                    nptt := next_peek.token_type
                    switch nptt {
                    case TokenType.LeftParen:
                        // Method call: parse arguments
                        method_args := parse_args(lexer)?

                        // Create a new FCall with the method name as identifier and previous result as first arg
                        // This represents: method_name(result, args...)
                        method_id := Expr.new_parse(NodeType.Identifier(method_name), method_t.clone(), Vec.new(Expr))
                        mut new_params := Vec.new(Expr)
                        new_params.push(method_id)
                        new_params.push(result)  // Previous call result becomes first argument
                        for j in 0..method_args.params.len() {
                            mut method_arg := Expr()
                            method_args.params.get(j, method_arg)?
                            new_params.push(method_arg)
                        }

                        // Issue #132: Check for ? after chained method call
                        mut method_does_throw := false
                        method_does_throw_peek := lexer.peek()?
                        switch method_does_throw_peek.token_type {
                        case TokenType.QuestionMark:
                            lexer.advance(1)?
                            method_does_throw = true
                        case:
                            method_does_throw = false
                        }
                        result = Expr.new_parse(NodeType.FCall(method_does_throw), method_t, new_params)
                    case:
                        // Bug #32 fix: Field access on expression result (no parentheses)
                        // Collect all field names in the chain until we hit something else
                        mut field_params := Vec.new(Expr)
                        field_params.push(result)  // First param is the FCall/expression result
                        // TODO Bug #56: inline clone should work here
                        field_params.push(Expr.new_parse(
                            node_type_clone(NodeType.Identifier(method_name)),
                            method_t.clone(), Vec.new(Expr)))

                        // Continue collecting fields until we hit '(' (method call) or something else
                        mut continue_field_loop := true
                        while continue_field_loop {
                            peek_after := lexer.peek()?
                            patt := peek_after.token_type
                            switch patt {
                            case TokenType.Dot:
                                lexer.advance(1)?
                                next_field_t := lexer.peek()?
                                nftt := next_field_t.token_type
                                switch nftt {
                                case TokenType.Identifier:
                                    next_field_name := next_field_t.token_str.clone()
                                    lexer.advance(1)?

                                    // Check if this is a method call
                                    check_paren := lexer.peek()?
                                    cptt := check_paren.token_type
                                    switch cptt {
                                    case TokenType.LeftParen:
                                        // This is a method call on the field chain
                                        // Build the Identifier chain for the field access, then wrap in FCall
                                        // Bug #32 fix: Use "_" to signal expression-based field access
                                        field_access := Expr.new_parse(NodeType.Identifier("_"), method_t.clone(), field_params)

                                        method_args2 := parse_args(lexer)?

                                        method_id2 := Expr.new_parse(NodeType.Identifier(next_field_name), next_field_t.clone(), Vec.new(Expr))
                                        mut new_params2 := Vec.new(Expr)
                                        new_params2.push(method_id2)
                                        new_params2.push(field_access)
                                        for k in 0..method_args2.params.len() {
                                            mut method_arg2 := Expr()
                                            method_args2.params.get(k, method_arg2)?
                                            new_params2.push(method_arg2)
                                        }

                                        // Issue #132: Check for ? after field method call
                                        mut field_method_does_throw := false
                                        field_method_does_throw_peek := lexer.peek()?
                                        switch field_method_does_throw_peek.token_type {
                                        case TokenType.QuestionMark:
                                            lexer.advance(1)?
                                            field_method_does_throw = true
                                        case:
                                            field_method_does_throw = false
                                        }
                                        result = Expr.new_parse(NodeType.FCall(field_method_does_throw), next_field_t, new_params2)
                                        continue_field_loop = false
                                    case:
                                        // Another field access
                                        field_params.push(Expr.new_parse(NodeType.Identifier(next_field_name), next_field_t.clone(), Vec.new(Expr)))
                                    }
                                case:
                                    throw next_field_t.error(lexer.path, format("Expected identifier after '.', found '", token_type_to_str(next_field_t.token_type), "'"))
                                }
                            case:
                                // End of chain - create the field access expression
                                // Bug #32 fix: Use "_" as identifier name to signal that
                                // params[0] is an expression (FCall) to evaluate first, then access
                                // the remaining params as fields on its result
                                result = Expr.new_parse(NodeType.Identifier("_"), method_t.clone(), field_params)
                                continue_field_loop = false
                            }
                        }
                        continue_loop = false
                    }
                case:
                    throw method_t.error(lexer.path, format("Expected identifier after '.', found '", token_type_to_str(method_t.token_type), "')"))
                }
            case:
                continue_loop = false
            }
        }

        result = result
    case:
        result = e
    }
    catch (err: IndexOutOfBoundsError) {
        throw err.msg
    }
    return result
}

get_combined_name := func(path: Str, e: Expr) returns Str throws Str {
    mut to_return := ""
    nt := e.node_type
    switch nt {
        case NodeType.Identifier(id_str):
            to_return = to_return.concat(id_str)
            to_return = to_return.concat(".")
        case:
            throw e.lang_error(path, "parse", "get_combined_name() is to be called with Identifier expressions only")
    }

    for i in 0..e.params.len() {
        mut p := Expr()
        e.params.get(i, p)?
        pnt := p.node_type
        switch pnt {
            case NodeType.Identifier(id_str):
                to_return = to_return.concat(id_str)
                to_return = to_return.concat(".")
            case:
                throw e.lang_error(path, "parse", "the params of an identifier expression must be Identifier expressions only")
        }
    }

    // Remove the last '.'
    to_return = to_return.get_substr(0, to_return.len().sub(1))?
    catch (err: IndexOutOfBoundsError) { throw concat("get_combined_name: ", err.msg) }
    return to_return
}

parse_statement_identifier := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    t := lexer.peek()?
    mut next_t := lexer.next()?
    mut next_token_type := next_t.token_type

    ntt := next_token_type
    switch ntt {
    case TokenType.LeftParen:
        return parse_primary_identifier(lexer)?
    case TokenType.Dot:
        e := parse_primary_identifier(lexer)?
        ent := e.node_type
        mut is_fcall := false
        switch ent {
        case NodeType.FCall(_):
            is_fcall = true
        case:
            is_fcall = false
        }
        if is_fcall {
            return e
        }
        mut is_identifier := false
        switch ent {
        case NodeType.Identifier(name):
            is_identifier = true
        case:
            is_identifier = false
        }
        if not(is_identifier) {
            throw t.lang_error(lexer.path, "a series of identifiers and dots should have been parsed as identifier or function call")
        }

        next_t = lexer.peek()?
        next_token_type = next_t.token_type
        ntt2 := next_token_type
        switch ntt2 {
        case TokenType.Equal:
            name := get_combined_name(lexer.path, e)?
            return parse_assignment(lexer, t, name)?
        case:
            throw t.lang_error(lexer.path, "While parsing a '.', this should never happen")
        }
    case TokenType.Equal:
        lexer.advance(1)?
        return parse_assignment(lexer, t, t.token_str)?
    case TokenType.Colon:
        next_next_t := lexer.peek_ahead(2)?
        next_next_token_type := next_next_t.token_type
        identifier := t.token_str

        nntt := next_next_token_type
        switch nntt {
        case TokenType.Identifier:
            type_name := next_next_t.token_str
            return parse_declaration(lexer, false, false, type_name, true)?
        case TokenType.Equal:
            return parse_declaration(lexer, false, false, INFER_TYPE, false)?
        case:
            throw t.error(lexer.path, format("Expected Type or '=' after '", identifier, " :' in statement, found '", token_type_to_str(next_next_token_type), "'."))
        }
    case:
        throw t.error(lexer.path, format("Expected '(', ':' or '=' after identifier in statement, found '", token_type_to_str(next_token_type), "'."))
    }
}

parse_primary := func(mut lexer: Lexer) returns Expr throws Str {
    t := lexer.peek()?
    mut result := Expr()

    if is_literal(t) {
        result = parse_literal(lexer, t)?
        return result
    }

    tt := t.token_type
    switch tt {
    case TokenType.Func:
        result = parse_func_proc_definition(lexer, FunctionType.FTFunc)?
    case TokenType.FuncExt:
        result = parse_func_proc_definition(lexer, FunctionType.FTFuncExt)?
    case TokenType.Macro:
        result = parse_func_proc_definition(lexer, FunctionType.FTMacro)?
    case TokenType.Proc:
        result = parse_func_proc_definition(lexer, FunctionType.FTProc)?
    case TokenType.ProcExt:
        result = parse_func_proc_definition(lexer, FunctionType.FTProcExt)?
    case TokenType.Enum:
        result = enum_definition(lexer)?
    case TokenType.Struct:
        result = parse_struct_definition(lexer)?
    case TokenType.Namespace:
        result = parse_namespace_definition(lexer)?
    case TokenType.LeftParen:
        result = parse_args(lexer)?
    case TokenType.Identifier:
        result = parse_primary_identifier(lexer)?
    case:
        throw t.error(lexer.path, format("Expected primary expression, found '", token_type_to_str(t.token_type), "'."))
    }
    catch (err: IndexOutOfBoundsError) {
        throw concat("parse_primary: ", err.msg)
    }
    return result
}

return_statement := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    initial_current := lexer.current
    lexer.advance(1)?
    mut params := Vec.new(Expr)

    // Try to parse first primary expression
    prim := parse_primary(lexer)?
    params.push(prim)
    catch (err: Str) {
        // No primary expression found, that's okay for return statements
    }

    mut t := lexer.peek()?
    mut continue_loop := true
    while continue_loop {
        tt := t.token_type
        switch tt {
        case TokenType.Comma:
            lexer.advance(1)?
            prim2 := parse_primary(lexer)?
            params.push(prim2)
            t = lexer.peek()?
        case:
            continue_loop = false
        }
    }

    initial_token := lexer.get_token(initial_current)?.clone()


    return Expr.new_parse(NodeType.Return, initial_token, params)
}

parse_break_statement := func(mut lexer: Lexer) returns Expr throws Str {
    initial_current := lexer.current
    lexer.advance(1)? // consume 'break'
    initial_token := lexer.get_token(initial_current)?.clone()
    mut params := Vec.new(Expr)
    return Expr.new_parse(NodeType.Break, initial_token, params)
}

parse_continue_statement := func(mut lexer: Lexer) returns Expr throws Str {
    initial_current := lexer.current
    lexer.advance(1)? // consume 'continue'
    initial_token := lexer.get_token(initial_current)?.clone()
    mut params := Vec.new(Expr)
    return Expr.new_parse(NodeType.Continue, initial_token, params)
}

parse_throw_statement := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    initial_current := lexer.current
    lexer.advance(1)?
    mut params := Vec.new(Expr)

    // Parse first primary expression (required for throw)
    prim := parse_primary(lexer)?
    params.push(prim)

    mut t := lexer.peek()?
    mut continue_loop := true
    while continue_loop {
        tt := t.token_type
        switch tt {
        case TokenType.Comma:
            lexer.advance(1)?
            prim2 := parse_primary(lexer)?
            params.push(prim2)
            t = lexer.peek()?
        case:
            continue_loop = false
        }
    }

    initial_token := lexer.get_token(initial_current)?.clone()


    return Expr.new_parse(NodeType.Throw, initial_token, params)
}

parse_catch_statement := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    initial_current := lexer.current
    lexer.advance(1)? // consume 'catch'

    _ := lexer.expect(TokenType.LeftParen)? // expect '('

    // Parse the error variable name
    name_token := lexer.expect(TokenType.Identifier)?
    name := name_token.token_str.clone()
    // TODO Bug #56: inline clone should work here
    name_expr := Expr.new_parse(
        node_type_clone(NodeType.Identifier(name)),
        name_token.clone(), Vec.new(Expr))

    _ := lexer.expect(TokenType.Colon)? // expect ':'

    // Parse the exception type
    type_token := lexer.expect(TokenType.Identifier)?
    // TODO Bug #56: inline clone should work here
    type_expr := Expr.new_parse(
        node_type_clone(NodeType.Identifier(type_token.token_str)),
        type_token.clone(), Vec.new(Expr))

    _ := lexer.expect(TokenType.RightParen)? // expect ')'

    _ := lexer.expect(TokenType.LeftBrace)? // expect '{'
    body_expr := parse_body(lexer, TokenType.RightBrace)?

    mut params := Vec.new(Expr)
    params.push(name_expr)
    params.push(type_expr)
    params.push(body_expr)

    initial_token := lexer.get_token(initial_current)?.clone()


    return Expr.new_parse(NodeType.Catch, initial_token, params)
}

if_statement := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    initial_current := lexer.current
    lexer.advance(1)?
    mut params := Vec.new(Expr)

    prim := parse_primary(lexer)?
    params.push(prim)

    t := lexer.peek()?
    tt := t.token_type
    switch tt {
    case TokenType.LeftBrace:
        lexer.advance(1)?
    case:
        throw t.error(lexer.path, format("Expected '{{' after condition in 'if' statement, found '", token_type_to_str(t.token_type), "'."))
    }

    body := parse_body(lexer, TokenType.RightBrace)?
    params.push(body)

    t2 := lexer.peek()?
    t2tt := t2.token_type
    switch t2tt {
    case TokenType.Else:
        lexer.advance(1)?
        next := lexer.peek()?
        ntt := next.token_type
        switch ntt {
        case TokenType.If:
            nested_if := if_statement(lexer)?
            params.push(nested_if)
        case TokenType.LeftBrace:
            lexer.advance(1)?
            else_body := parse_body(lexer, TokenType.RightBrace)?
            params.push(else_body)
        case:
            throw t2.error(lexer.path, format("Expected '{{' or 'if' after 'else', found '", token_type_to_str(next.token_type), "'."))
        }
    case:
        // No else clause
    }

    initial_token := lexer.get_token(initial_current)?.clone()


    return Expr.new_parse(NodeType.If, initial_token, params)
}

while_statement := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    initial_current := lexer.current
    lexer.advance(1)?
    mut params := Vec.new(Expr)

    prim := parse_primary(lexer)?
    params.push(prim)

    t := lexer.peek()?
    tt := t.token_type
    switch tt {
    case TokenType.LeftBrace:
        lexer.advance(1)?
    case:
        throw t.error(lexer.path, "Expected '{{' after condition in 'while' statement.")
    }

    body := parse_body(lexer, TokenType.RightBrace)?
    params.push(body)

    initial_token := lexer.get_token(initial_current)?.clone()


    return Expr.new_parse(NodeType.While, initial_token, params)
}

// Bug #57 fix: Transform continue statements to include the step expression before continue.
// This ensures the loop variable is incremented/decremented even when continue is used.
// Transforms: continue -> { step_expr; continue }
transform_continue_with_step := func(expr: Expr, step_expr: Expr) returns Expr throws IndexOutOfBoundsError {
    switch expr.node_type {
    case NodeType.Continue:
        // Replace continue with { step_expr; continue }
        mut body_params := Vec.new(Expr)
        body_params.push(step_expr.clone())
        body_params.push(expr.clone())
        return Expr.new_explicit(NodeType.Body, body_params, expr.line, expr.col)
    case NodeType.While:
        // Don't recurse into nested loops - their continues are for their own loop
        return expr.clone()
    case NodeType.ForIn(_):
        // Don't recurse into nested loops
        return expr.clone()
    case:
        // Recurse into other nodes
        mut new_params := Vec.new(Expr)
        for i in 0..expr.params.len() {
            mut p := Expr()
            expr.params.get(i, p)?
            new_params.push(transform_continue_with_step(p, step_expr)?)
        }
        return Expr(node_type=expr.node_type, params=new_params, line=expr.line, col=expr.col)
    }
}

// Helper to desugar range-based for loop to while loop
// Shared by typed (for i: I64 in 0..10) and untyped (for i in 0..10) range loops
desugar_range_for := func(loop_var_name: Str, var_type: Str, start_expr: Expr, end_expr: Expr, body_expr: Expr, initial_token: Token) returns Expr throws Str {
    // Check if both bounds are numeric literals for compile-time direction detection
    // is_forward: 0 = unknown (runtime), 1 = forward, 2 = reverse
    mut is_forward := 0
    switch start_expr.node_type {
    case NodeType.LLiteral(start_lit):
        switch start_lit {
        case Literal.Number(start_str):
            switch end_expr.node_type {
            case NodeType.LLiteral(end_lit):
                switch end_lit {
                case Literal.Number(end_str):
                    start_val := start_str.to_i64()
                    end_val := end_str.to_i64()
                    if lt(start_val, end_val) {
                        is_forward = 1
                    } else {
                        is_forward = 2
                    }
                case:
                }
            case:
            }
        case:
        }
    case:
    }

    // Declaration with specified type (could be explicit or INFER_TYPE)
    mut decl := Declaration()
    decl.name = loop_var_name.clone()
    decl.value_type = str_to_value_type(var_type)
    decl.is_mut = true
    mut decl_params := Vec.new(Expr)
    decl_params.push(start_expr.clone())
    decl_expr := Expr.new_parse(NodeType.Declaration(decl), initial_token.clone(), decl_params)

    // Helper to create forward while loop
    make_fwd_while := func(body: Expr, loop_var: Str, end: Expr, tok: Token) returns Expr throws Str {
        mut lt_params := Vec.new(Expr)
        lt_ident := Expr.new_parse(NodeType.Identifier("lt"), tok.clone(), Vec.new(Expr))
        lt_params.push(lt_ident)
        cond_var := Expr.new_parse(node_type_clone(NodeType.Identifier(loop_var)), tok.clone(), lt_params)
        mut cond_params := Vec.new(Expr)
        cond_params.push(cond_var)
        cond_params.push(end.clone())
        cond := Expr.new_explicit(NodeType.FCall(false),cond_params, tok.line, tok.col)

        mut inc_method_params := Vec.new(Expr)
        inc_method := Expr.new_parse(NodeType.Identifier("inc"), tok.clone(), Vec.new(Expr))
        inc_method_params.push(inc_method)
        inc_var := Expr.new_parse(node_type_clone(NodeType.Identifier(loop_var)), tok.clone(), inc_method_params)
        mut inc_params := Vec.new(Expr)
        inc_params.push(inc_var)
        inc := Expr.new_explicit(NodeType.FCall(false),inc_params, tok.line, tok.col)

        // Bug #57 fix: Transform continue statements to include increment before continue
        transformed_body := transform_continue_with_step(body, inc)?
        mut body_params := transformed_body.params_clone()?
        body_params.push(inc.clone())
        while_body := Expr.new_explicit(NodeType.Body, body_params, body.line, body.col)

        mut while_params := Vec.new(Expr)
        while_params.push(cond)
        while_params.push(while_body)

        catch (err: IndexOutOfBoundsError) { throw err.msg }
        return Expr.new_explicit(NodeType.While, while_params, tok.line, tok.col)
    }

    // Helper to create reverse while loop
    make_rev_while := func(body: Expr, loop_var: Str, end: Expr, tok: Token) returns Expr throws Str {
        mut gt_params := Vec.new(Expr)
        gt_ident := Expr.new_parse(NodeType.Identifier("gt"), tok.clone(), Vec.new(Expr))
        gt_params.push(gt_ident)
        cond_var := Expr.new_parse(node_type_clone(NodeType.Identifier(loop_var)), tok.clone(), gt_params)
        mut cond_params := Vec.new(Expr)
        cond_params.push(cond_var)
        cond_params.push(end.clone())
        cond := Expr.new_explicit(NodeType.FCall(false),cond_params, tok.line, tok.col)

        mut dec_method_params := Vec.new(Expr)
        dec_method := Expr.new_parse(NodeType.Identifier("dec"), tok.clone(), Vec.new(Expr))
        dec_method_params.push(dec_method)
        dec_var := Expr.new_parse(node_type_clone(NodeType.Identifier(loop_var)), tok.clone(), dec_method_params)
        mut dec_params := Vec.new(Expr)
        dec_params.push(dec_var)
        dec := Expr.new_explicit(NodeType.FCall(false),dec_params, tok.line, tok.col)

        // Bug #57 fix: Transform continue statements to include decrement before continue
        transformed_body := transform_continue_with_step(body, dec)?
        mut body_params := transformed_body.params_clone()?
        body_params.push(dec.clone())
        while_body := Expr.new_explicit(NodeType.Body, body_params, body.line, body.col)

        mut while_params := Vec.new(Expr)
        while_params.push(cond)
        while_params.push(while_body)

        catch (err: IndexOutOfBoundsError) { throw err.msg }
        return Expr.new_explicit(NodeType.While, while_params, tok.line, tok.col)
    }

    mut loop_expr := Expr()
    if is_forward.eq(1) {
        // Compile-time: forward loop only
        loop_expr = make_fwd_while(body_expr, loop_var_name, end_expr, initial_token)?
    } else {
        if is_forward.eq(2) {
            // Compile-time: reverse loop only
            loop_expr = make_rev_while(body_expr, loop_var_name, end_expr, initial_token)?
        } else {
            // Runtime: if start < end { forward } else { reverse }
            fwd_while := make_fwd_while(body_expr, loop_var_name, end_expr, initial_token)?
            rev_while := make_rev_while(body_expr, loop_var_name, end_expr, initial_token)?

            mut dir_cond_params := Vec.new(Expr)
            dir_lt_ident := Expr.new_parse(NodeType.Identifier("lt"), initial_token.clone(), Vec.new(Expr))
            dir_cond_params.push(dir_lt_ident)
            dir_cond_params.push(start_expr.clone())
            dir_cond_params.push(end_expr.clone())
            dir_cond_expr := Expr.new_explicit(NodeType.FCall(false),dir_cond_params, initial_token.line, initial_token.col)

            mut fwd_body_params := Vec.new(Expr)
            fwd_body_params.push(fwd_while)
            fwd_body := Expr.new_explicit(NodeType.Body, fwd_body_params, initial_token.line, initial_token.col)

            mut rev_body_params := Vec.new(Expr)
            rev_body_params.push(rev_while)
            rev_body := Expr.new_explicit(NodeType.Body, rev_body_params, initial_token.line, initial_token.col)

            mut if_params := Vec.new(Expr)
            if_params.push(dir_cond_expr)
            if_params.push(fwd_body)
            if_params.push(rev_body)
            loop_expr = Expr.new_explicit(NodeType.If, if_params, initial_token.line, initial_token.col)
        }
    }

    mut final_params := Vec.new(Expr)
    final_params.push(decl_expr)
    final_params.push(loop_expr)

    return Expr.new_explicit(NodeType.Body, final_params, initial_token.line, initial_token.col)
}

parse_for_statement := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    initial_token := lexer.peek()?
    lexer.advance(1)? // consume 'for'

    // Expect loop variable name
    ident_token := lexer.expect(TokenType.Identifier)?
    // Replace _ with a unique generated name to avoid C redeclaration issues
    mut loop_var_name := ""
    if ident_token.token_str.eq("_") {
        loop_var_name = next_loop_var()
    } else {
        loop_var_name = ident_token.token_str.clone()
    }

    // Check for type annotation: for VAR: TYPE in ...
    // Bug #32 fix: Now we can use lexer.peek().token_type directly
    switch lexer.peek()?.token_type {
    case TokenType.Colon:
        lexer.advance(1)? // consume ':'

        // Get the type annotation
        type_token := lexer.expect(TokenType.Identifier)?
        var_type_name := type_token.token_str.clone()

        _ := lexer.expect(TokenType.In)?

        // Parse expression after 'in' - could be Range or collection
        // Bug #92: Don't use parse_case_expr here because its pattern matching logic
        // interferes with function calls like list_dir(dir)?
        first_expr := parse_primary(lexer)?

        // Check if it's a Range expression (primary followed by ..)
        mut typed_is_range := false
        switch lexer.peek()?.token_type {
        case TokenType.DoubleDot:
            typed_is_range = true
        case:
        }

        if typed_is_range {
            // Range-based for with type annotation: for VAR: TYPE in start..end { ... }
            lexer.advance(1)? // consume ..
            typed_start_expr := first_expr
            typed_end_expr := parse_primary(lexer)?

            _ := lexer.expect(TokenType.LeftBrace)?
            typed_body_expr := parse_body(lexer, TokenType.RightBrace)?

            // Use shared helper for range desugaring with explicit type
            return desugar_range_for(loop_var_name, var_type_name, typed_start_expr, typed_end_expr, typed_body_expr, initial_token)?
        }

        // Collection-based for: for VAR: TYPE in COLLECTION { ... }
        _ := lexer.expect(TokenType.LeftBrace)?
        forin_body_expr := parse_body(lexer, TokenType.RightBrace)?

        // ForIn node: params[0] = Identifier(var_name), params[1] = collection, params[2] = body
        var_ident := Expr.new_parse(NodeType.Identifier(loop_var_name), initial_token.clone(), Vec.new(Expr))

        mut forin_params := Vec.new(Expr)
        forin_params.push(var_ident)
        forin_params.push(first_expr)
        forin_params.push(forin_body_expr)


        return Expr.new_explicit(NodeType.ForIn(var_type_name), forin_params, initial_token.line, initial_token.col)
    case:
        // Fall through to range-based for
    }

    // Range-based for: for VAR in RANGE { ... }
    _ := lexer.expect(TokenType.In)?

    // Parse the range expression (e.g., 1..10)
    range_expr := parse_case_expr(lexer)?

    // Check if it's a Range node type
    mut is_range := false
    switch range_expr.node_type {
        case NodeType.Range:
            is_range = true
        case:
    }
    if not(is_range) {
        throw ident_token.error(lexer.path, "Expected range expression (start..end) after 'in'. For collection iteration, use 'for VAR: TYPE in COLLECTION'")
    }
    start_expr := range_expr.get(0)?.clone()
    end_expr := range_expr.get(1)?.clone()

    _ := lexer.expect(TokenType.LeftBrace)?
    body_expr := parse_body(lexer, TokenType.RightBrace)?

    // Use shared helper for range desugaring with inferred type
    return desugar_range_for(loop_var_name, INFER_TYPE, start_expr, end_expr, body_expr, initial_token)?
}

// Helper function to extract full identifier name from an expression
// Handles both simple identifiers and dotted names (represented as FCall)
get_full_identifier_name := func(e: Expr) returns Str throws Str {
    mut result := ""
    ent := e.node_type
    switch ent {
        case NodeType.Identifier(name):
            result = name.clone()
            // Check if this is a dotted name: Identifier with params
            // For example, "Color.Green" is Identifier("Color") with params=[Identifier("Green")]
            if e.params.len().eq(1) {
                mut p0 := Expr()
                e.params.get(0, p0)?
                p0nt := p0.node_type
                switch p0nt {
                    case NodeType.Identifier(param_name):
                        result = name.concat(".").concat(param_name)
                    case:
                }
            }
        case NodeType.FCall(_):
            if e.params.len().gteq(1) {
                // For FCall, try to extract the function name
                mut p0_fcall := Expr()
                e.params.get(0, p0_fcall)?
                result = get_full_identifier_name(p0_fcall)?
            }
        case:
    }
    catch (err: IndexOutOfBoundsError) { throw concat("get_full_identifier_name: ", err.msg) }
    return result
}

parse_case_expr := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    left := parse_primary(lexer)?
    t := lexer.peek()?
    tt := t.token_type
    switch tt {
    case TokenType.DoubleDot:
        lexer.advance(1)?
        right := parse_primary(lexer)?
        mut params := Vec.new(Expr)
        params.push(left)
        params.push(right)
        return Expr.new_parse(NodeType.Range, t, params)
    case:
        // Not a range, continue to check for pattern match
    }

    // Check if this is a pattern match: EnumVariant(binding_var)
    // This would have been parsed as FCall with one Identifier parameter
    lnt := left.node_type

    // Check if node type is FCall using switch (tag() doesn't exist in TIL)
    switch lnt {
        case NodeType.FCall(_):
            if left.params.len().eq(2) {
            // FCall params are: [function_name, arg1, arg2, ...]
            // For pattern matching, we expect: [variant_identifier, binding_identifier]
            // Note: variant_identifier might be a dotted name like "Color.Green" which could be
            // represented as an FCall itself (for the dot access)

            // Get the full variant name (handling dotted names)
            mut p0 := Expr()
            left.params.get(0, p0)?
            variant_name := get_full_identifier_name(p0)?

                mut p1 := Expr()
                left.params.get(1, p1)?
                p1nt := p1.node_type

                // Check if p1 is an Identifier using switch (tag() doesn't exist in TIL)
                switch p1nt {
                    case NodeType.Identifier(binding_var):
                        // Bug #77 fix: Only treat as a binding variable if it's a simple identifier
                        // (no nested params like TTypeDef.TEnumDef)
                        if p1.params.len().eq(0) {
                            // Convert FCall to Pattern
                            mut pattern_info := PatternInfo()
                            pattern_info.variant_name = variant_name
                            pattern_info.binding_var = binding_var.clone()
                            // Bug #27 FIXED - can now inline Vec.new(Expr) as function argument
                            return Expr.new_explicit(NodeType.Pattern(pattern_info), Vec.new(Expr), left.line, left.col)
                        }
                        // Otherwise, it's a nested pattern like ValueType.TType(TTypeDef.TEnumDef)
                        // Keep it as FCall for now (will be handled by interpreter)
                    case:
                }
            }
        case:
    }


    return left
}

parse_switch_statement := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    t := lexer.peek()?
    initial_current := lexer.current
    lexer.advance(1)?
    mut params := Vec.new(Expr)

    prim := parse_primary(lexer)?
    params.push(prim)

    peek_t := lexer.peek()?
    ptt := peek_t.token_type
    switch ptt {
    case TokenType.LeftBrace:
        lexer.advance(1)?
    case:
        throw t.error(lexer.path, "Expected '{{' after primary expression in 'switch' statement.")
    }

    mut end_found := false
    mut seen_default := false
    while lexer.current.lt(lexer.len()).and(not(end_found)) {
        mut next_t := lexer.peek()?
        ntt := next_t.token_type
        switch ntt {
        case TokenType.Case:
            // Continue parsing case
        case:
            throw next_t.error(lexer.path, format("Expected 'case' in switch, found '", token_type_to_str(next_t.token_type), "'"))
        }
        if seen_default {
            throw next_t.error(lexer.path, "default case must be last in switch")
        }

        lexer.advance(1)?
        next_t = lexer.peek()?
        ntt2 := next_t.token_type
        switch ntt2 {
        case TokenType.Colon:
            seen_default = true
            default_case_expr := Expr.new_parse(NodeType.DefaultCase, t.clone(), Vec.new(Expr))
            params.push(default_case_expr)
        case:
            prim2 := parse_case_expr(lexer)?
            params.push(prim2)
        }

        next_t = lexer.peek()?
        ntt3 := next_t.token_type
        switch ntt3 {
        case TokenType.Colon:
            // Continue parsing case body
        case:
            throw next_t.error(lexer.path, format("Expected ':' case <primary_expr> in switch, found '", token_type_to_str(next_t.token_type), "'"))
        }

        lexer.advance(1)?
        next_t = lexer.peek()?
        mut body_params := Vec.new(Expr)
        mut inner_loop_done := false
        while lexer.current.lt(lexer.len()).and(not(inner_loop_done)) {  // TODO: Replace with 'while lexer.current < lexer.len()' + 'break' when implemented
            ntt4 := next_t.token_type
            switch ntt4 {
            case TokenType.RightBrace:
                brace_body_expr := Expr.new_parse(NodeType.Body, t.clone(), body_params)
                params.push(brace_body_expr)
                end_found = true
                lexer.advance(1)?
                inner_loop_done = true  // TODO: Replace with 'break' when implemented (Rust line 1238)
            case TokenType.Case:
                case_body_expr := Expr.new_parse(NodeType.Body, t.clone(), body_params)
                params.push(case_body_expr)
                inner_loop_done = true  // TODO: Replace with 'break' when implemented (Rust line 1242)
            case:
                stmt := parse_statement(lexer)?
                body_params.push(stmt)
                next_t = lexer.peek()?
            }
        }
    }


    if end_found {
        initial_token := lexer.get_token(initial_current)?.clone()
        return Expr.new_parse(NodeType.Switch, initial_token, params)
    }
    throw t.error(lexer.path, "Expected '}}' to end switch.")
}

parse_declaration := func(mut lexer: Lexer, is_mut: Bool, is_copy: Bool, explicit_type: Str, has_explicit_type: Bool) returns Expr throws Str, IndexOutOfBoundsError {
    t := lexer.peek()?
    decl_name := t.token_str
    initial_current := lexer.current

    lexer.advance(3)? // skip identifier, colon and equal (or identifier, colon, type)
    if has_explicit_type {
        lexer.advance(1)? // skip equal after type token (Bug #129: use boolean, not string comparison)
    }

    mut params := Vec.new(Expr)
    value := parse_primary(lexer)?
    params.push(value)

    mut decl := Declaration()
    decl.name = decl_name
    decl.value_type = str_to_value_type(explicit_type)
    decl.is_mut = is_mut
    decl.is_copy = is_copy
    decl.is_own = false

    initial_token := lexer.get_token(initial_current)?.clone()


    return Expr.new_parse(NodeType.Declaration(decl), initial_token, params)
}

parse_mut_declaration := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    t := lexer.peek()?
    mut next_t := lexer.next()?
    mut next_token_type := next_t.token_type

    ntt := next_token_type
    switch ntt {
    case TokenType.Identifier:
        identifier := next_t.token_str
        lexer.advance(1)?
        next_t = lexer.next()?
        next_token_type = next_t.token_type

        ntt2 := next_token_type
        switch ntt2 {
        case TokenType.Colon:
            next_next_t := lexer.peek_ahead(2)?
            next_next_token_type := next_next_t.token_type

            nntt := next_next_token_type
            switch nntt {
            case TokenType.Identifier:
                type_name := next_next_t.token_str
                return parse_declaration(lexer, true, false, type_name, true)?
            case TokenType.Equal:
                return parse_declaration(lexer, true, false, INFER_TYPE, false)?
            case:
                throw t.error(lexer.path, format("Expected a type identifier or '=' after 'mut ", identifier, " :' in statement, found '", token_type_to_str(next_next_token_type), "'."))
            }
        case:
            throw t.error(lexer.path, format("Expected ':' after 'mut ", identifier, "', found '", token_type_to_str(next_token_type), "'."))
        }
    case:
        throw t.error(lexer.path, format("Expected identifier after 'mut', found '", token_type_to_str(next_token_type), "'."))
    }
}

parse_statement := func(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    t := lexer.peek()?
    tt := t.token_type

    switch tt {
    case TokenType.Return:
        return return_statement(lexer)?
    case TokenType.Throw:
        return parse_throw_statement(lexer)?
    case TokenType.Break:
        return parse_break_statement(lexer)?
    case TokenType.Continue:
        return parse_continue_statement(lexer)?
    case TokenType.If:
        return if_statement(lexer)?
    case TokenType.While:
        return while_statement(lexer)?
    case TokenType.For:
        return parse_for_statement(lexer)?
    case TokenType.Switch:
        return parse_switch_statement(lexer)?
    case TokenType.Mut:
        return parse_mut_declaration(lexer)?
    case TokenType.Identifier:
        return parse_statement_identifier(lexer)?
    case TokenType.Catch:
        return parse_catch_statement(lexer)?
    case TokenType.LeftBrace:
        lexer.advance(1)? // Skip LeftBrace
        return parse_body(lexer, TokenType.RightBrace)?
    case TokenType.Namespace:
        return parse_namespace_definition(lexer)?
    case:
        throw t.error(lexer.path, format("Expected statement, found ", token_type_to_str(t.token_type), "."))
    }
}

parse_body := func(mut lexer: Lexer, end_token: TokenType) returns Expr throws Str, IndexOutOfBoundsError {
    initial_current := lexer.current
    mut params := Vec.new(Expr)
    mut end_found := false
    mut t := lexer.peek()? // track last token for error reporting

    mut continue_loop := true
    while lexer.current.lt(lexer.len()).and(not(end_found)) {
        if not(continue_loop) {
            continue_loop = true
        }

        t = lexer.peek()? // update t with the current token
        token_type := t.token_type
        mut matches_end := false
        switch token_type {
        case end_token:
            matches_end = true
        case:
            matches_end = false
        }
        if matches_end {
            lexer.advance(1)?
            end_found = true
            continue_loop = false
        } else {
            stmt := parse_statement(lexer)?
            params.push(stmt)
        }
    }


    if end_found {
        initial_token := lexer.get_token(initial_current)?.clone()
        return Expr.new_parse(NodeType.Body, initial_token, params)
    }
    throw t.error(lexer.path, format("Expected '", token_type_to_str(end_token), "' to end body."))
}

parse_tokens := proc(mut lexer: Lexer) returns Expr throws Str, IndexOutOfBoundsError {
    e := parse_body(lexer, TokenType.Eof)?

    lexer_len_val := lexer.len()
    mut unparsed_tokens := 0

    if lexer.current.lt(lexer_len_val) {
        unparsed_tokens = lexer_len_val.sub(lexer.current)
    }

    if unparsed_tokens.gt(0) {
        println(format("Total tokens parsed: ", lexer.current.to_str(), "/", lexer_len_val.to_str()))
    }

    for i in lexer.current..lexer_len_val {
        t := lexer.get_token(i)?
        println(format("Token: ", token_type_to_str(t.token_type)))
    }

    if unparsed_tokens.gt(0) {
        throw format("Total unparsed tokens: ", unparsed_tokens.to_str(), "/", lexer_len_val.to_str())
    }

    return e
}
