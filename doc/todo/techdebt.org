#+TITLE: Performance Technical Debt
#+DATE: 2025-11-12
#+UPDATED: 2025-11-14 (Corrected)
#+AUTHOR: TIL Development

* Overview

Two interconnected performance issues require attention before self-hosting:

1. **Field Registration** (Issue 1) - COMPLETED 2025-11-13 ✓
2. **Context Cloning** (Issue 2) - IN PROGRESS (new incremental plan 2025-11-14)

**Status Update (2025-11-13)**: Issue 1 has been successfully completed. arena_index now uses
O(variables) entries instead of O(fields), and field offsets are calculated dynamically on demand.
All tests pass. This enables Issue 2 to proceed with maximum benefit.

**Status Update (2025-11-14)**: Issue 2 plan updated to use incremental ScopeStack-based approach
instead of GlobalContext+LocalContext split. Phase 1 (ScopeStack infrastructure) completed in
commit ee84e5e. Ready to begin Step 1.

* Issue 1: Field Registration - COMPLETED ✓

** Current State

Every struct instance creates O(n) arena_index entries where n = total fields including nested structs.

*** Example: Vec instance creates 8 entries

#+BEGIN_SRC rust
// From init.rs:1009-1063 (map_instance_fields)
// Given: Vec instance "my_vec"

self.arena_index.insert("my_vec", base_offset);           // 1: base struct
self.arena_index.insert("my_vec.length", offset1);        // 2: length field
self.arena_index.insert("my_vec.cap", offset2);           // 3: cap field
self.arena_index.insert("my_vec.data_ptr", offset3);      // 4: data_ptr field
self.arena_index.insert("my_vec.typ", offset4);           // 5: typ field
self.arena_index.insert("my_vec.typ.c_string", offset5);  // 6: nested Str field
self.arena_index.insert("my_vec.typ.cap", offset6);       // 7: nested Str field
self.arena_index.insert("my_vec.elem_size", offset7);     // 8: elem_size field
#+END_SRC

This is **8 HashMap entries** for a single Vec instance.

*** Where It Happens

#+BEGIN_SRC rust
// init.rs:1009-1063
pub fn map_instance_fields(&mut self, type_name: &str, instance_name: &str, e: &mut Evaluator) -> Result<(), Error> {
    let members = /* ... get struct members ... */;

    for (field_name, decl) in members {
        if decl.is_mut {
            let combined_name = format!("{}.{}", instance_name, field_name);  // Line 1029

            // Creates "obj.field" entry
            self.arena_index.insert(combined_name.clone(), field_offset);     // Line 1031

            // Recurses for nested structs - creates "obj.field.subfield" entries
            if let ValueType::TCustom(type_name) = &decl.value_type {
                if self.struct_defs.contains_key(type_name) {
                    self.map_instance_fields(type_name, &combined_name, e)?;  // Line 1046
                }
            }
        }
    }
}
#+END_SRC

*** Where It's Used

Field access looks up full dotted names:

#+BEGIN_SRC rust
// interpreter.rs:1120-1213 (eval_custom_expr)
NodeType::LVar(inner_name) => {
    current_name = format!("{}.{}", current_name, inner_name);  // Build "obj.field.subfield"
    // ...
    if let Some(&offset) = context.arena_index.get(&current_name) {  // Lookup full dotted name
        // ... use offset
    }
}
#+END_SRC

** Impact

- **Memory**: O(n) entries per struct where n = total fields including nested
- **Clone cost**: arena_index is cloned on every function call (see Issue 2)
- **Example**: 100 Vec instances = 800 arena_index entries instead of 100

** Proper Solution

Calculate field offsets dynamically from StructDef instead of pre-registering them.

*** Keep in arena_index

Only base variable names:
- "my_vec" -> offset

*** Remove from arena_index

All field paths:
- "my_vec.length" (remove)
- "my_vec.cap" (remove)
- "my_vec.typ.c_string" (remove)
- etc.

*** Calculate offsets on demand

#+BEGIN_SRC rust
// Pseudocode for eval_custom_expr
fn get_field_offset(context: &Context, base_var: &str, field_path: &[&str]) -> Result<usize, Error> {
    let base_offset = context.arena_index.get(base_var)?;  // Only lookup base variable
    let mut current_offset = base_offset;
    let mut current_type = context.get_var_type(base_var)?;

    for field_name in field_path {
        let struct_def = context.struct_defs.get(&current_type)?;
        let field_offset = calculate_field_offset(struct_def, field_name)?;  // Calculate from StructDef
        current_offset += field_offset;
        current_type = struct_def.get_field_type(field_name)?;
    }

    Ok(current_offset)
}
#+END_SRC

*** Changes Required

1. **Remove map_instance_fields()** entirely (init.rs:1009-1063)

2. **Update eval_custom_expr()** (interpreter.rs:1120-1213)
   - Parse dotted names into base + field path
   - Calculate offset from StructDef on demand

3. **Add offset calculation helper**
   - Walk StructDef to find field
   - Sum offsets of preceding fields
   - Handle nested structs recursively

4. **Update insert_struct()** (init.rs:1160-1332)
   - Only insert base variable name
   - Remove map_instance_fields() call

** Performance Trade-off

- **Cost**: Field access becomes O(d) where d = nesting depth (typically 1-3)
- **Benefit**: arena_index becomes O(v) where v = number of variables (not fields)
- **Net win**: Smaller HashMap, faster clones, simpler code

** Implementation Summary (Completed 2025-11-13)

*** Changes Made

1. **Removed map_instance_fields() function** (init.rs:1083-1137)
   - Previously registered O(n) field paths in arena_index
   - Now completely removed - fields accessed via dynamic calculation

2. **Removed field registration from copy_fields()** (init.rs:1188-1194)
   - Previously inserted field paths into arena_index and symbols
   - Now only copies memory, no registration

3. **Updated type checker for dynamic field validation** (typer.rs:984-1021)
   - check_assignment() now validates fields against struct definitions
   - Walks field path to verify each field exists in the struct
   - Checks base variable mutability, not individual field entries

4. **Fixed field mutability inheritance** (init.rs:1522-1552)
   - register_struct_fields_for_typecheck() now inherits instance mutability
   - Fields of const instances are const, fields of mut instances are mut

5. **Removed 3 map_instance_fields() calls** (interpreter.rs:945, 1857, 1904)
   - Replaced with comments explaining dynamic offset calculation

*** Results

- ✅ All tests pass (src/tests.til complete test suite)
- ✅ arena_index now O(variables) instead of O(fields)
- ✅ Field access via get_field_offset() and calculate_field_offset()
- ✅ Type checking validates against struct definitions
- ✅ No observable behavior changes

*** Performance Impact

- **Before**: Vec instance → 8 arena_index entries (base + 7 fields)
- **After**: Vec instance → 1 arena_index entry (base only)
- **Improvement**: ~8x reduction in arena_index size for Vec
- **Benefit for Issue 2**: Smaller arena_index → cheaper context clones

** Blockers

None - COMPLETED ✓

** Testing Strategy

1. ✅ All existing tests pass - same observable behavior
2. ✅ Type checker correctly validates field assignments
3. ⏭ Performance microbenchmark for field access (optional future work)

* Issue 2: Context Cloning - IN PROGRESS (New Incremental Plan)

** Current State (as of 2025-11-14)

*** ScopeStack Infrastructure - Phase 1 COMPLETED ✓

Commit ee84e5e added lexical scoping infrastructure but it's not yet used:

#+BEGIN_SRC rust
// init.rs:37-123
#[derive(Clone)]
pub struct ScopeFrame {
    pub local_vars: HashMap<String, usize>,        // Variable name → arena offset
    pub local_symbols: HashMap<String, SymbolInfo>, // Variable name → type info
    pub scope_type: ScopeType,                      // Global, Function, Block, Catch
}

#[derive(Clone)]
pub struct ScopeStack {
    pub frames: Vec<ScopeFrame>,
}
#+END_SRC

Methods available:
- push(scope_type) - Create new scope frame
- pop() - Remove current scope frame
- lookup_var(name) - Find variable offset (walks stack)
- lookup_symbol(name) - Find symbol info (walks stack)
- declare_var(name, offset, symbol) - Declare in current scope

*** Context Structure

#+BEGIN_SRC rust
// init.rs:917-949
#[derive(Clone)]
pub struct Context {
    pub mode: ModeDef,
    pub path: String,

    // === IMMUTABLE AFTER PROGRAM LOAD ===
    pub symbols: HashMap<String, SymbolInfo>,      // TARGET: Move to scope_stack.frames[0]
    pub funcs: HashMap<String, SFuncDef>,
    pub enum_defs: HashMap<String, SEnumDef>,
    pub struct_defs: HashMap<String, SStructDef>,

    // === MUTABLE DURING RUNTIME ===
    pub arena_index: HashMap<String, usize>,       // TARGET: Move to scope_stack.frames[*]
    pub scope_stack: ScopeStack,                   // NEW - ready to use

    // === TEMPORARY STATE ===
    pub temp_enum_payload: Option<(Vec<u8>, ValueType)>,

    // === IMPORT CACHING ===
    pub imports_declarations_done: HashSet<String>,
    pub imports_values_done: HashMap<String, Result<EvalResult, String>>,
    pub imports_wip: HashSet<String>,
}
#+END_SRC

*** Clone Sites (5 locations)

1. **interpreter.rs:1440** - eval_user_func_proc_call() - EVERY FUNCTION CALL
2. **typer.rs:131** - check_user_proc()
3. **typer.rs:862** - check_catch()
4. **typer.rs:1117** - check_loop()
5. **typer.rs:1264** - check_if_else()

**Current clone cost**: 68-247 KB per function call

** New Incremental Migration Plan

Instead of the old GlobalContext+LocalContext split, we'll incrementally migrate ALL fields from
Context to ScopeStack, with arena_index migrated LAST (right before removing clones), testing
after each step.

*** STEP 1: Extend ScopeStack to support all definition types

**Status**: TODO - Not started

**Goal**: Add fields to ScopeFrame for funcs, enums, structs (not just symbols and vars)

**Changes**:
1. Add fields to ScopeFrame (init.rs:37-123):
   - pub local_funcs: HashMap<String, SFuncDef>
   - pub local_enums: HashMap<String, SEnumDef>
   - pub local_structs: HashMap<String, SStructDef>

2. Add lookup methods to ScopeStack:
   - lookup_func(&self, name: &str) -> Option<&SFuncDef>
   - lookup_enum(&self, name: &str) -> Option<&SEnumDef>
   - lookup_struct(&self, name: &str) -> Option<&SStructDef>

3. Add declare methods to ScopeStack:
   - declare_func(&mut self, name: String, func_def: SFuncDef)
   - declare_enum(&mut self, name: String, enum_def: SEnumDef)
   - declare_struct(&mut self, name: String, struct_def: SStructDef)

**Bonus**: This enables local function/enum/struct definitions in the future!

**Files affected**: init.rs (ScopeFrame and ScopeStack implementation)

**Verification**: make tests - compiles, all tests pass

**Deliverables**:
- Code changes committed
- This doc updated with "Step 1 COMPLETED ✓"

*** STEP 2: Migrate Context::symbols → ScopeStack::symbols - COMPLETED ✓

**Status**: COMPLETED 2025-11-14 (commit c7a07d0)

**Goal**: Move all symbol lookups/insertions from Context.symbols to scope_stack.symbols

**Changes completed**:
1. Renamed ScopeFrame.local_symbols → symbols for simplicity
2. Preserved original "context priming"/"declaration indexing" comment
3. Added ScopeStack methods: declare_symbol(), remove_symbol(), get_symbols_with_prefix()
4. Migrated 109 call sites across 4 files
5. Removed Context.symbols field from struct

**Files affected**: init.rs, interpreter.rs, typer.rs, ext.rs

**Verification**: ✓ make tests passes - all tests pass with no behavior change

**Result**: Symbol table now lives in ScopeStack, enabling proper lexical scoping

*** STEP 3: Migrate Context::funcs → ScopeStack::funcs - COMPLETED ✓

**Status**: COMPLETED 2025-11-14

**Goal**: Move all function lookups from Context.funcs to scope_stack.funcs

**Changes completed**:
1. Added funcs field to ScopeFrame with original comment
2. Added ScopeStack methods: lookup_func(), declare_func()
3. Updated all context.funcs.get() → context.scope_stack.lookup_func()
4. Updated all context.funcs.insert() → context.scope_stack.declare_func()
5. Updated all context.funcs.contains_key() → context.scope_stack.lookup_func().is_some()
6. Removed Context.funcs field from struct

**Bonus**: Enables local function definitions in future!

**Files affected**: init.rs, interpreter.rs, typer.rs

**Verification**: ✓ make tests passes - all tests pass with no behavior change

**Result**: Functions now live in ScopeStack, enabling scoped function definitions

*** STEP 4: Migrate Context::enum_defs → ScopeStack::enums - COMPLETED ✓

**Status**: COMPLETED 2025-11-14

**Goal**: Move all enum lookups from Context.enum_defs to scope_stack.enums

**Changes completed**:
1. Added enums field to ScopeFrame with original comment
2. Added ScopeStack methods: lookup_enum(), declare_enum()
3. Updated all context.enum_defs.get() → context.scope_stack.lookup_enum()
4. Updated all context.enum_defs.insert() → context.scope_stack.declare_enum()
5. Updated all context.enum_defs.contains_key() → context.scope_stack.lookup_enum().is_some()
6. Removed Context.enum_defs field from struct

**Bonus**: Enables local enum definitions in future!

**Files affected**: init.rs, interpreter.rs, typer.rs

**Verification**: ✓ make tests passes - all tests pass with no behavior change

**Result**: Enums now live in ScopeStack, enabling scoped enum definitions

*** STEP 5: Migrate Context::struct_defs → ScopeStack::local_structs

**Status**: TODO - Blocked by Step 4

**Goal**: Move all struct lookups from Context.struct_defs to scope_stack.local_structs

**Changes**:
1. During init_context(), populate scope_stack.frames[0].local_structs with all structs
2. Update all context.struct_defs.get() → context.scope_stack.lookup_struct()
3. Update all context.struct_defs.contains_key() → context.scope_stack.lookup_struct().is_some()
4. Remove struct_defs: HashMap<String, SStructDef> from Context struct

**Bonus**: Enables local struct definitions in future!

**Files affected**: init.rs, interpreter.rs, typer.rs (~120 lookup sites)

**Verification**: make tests - all tests pass

**Deliverables**:
- Code changes committed
- This doc updated with "Step 5 COMPLETED ✓"

*** STEP 6: Migrate Context::arena_index → ScopeStack::local_vars (LAST BEFORE REMOVING CLONES)

**Status**: TODO - Blocked by Step 5

**Goal**: Move variable-to-offset mappings to scope stack - THIS IS THE CRITICAL STEP

**Changes**:
1. During init_context(), populate scope_stack.frames[0].local_vars with all globals
2. In function calls, use scope_stack.declare_var() for parameters and local vars
3. Update all context.arena_index.get() → context.scope_stack.lookup_var()
4. Update all context.arena_index.insert() → context.scope_stack.declare_var()
5. Handle arena_index.remove() for ownership transfer (need special logic)
6. Remove arena_index: HashMap<String, usize> from Context struct

**This is the most critical field** - it tracks where variables are stored in memory at runtime.
We do this LAST because it's the most sensitive to bugs.

**Files affected**: init.rs, interpreter.rs, ext.rs (~150 lookup sites)

**Verification**: make tests - all tests pass, no memory corruption

**Deliverables**:
- Code changes committed
- This doc updated with "Step 6 COMPLETED ✓"

*** STEP 7: Replace all context.clone() with scope push/pop

**Status**: TODO - Blocked by Step 6

**Goal**: Eliminate Context clones by using scope management - THE PAYOFF!

**Changes**:
1. In eval_user_func_proc_call() (interpreter.rs:1440):
   - Replace: let mut func_context = context.clone();
   - With: context.scope_stack.push(ScopeType::Function);
   - Replace: eval_body(&mut function_context, ...) → eval_body(context, ...)
   - Add cleanup: context.scope_stack.pop()?; before return
   - Handle path update separately (function_path variable?)

2. In typer.rs check_user_proc (line 131):
   - Same pattern: replace clone with push/pop

3. In typer.rs check_catch (line 862):
   - Same pattern: replace temp_context.clone() with push/pop

4. In typer.rs check_loop pattern matching (line 1117):
   - Same pattern: replace temp_context.clone() with push/pop

5. In typer.rs check_struct method definitions (line 1264):
   - Same pattern: replace function_context.clone() with push/pop

**Result**: Context clones ELIMINATED! Only scope frame push/pop operations.

**Verification**: make tests - all tests pass, performance massively improves

**Deliverables**:
- Code changes committed
- This doc updated with "Step 7 COMPLETED ✓"
- Performance improvement measured and documented

*** STEP 8: Verify imports_* fields don't need cloning

**Status**: TODO - Blocked by Step 7

**Goal**: Confirm that import tracking fields are never modified in cloned contexts

**Changes**:
1. Search all clone sites to verify imports_* fields are never modified in cloned contexts
2. Document findings: imports are always global, never scoped
3. If any modifications found, handle appropriately (likely move to ScopeStack or document why safe)

**Files affected**: Just documentation

**Verification**: Code inspection

**Deliverables**:
- Analysis documented in this file
- This doc updated with "Step 8 COMPLETED ✓"

** Expected Benefits

After all steps complete:

- **Memory**: Eliminate 68-247 KB clones per function call → minimal overhead
- **Correctness**: Proper lexical scoping (variables scoped to declaration blocks)
- **Performance**: Faster function calls, reduced allocations
- **Foundation**: Enables future pass-by-reference implementation

** Performance Estimate

- **Before**: 68-247 KB per function call
- **After Step 7**: ~100 bytes per function call (scope stack push/pop only)
- **Improvement**: ~1500x reduction in cloning overhead

** Risk Mitigation

- **Incremental**: Each step is small, testable, committable
- **No dual-write complexity**: Direct migration, remove old field immediately after migrating
- **Verification**: make tests after every step ensures correctness
- **Rollback**: Git commits allow reverting any problematic step
- **Arena_index last**: Most critical field migrated last when we're confident in the approach

** Why This Replaces the Old Plan

The old plan (GlobalContext + LocalContext with Arc) was designed before ScopeStack existed.
The corrected plan:
- Leverages existing ScopeStack infrastructure (Phase 1 complete)
- Migrates ALL Context fields to ScopeStack (not just symbols and arena_index)
- Enables local func/enum/struct definitions as bonus feature
- Does arena_index migration LAST (right before removing clones) since it's most critical
- Provides proper lexical scoping as a bonus
- More incremental and testable
- Better performance and correctness

** Implementation Status

- Phase 1: ScopeStack infrastructure ✓ (commit ee84e5e)
- Step 1: Extend ScopeStack (funcs, enums, structs) → TODO
- Step 2: Migrate Context::symbols → TODO
- Step 3: Migrate Context::funcs → TODO
- Step 4: Migrate Context::enum_defs → TODO
- Step 5: Migrate Context::struct_defs → TODO
- Step 6: Migrate Context::arena_index → TODO (LAST BEFORE CLONES)
- Step 7: Replace clones with push/pop → TODO (THE PAYOFF!)
- Step 8: Verify imports_* fields → TODO

* Combined Performance Estimate

** Current State
- Field registration: O(v) entries (v = variables) ✓ Issue 1 complete
- Context cloning: 68-247 KB per function call
- Example: 1000 function calls = 68-247 MB cloned

** After Issue 2 Steps 1-5
- Field registration: O(v) entries
- Context cloning: ~100 bytes per function call (scope push/pop)
- Example: 1000 function calls = ~100 KB cloned

** After Issue 2 Steps 6-7
- Field registration: O(v) entries
- Context cloning: Arc reference counting only
- Block-level scoping: Correct lexical scope handling
- Example: 1000 function calls = negligible overhead

**Total improvement**: ~1500x reduction in cloning overhead + correct scoping

* Implementation Priority

1. **Issue 1 (Field Registration)** - COMPLETED ✓
   - arena_index now O(variables) instead of O(fields)
   - Sets up Issue 2 for maximum benefit

2. **Issue 2 (Context Cloning)** - IN PROGRESS
   - Follow 7-step incremental plan
   - Test and commit after each step
   - Track progress in this document

* Relationship to Other Work

** Not Blocked By
- Pass-by-reference refactoring (doc/todo/byref_plan.org)
- Self-hosting work

** May Help With
- Self-hosting performance (fewer allocations to port)
- Simpler mental model (proper lexical scoping)

** Blocks
- Nothing critical, but performance matters for self-hosting

* Notes

- Arena itself is NOT cloned (it's a singleton static mut INSTANCE)
- Only arena_index HashMap (variable name → offset mapping) is cloned (until Step 4)
- ScopeStack infrastructure added in commit ee84e5e (2025-11-13)
- New incremental plan replaces old GlobalContext+LocalContext approach
- Tests should pass unchanged after each step
