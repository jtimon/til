#+TITLE: Performance Technical Debt
#+DATE: 2025-11-12
#+UPDATED: 2025-11-14
#+AUTHOR: TIL Development

* Overview

Two interconnected performance issues require attention before self-hosting:

1. **Field Registration** (Issue 1) - COMPLETED 2025-11-13 ✓
2. **Context Cloning** (Issue 2) - IN PROGRESS (new incremental plan 2025-11-14)

**Status Update (2025-11-13)**: Issue 1 has been successfully completed. arena_index now uses
O(variables) entries instead of O(fields), and field offsets are calculated dynamically on demand.
All tests pass. This enables Issue 2 to proceed with maximum benefit.

**Status Update (2025-11-14)**: Issue 2 plan updated to use incremental ScopeStack-based approach
instead of GlobalContext+LocalContext split. Phase 1 (ScopeStack infrastructure) completed in
commit ee84e5e. Ready to begin Step 1.

* Issue 1: Field Registration - COMPLETED ✓

** Current State

Every struct instance creates O(n) arena_index entries where n = total fields including nested structs.

*** Example: Vec instance creates 8 entries

#+BEGIN_SRC rust
// From init.rs:1009-1063 (map_instance_fields)
// Given: Vec instance "my_vec"

self.arena_index.insert("my_vec", base_offset);           // 1: base struct
self.arena_index.insert("my_vec.length", offset1);        // 2: length field
self.arena_index.insert("my_vec.cap", offset2);           // 3: cap field
self.arena_index.insert("my_vec.data_ptr", offset3);      // 4: data_ptr field
self.arena_index.insert("my_vec.typ", offset4);           // 5: typ field
self.arena_index.insert("my_vec.typ.c_string", offset5);  // 6: nested Str field
self.arena_index.insert("my_vec.typ.cap", offset6);       // 7: nested Str field
self.arena_index.insert("my_vec.elem_size", offset7);     // 8: elem_size field
#+END_SRC

This is **8 HashMap entries** for a single Vec instance.

*** Where It Happens

#+BEGIN_SRC rust
// init.rs:1009-1063
pub fn map_instance_fields(&mut self, type_name: &str, instance_name: &str, e: &mut Evaluator) -> Result<(), Error> {
    let members = /* ... get struct members ... */;

    for (field_name, decl) in members {
        if decl.is_mut {
            let combined_name = format!("{}.{}", instance_name, field_name);  // Line 1029

            // Creates "obj.field" entry
            self.arena_index.insert(combined_name.clone(), field_offset);     // Line 1031

            // Recurses for nested structs - creates "obj.field.subfield" entries
            if let ValueType::TCustom(type_name) = &decl.value_type {
                if self.struct_defs.contains_key(type_name) {
                    self.map_instance_fields(type_name, &combined_name, e)?;  // Line 1046
                }
            }
        }
    }
}
#+END_SRC

*** Where It's Used

Field access looks up full dotted names:

#+BEGIN_SRC rust
// interpreter.rs:1120-1213 (eval_custom_expr)
NodeType::LVar(inner_name) => {
    current_name = format!("{}.{}", current_name, inner_name);  // Build "obj.field.subfield"
    // ...
    if let Some(&offset) = context.arena_index.get(&current_name) {  // Lookup full dotted name
        // ... use offset
    }
}
#+END_SRC

** Impact

- **Memory**: O(n) entries per struct where n = total fields including nested
- **Clone cost**: arena_index is cloned on every function call (see Issue 2)
- **Example**: 100 Vec instances = 800 arena_index entries instead of 100

** Proper Solution

Calculate field offsets dynamically from StructDef instead of pre-registering them.

*** Keep in arena_index

Only base variable names:
- "my_vec" -> offset

*** Remove from arena_index

All field paths:
- "my_vec.length" (remove)
- "my_vec.cap" (remove)
- "my_vec.typ.c_string" (remove)
- etc.

*** Calculate offsets on demand

#+BEGIN_SRC rust
// Pseudocode for eval_custom_expr
fn get_field_offset(context: &Context, base_var: &str, field_path: &[&str]) -> Result<usize, Error> {
    let base_offset = context.arena_index.get(base_var)?;  // Only lookup base variable
    let mut current_offset = base_offset;
    let mut current_type = context.get_var_type(base_var)?;

    for field_name in field_path {
        let struct_def = context.struct_defs.get(&current_type)?;
        let field_offset = calculate_field_offset(struct_def, field_name)?;  // Calculate from StructDef
        current_offset += field_offset;
        current_type = struct_def.get_field_type(field_name)?;
    }

    Ok(current_offset)
}
#+END_SRC

*** Changes Required

1. **Remove map_instance_fields()** entirely (init.rs:1009-1063)

2. **Update eval_custom_expr()** (interpreter.rs:1120-1213)
   - Parse dotted names into base + field path
   - Calculate offset from StructDef on demand

3. **Add offset calculation helper**
   - Walk StructDef to find field
   - Sum offsets of preceding fields
   - Handle nested structs recursively

4. **Update insert_struct()** (init.rs:1160-1332)
   - Only insert base variable name
   - Remove map_instance_fields() call

** Performance Trade-off

- **Cost**: Field access becomes O(d) where d = nesting depth (typically 1-3)
- **Benefit**: arena_index becomes O(v) where v = number of variables (not fields)
- **Net win**: Smaller HashMap, faster clones, simpler code

** Implementation Summary (Completed 2025-11-13)

*** Changes Made

1. **Removed map_instance_fields() function** (init.rs:1083-1137)
   - Previously registered O(n) field paths in arena_index
   - Now completely removed - fields accessed via dynamic calculation

2. **Removed field registration from copy_fields()** (init.rs:1188-1194)
   - Previously inserted field paths into arena_index and symbols
   - Now only copies memory, no registration

3. **Updated type checker for dynamic field validation** (typer.rs:984-1021)
   - check_assignment() now validates fields against struct definitions
   - Walks field path to verify each field exists in the struct
   - Checks base variable mutability, not individual field entries

4. **Fixed field mutability inheritance** (init.rs:1522-1552)
   - register_struct_fields_for_typecheck() now inherits instance mutability
   - Fields of const instances are const, fields of mut instances are mut

5. **Removed 3 map_instance_fields() calls** (interpreter.rs:945, 1857, 1904)
   - Replaced with comments explaining dynamic offset calculation

*** Results

- ✅ All tests pass (src/tests.til complete test suite)
- ✅ arena_index now O(variables) instead of O(fields)
- ✅ Field access via get_field_offset() and calculate_field_offset()
- ✅ Type checking validates against struct definitions
- ✅ No observable behavior changes

*** Performance Impact

- **Before**: Vec instance → 8 arena_index entries (base + 7 fields)
- **After**: Vec instance → 1 arena_index entry (base only)
- **Improvement**: ~8x reduction in arena_index size for Vec
- **Benefit for Issue 2**: Smaller arena_index → cheaper context clones

** Blockers

None - COMPLETED ✓

** Testing Strategy

1. ✅ All existing tests pass - same observable behavior
2. ✅ Type checker correctly validates field assignments
3. ⏭ Performance microbenchmark for field access (optional future work)

* Issue 2: Context Cloning - IN PROGRESS (New Incremental Plan)

** Current State (as of 2025-11-14)

*** ScopeStack Infrastructure - Phase 1 COMPLETED ✓

Commit ee84e5e added lexical scoping infrastructure but it's not yet used:

#+BEGIN_SRC rust
// init.rs:37-123
#[derive(Clone)]
pub struct ScopeFrame {
    pub local_vars: HashMap<String, usize>,        // Variable name → arena offset
    pub local_symbols: HashMap<String, SymbolInfo>, // Variable name → type info
    pub scope_type: ScopeType,                      // Global, Function, Block, Catch
}

#[derive(Clone)]
pub struct ScopeStack {
    pub frames: Vec<ScopeFrame>,
}
#+END_SRC

Methods available:
- push(scope_type) - Create new scope frame
- pop() - Remove current scope frame
- lookup_var(name) - Find variable offset (walks stack)
- lookup_symbol(name) - Find symbol info (walks stack)
- declare_var(name, offset, symbol) - Declare in current scope

*** Context Structure

#+BEGIN_SRC rust
// init.rs:917-949
#[derive(Clone)]
pub struct Context {
    pub mode: ModeDef,
    pub path: String,

    // === IMMUTABLE AFTER PROGRAM LOAD ===
    pub symbols: HashMap<String, SymbolInfo>,      // TARGET: Move to scope_stack.frames[0]
    pub funcs: HashMap<String, SFuncDef>,
    pub enum_defs: HashMap<String, SEnumDef>,
    pub struct_defs: HashMap<String, SStructDef>,

    // === MUTABLE DURING RUNTIME ===
    pub arena_index: HashMap<String, usize>,       // TARGET: Move to scope_stack.frames[*]
    pub scope_stack: ScopeStack,                   // NEW - ready to use

    // === TEMPORARY STATE ===
    pub temp_enum_payload: Option<(Vec<u8>, ValueType)>,

    // === IMPORT CACHING ===
    pub imports_declarations_done: HashSet<String>,
    pub imports_values_done: HashMap<String, Result<EvalResult, String>>,
    pub imports_wip: HashSet<String>,
}
#+END_SRC

*** Clone Sites (5 locations)

1. **interpreter.rs:1440** - eval_user_func_proc_call() - EVERY FUNCTION CALL
2. **typer.rs:131** - check_user_proc()
3. **typer.rs:862** - check_catch()
4. **typer.rs:1117** - check_loop()
5. **typer.rs:1264** - check_if_else()

**Current clone cost**: 68-247 KB per function call

** New Incremental Migration Plan

Instead of the old GlobalContext+LocalContext split, we'll incrementally migrate fields from
Context to ScopeStack, testing after each step.

*** STEP 1: Migrate Context::symbols → ScopeStack::local_symbols

**Status**: TODO - Not started

**Goal**: Move symbol lookups from global Context.symbols to scope_stack.local_symbols

**Changes**:
1. During init_context(), insert all globals/imports into scope_stack.frames[0].local_symbols
2. In function calls, insert parameters into new scope frame instead of Context.symbols
3. Update all symbol lookups to use scope_stack.lookup_symbol() instead of context.symbols.get()
4. Keep Context.symbols as backup (dual-write mode) for safety

**Files affected**: init.rs, interpreter.rs, typer.rs (~200 lookup sites)

**Verification**: make tests - all tests pass with no behavior change

**Deliverables**:
- Code changes committed
- This doc updated with "Step 1 COMPLETED ✓"
- Dual-write mode documented

*** STEP 2: Remove Context::symbols (verify ScopeStack works)

**Status**: TODO - Blocked by Step 1

**Goal**: Prove ScopeStack handles all symbol tracking correctly

**Changes**:
1. Remove symbols: HashMap<String, SymbolInfo> from Context struct
2. Remove all dual-write code from Step 1
3. All symbol operations now exclusively use scope_stack

**Verification**: make tests - all tests still pass

**Deliverables**:
- Code changes committed
- This doc updated with "Step 2 COMPLETED ✓"

*** STEP 3: Migrate Context::arena_index → ScopeStack::local_vars

**Status**: TODO - Blocked by Step 2

**Goal**: Move variable-to-offset mappings to scope stack

**Changes**:
1. During init_context(), insert globals into scope_stack.frames[0].local_vars
2. In function calls, insert local vars into new scope frame
3. Update all arena_index lookups to use scope_stack.lookup_var()
4. Keep Context.arena_index as backup (dual-write mode)

**Files affected**: init.rs, interpreter.rs, ext.rs (~150 lookup sites)

**Verification**: make tests - all tests pass

**Deliverables**:
- Code changes committed
- This doc updated with "Step 3 COMPLETED ✓"
- Dual-write mode documented

*** STEP 4: Remove Context::arena_index

**Status**: TODO - Blocked by Step 3

**Goal**: Complete migration to ScopeStack for all local state

**Changes**:
1. Remove arena_index: HashMap<String, usize> from Context struct
2. Remove dual-write code from Step 3
3. All variable lookups use scope_stack exclusively

**Verification**: make tests - all tests still pass

**Deliverables**:
- Code changes committed
- This doc updated with "Step 4 COMPLETED ✓"

*** STEP 5: Replace context.clone() with scope push/pop

**Status**: TODO - Blocked by Step 4

**Goal**: Eliminate Context clones by using scope management

**Changes**:
1. In eval_user_func_proc_call() (interpreter.rs:1440):
   - Replace: let mut func_context = context.clone();
   - With: context.scope_stack.push(ScopeType::Function);
   - Add cleanup: context.scope_stack.pop()?; before return

2. Do same for typer.rs clones (check_user_proc, check_catch, check_loop, check_if_else)

**Result**: Context clones eliminated! Only mutable fields remain in Context

**Verification**: make tests - all tests pass, performance improves

**Deliverables**:
- Code changes committed
- This doc updated with "Step 5 COMPLETED ✓"
- Performance improvement documented

*** STEP 6: Wrap immutable Context fields in Arc

**Status**: TODO - Blocked by Step 5

**Goal**: Make any remaining Context clones (if needed) cheap

**Changes**:
#+BEGIN_SRC rust
#[derive(Clone)]
struct ImmutableContext {
    funcs: HashMap<String, SFuncDef>,
    enum_defs: HashMap<String, SEnumDef>,
    struct_defs: HashMap<String, SStructDef>,
    imports_declarations_done: HashSet<String>,
    imports_values_done: HashMap<String, Result<EvalResult, String>>,
}

pub struct Context {
    immutable: Arc<ImmutableContext>,
    scope_stack: ScopeStack,
    // ... remaining mutable fields
}
#+END_SRC

Update field accesses: context.funcs → context.immutable.funcs

**Verification**: make tests - all tests pass

**Deliverables**:
- Code changes committed
- This doc updated with "Step 6 COMPLETED ✓"

*** STEP 7: Enable block-level scoping

**Status**: TODO - Blocked by Step 6

**Goal**: Proper lexical scoping for all block types

**Changes**:
1. In loop evaluation: push/pop Block scope
2. In if/else: push/pop Block scope
3. In catch blocks: push/pop Catch scope

**Result**: Variables properly scoped to their declaration blocks

**Verification**: make tests + add tests for block scoping

**Deliverables**:
- Code changes committed
- This doc updated with "Step 7 COMPLETED ✓"
- Block scoping correctness documented

** Expected Benefits

After all steps complete:

- **Memory**: Eliminate 68-247 KB clones per function call → minimal overhead
- **Correctness**: Proper lexical scoping (variables scoped to declaration blocks)
- **Performance**: Faster function calls, reduced allocations
- **Foundation**: Enables future pass-by-reference implementation

** Performance Estimate

- **Before**: 68-247 KB per function call
- **After Step 5**: ~100 bytes per function call (scope stack push/pop only)
- **After Step 6**: Arc clones negligible (~8 bytes, ref count increment)
- **Improvement**: ~1500x reduction in cloning overhead

** Risk Mitigation

- **Incremental**: Each step is small, testable, committable
- **Dual-write**: Steps 1 and 3 keep backup data structures
- **Verification**: make tests after every step ensures correctness
- **Rollback**: Git commits allow reverting any problematic step

** Why This Replaces the Old Plan

The old plan (GlobalContext + LocalContext with Arc) was designed before ScopeStack existed.
The new plan:
- Leverages existing ScopeStack infrastructure (Phase 1 complete)
- Provides proper lexical scoping as a bonus
- More incremental and testable
- Same performance benefits as old plan
- Better correctness (block-level scoping)

** Implementation Status

- Phase 1: ScopeStack infrastructure ✓ (commit ee84e5e)
- Step 1: Migrate symbols → TODO
- Step 2: Remove Context::symbols → TODO
- Step 3: Migrate arena_index → TODO
- Step 4: Remove Context::arena_index → TODO
- Step 5: Replace clones with push/pop → TODO
- Step 6: Wrap immutables in Arc → TODO
- Step 7: Block-level scoping → TODO

* Combined Performance Estimate

** Current State
- Field registration: O(v) entries (v = variables) ✓ Issue 1 complete
- Context cloning: 68-247 KB per function call
- Example: 1000 function calls = 68-247 MB cloned

** After Issue 2 Steps 1-5
- Field registration: O(v) entries
- Context cloning: ~100 bytes per function call (scope push/pop)
- Example: 1000 function calls = ~100 KB cloned

** After Issue 2 Steps 6-7
- Field registration: O(v) entries
- Context cloning: Arc reference counting only
- Block-level scoping: Correct lexical scope handling
- Example: 1000 function calls = negligible overhead

**Total improvement**: ~1500x reduction in cloning overhead + correct scoping

* Implementation Priority

1. **Issue 1 (Field Registration)** - COMPLETED ✓
   - arena_index now O(variables) instead of O(fields)
   - Sets up Issue 2 for maximum benefit

2. **Issue 2 (Context Cloning)** - IN PROGRESS
   - Follow 7-step incremental plan
   - Test and commit after each step
   - Track progress in this document

* Relationship to Other Work

** Not Blocked By
- Pass-by-reference refactoring (doc/todo/byref_plan.org)
- Self-hosting work

** May Help With
- Self-hosting performance (fewer allocations to port)
- Simpler mental model (proper lexical scoping)

** Blocks
- Nothing critical, but performance matters for self-hosting

* Notes

- Arena itself is NOT cloned (it's a singleton static mut INSTANCE)
- Only arena_index HashMap (variable name → offset mapping) is cloned (until Step 4)
- ScopeStack infrastructure added in commit ee84e5e (2025-11-13)
- New incremental plan replaces old GlobalContext+LocalContext approach
- Tests should pass unchanged after each step
