#+TITLE: Performance Technical Debt
#+DATE: 2025-11-12
#+AUTHOR: TIL Development

* Overview

Two interconnected performance issues require attention before self-hosting:

1. **Field Registration** (Issue 1) - O(n) arena_index entries per struct instance
2. **Context Cloning** (Issue 2) - Unnecessary copying of global state on every function call

**Critical ordering**: Issue 1 MUST be fixed before Issue 2 because field registration bloats
arena_index, making context cloning more expensive. Fixing field registration first makes
arena_index smaller, compounding the benefit of context cloning optimization.

* Issue 1: Field Registration (FIX THIS FIRST)

** Current State

Every struct instance creates O(n) arena_index entries where n = total fields including nested structs.

*** Example: Vec instance creates 8 entries

#+BEGIN_SRC rust
// From init.rs:1009-1063 (map_instance_fields)
// Given: Vec instance "my_vec"

self.arena_index.insert("my_vec", base_offset);           // 1: base struct
self.arena_index.insert("my_vec.length", offset1);        // 2: length field
self.arena_index.insert("my_vec.cap", offset2);           // 3: cap field
self.arena_index.insert("my_vec.data_ptr", offset3);      // 4: data_ptr field
self.arena_index.insert("my_vec.typ", offset4);           // 5: typ field
self.arena_index.insert("my_vec.typ.c_string", offset5);  // 6: nested Str field
self.arena_index.insert("my_vec.typ.cap", offset6);       // 7: nested Str field
self.arena_index.insert("my_vec.elem_size", offset7);     // 8: elem_size field
#+END_SRC

This is **8 HashMap entries** for a single Vec instance.

*** Where It Happens

#+BEGIN_SRC rust
// init.rs:1009-1063
pub fn map_instance_fields(&mut self, type_name: &str, instance_name: &str, e: &mut Evaluator) -> Result<(), Error> {
    let members = /* ... get struct members ... */;

    for (field_name, decl) in members {
        if decl.is_mut {
            let combined_name = format!("{}.{}", instance_name, field_name);  // Line 1029

            // Creates "obj.field" entry
            self.arena_index.insert(combined_name.clone(), field_offset);     // Line 1031

            // Recurses for nested structs - creates "obj.field.subfield" entries
            if let ValueType::TCustom(type_name) = &decl.value_type {
                if self.struct_defs.contains_key(type_name) {
                    self.map_instance_fields(type_name, &combined_name, e)?;  // Line 1046
                }
            }
        }
    }
}
#+END_SRC

*** Where It's Used

Field access looks up full dotted names:

#+BEGIN_SRC rust
// interpreter.rs:1120-1213 (eval_custom_expr)
NodeType::LVar(inner_name) => {
    current_name = format!("{}.{}", current_name, inner_name);  // Build "obj.field.subfield"
    // ...
    if let Some(&offset) = context.arena_index.get(&current_name) {  // Lookup full dotted name
        // ... use offset
    }
}
#+END_SRC

** Impact

- **Memory**: O(n) entries per struct where n = total fields including nested
- **Clone cost**: arena_index is cloned on every function call (see Issue 2)
- **Example**: 100 Vec instances = 800 arena_index entries instead of 100

** Proper Solution

Calculate field offsets dynamically from StructDef instead of pre-registering them.

*** Keep in arena_index

Only base variable names:
- "my_vec" -> offset

*** Remove from arena_index

All field paths:
- "my_vec.length" (remove)
- "my_vec.cap" (remove)
- "my_vec.typ.c_string" (remove)
- etc.

*** Calculate offsets on demand

#+BEGIN_SRC rust
// Pseudocode for eval_custom_expr
fn get_field_offset(context: &Context, base_var: &str, field_path: &[&str]) -> Result<usize, Error> {
    let base_offset = context.arena_index.get(base_var)?;  // Only lookup base variable
    let mut current_offset = base_offset;
    let mut current_type = context.get_var_type(base_var)?;

    for field_name in field_path {
        let struct_def = context.struct_defs.get(&current_type)?;
        let field_offset = calculate_field_offset(struct_def, field_name)?;  // Calculate from StructDef
        current_offset += field_offset;
        current_type = struct_def.get_field_type(field_name)?;
    }

    Ok(current_offset)
}
#+END_SRC

*** Changes Required

1. **Remove map_instance_fields()** entirely (init.rs:1009-1063)

2. **Update eval_custom_expr()** (interpreter.rs:1120-1213)
   - Parse dotted names into base + field path
   - Calculate offset from StructDef on demand

3. **Add offset calculation helper**
   - Walk StructDef to find field
   - Sum offsets of preceding fields
   - Handle nested structs recursively

4. **Update insert_struct()** (init.rs:1160-1332)
   - Only insert base variable name
   - Remove map_instance_fields() call

** Performance Trade-off

- **Cost**: Field access becomes O(d) where d = nesting depth (typically 1-3)
- **Benefit**: arena_index becomes O(v) where v = number of variables (not fields)
- **Net win**: Smaller HashMap, faster clones, simpler code

** Blockers

None. This can be implemented immediately.

** Testing Strategy

1. Run existing tests (should all pass - same observable behavior)
2. Verify arena_index size reduction
3. Performance microbenchmark for field access

* Issue 2: Context Cloning (FIX AFTER ISSUE 1)

** Current State

Context is cloned on every function call, copying all global state unnecessarily.

*** Context Structure

#+BEGIN_SRC rust
// init.rs:820-851
#[derive(Clone)]
pub struct Context {
    pub symbols: HashMap<String, SymbolInfo>,      // ~10-50 KB - all global symbols
    pub funcs: HashMap<String, SFuncDef>,          // ~50-200 KB - all functions with AST
    pub enum_defs: HashMap<String, SEnumDef>,      // ~5-10 KB - all enum definitions
    pub struct_defs: HashMap<String, SStructDef>,  // ~10-50 KB - all struct definitions
    pub arena_index: HashMap<String, usize>,       // ~1 KB (after Issue 1 fix) - var to offset mapping
    // ... other fields
}
#+END_SRC

**Total clone cost**: 68-247 KB per function call (estimate based on typical TIL programs)

*** Where It Happens

5 locations clone context:

1. **interpreter.rs:1438** (eval_user_func_proc_call) - EVERY FUNCTION CALL
   #+BEGIN_SRC rust
   let mut function_context = context.clone();  // Clone entire Context
   #+END_SRC

2. **typer.rs:131** (check_user_proc)
3. **typer.rs:823** (check_user_func)
4. **typer.rs:1049** (check_loop)
5. **typer.rs:1196** (check_if_else)

*** What Needs Isolation

**Only arena_index** needs to be local per function call because:
- Functions create local variables
- Local variables need isolated namespace
- Prevents name collisions between recursive calls

**Everything else is immutable** after initial program load:
- symbols: Global symbol table (const after parsing)
- funcs: Function definitions (const after parsing)
- enum_defs: Enum definitions (const after parsing)
- struct_defs: Struct definitions (const after parsing)

** Impact

- **Per function call**: 68-247 KB cloned
- **Deep recursion**: Compounds exponentially
- **Unnecessary**: 99% of cloned data is immutable globals

** Proper Solution

Split Context into GlobalContext (Arc-wrapped, shared) and LocalContext (cloned per function).

*** New Structure

#+BEGIN_SRC rust
// Global state - immutable after program load
#[derive(Clone)]
pub struct GlobalContext {
    pub symbols: HashMap<String, SymbolInfo>,
    pub funcs: HashMap<String, SFuncDef>,
    pub enum_defs: HashMap<String, SEnumDef>,
    pub struct_defs: HashMap<String, SStructDef>,
    // ... other immutable fields
}

// Local state - needs isolation per function call
pub struct LocalContext {
    pub global: Arc<GlobalContext>,  // Shared reference (cheap clone)
    pub arena_index: HashMap<String, usize>,  // Isolated per function
}
#+END_SRC

*** Cloning Cost Comparison

**Before** (current):
#+BEGIN_SRC rust
let mut function_context = context.clone();  // 68-247 KB
#+END_SRC

**After** (with Arc):
#+BEGIN_SRC rust
let mut function_context = LocalContext {
    global: context.global.clone(),  // Just Arc clone (~8 bytes, ref count increment)
    arena_index: HashMap::new(),      // Empty HashMap for local vars
};
// Total: ~100 bytes instead of 68-247 KB
#+END_SRC

*** Changes Required

1. **Split Context struct** (init.rs:820-851)
   - Create GlobalContext for immutable state
   - Create LocalContext with Arc<GlobalContext>

2. **Update all Context users**
   - Change field access: `context.symbols` -> `context.global.symbols`
   - Majority are reads (no semantic change)

3. **Update clone sites** (5 locations)
   - Replace `context.clone()` with `LocalContext { global: context.global.clone(), ... }`

4. **Update initialization**
   - Create GlobalContext once at program load
   - Wrap in Arc
   - Create initial LocalContext

** Performance Estimate

- **Clone reduction**: 68-247 KB -> ~100 bytes (~1000x improvement)
- **Recursive calls**: Exponential improvement with call depth
- **Arc overhead**: Negligible (atomic ref count increment)

** Why Issue 1 Must Come First

Context cloning optimization depends on arena_index being small:

1. **Current state**: arena_index has O(n) entries per struct (Issue 1 unfixed)
   - Vec instance = 8 entries
   - 100 Vec instances = 800 entries
   - arena_index might be ~10 KB instead of ~1 KB

2. **After fixing Issue 1**: arena_index has O(v) entries where v = variables
   - 100 Vec instances = 100 entries
   - arena_index is ~1 KB

3. **Impact on Issue 2**:
   - We still need to clone arena_index per function call (local variables need isolation)
   - Smaller arena_index = cheaper clones
   - Benefits compound

**Proper sequence**:
1. Fix Issue 1 -> arena_index shrinks from ~10 KB to ~1 KB
2. Fix Issue 2 -> total clone cost drops from 68-247 KB to ~100 bytes
3. Combined benefit: ~1500x improvement instead of ~700x

* Combined Performance Estimate

** Current State
- Field registration: O(n) entries per struct
- Context cloning: 68-247 KB per function call
- Example: 1000 function calls = 68-247 MB cloned

** After Issue 1 Only
- Field registration: O(v) entries (v = variables)
- Context cloning: 60-237 KB per function call (arena_index smaller)
- Example: 1000 function calls = 60-237 MB cloned

** After Both Issues
- Field registration: O(v) entries
- Context cloning: ~100 bytes per function call
- Example: 1000 function calls = ~100 KB cloned

**Total improvement**: ~1500x reduction in cloning overhead

* Implementation Priority

1. **Issue 1 (Field Registration)** - Implement first
   - No blockers
   - Makes arena_index smaller
   - Sets up Issue 2 for maximum benefit

2. **Issue 2 (Context Cloning)** - Implement second
   - Benefits from smaller arena_index
   - Larger code change (more struct updates)
   - Maximum combined performance gain

* Relationship to Other Work

** Not Blocked By
- Pass-by-reference refactoring (doc/todo/byref_plan.org)
- Self-hosting work

** May Help With
- Self-hosting performance (fewer allocations to port)
- Simpler mental model (fewer arena_index entries to track)

** Blocks
- Nothing critical, but performance matters for self-hosting

* Notes

- Arena itself is NOT cloned (it's a singleton static mut INSTANCE)
- Only arena_index HashMap (variable name -> offset mapping) is cloned
- Both issues are performance optimizations with no observable behavior change
- Tests should pass unchanged after each fix
